{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:03:34,344 | INFO : Loading packages ...\n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Loading packages ...\")\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from argparse import Namespace\n",
    "\n",
    "# 3rd party packages\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from src.options import Options\n",
    "from src.running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from src.utils import utils\n",
    "from src.datasets.data import data_factory, Normalizer\n",
    "from src.datasets.datasplit import split_dataset\n",
    "from src.models.ts_transformer import model_factory\n",
    "from src.models.loss import get_loss_module\n",
    "from src.optimizers import get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:03:43,210 | INFO : Stored configuration file in './tmp\\test_run'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_dir': './tmp\\\\test_run',\n",
       " 'seed': 123,\n",
       " 'gpu': '0',\n",
       " 'n_proc': 1,\n",
       " 'load_model': None,\n",
       " 'test_only': None,\n",
       " 'config_filepath': None,\n",
       " 'task': 'classification',\n",
       " 'experiment_name': 'test_run',\n",
       " 'no_timestamp': True,\n",
       " 'limit_size': 300,\n",
       " 'data_class': 'kuhar',\n",
       " 'data_dir': '../../Data/KU-HAR_time_domain_subsamples_20750x300.csv',\n",
       " 'val_ratio': 0.1,\n",
       " 'test_ratio': 0.1,\n",
       " 'norm_form': False,\n",
       " 'normalization': None,\n",
       " 'record_file': './tmp/Imputation_records.xls',\n",
       " 'records_file': './tmp/records.xls',\n",
       " 'num_workers': 0,\n",
       " 'console': True,\n",
       " 'save_all': False,\n",
       " 'comment': 'testing experiment on classification task',\n",
       " 'test_pattern': False,\n",
       " 'val_pattern': False,\n",
       " 'test_from': False,\n",
       " 'norm_from': False,\n",
       " 'freeze': False,\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 5,\n",
       " 'mask_mode': 'seperate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': '0, 1',\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': True,\n",
       " 'model': 'transformer',\n",
       " 'pos_encoding': 'learnable',\n",
       " 'd_model': 128,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'activation': 'relu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'data_window_len': None,\n",
       " 'max_seq_len': 300,\n",
       " 'epochs': 100,\n",
       " 'lr': 0.001,\n",
       " 'val_interval': 5,\n",
       " 'lr_step': '100000',\n",
       " 'lr_factor': '0.1',\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': 'store_true',\n",
       " 'key_metric': 'loss',\n",
       " 'optimizer': 'RAdam',\n",
       " 'batch_size': 32,\n",
       " 'print_interval': 5,\n",
       " 'initial_timestamp': '2022-09-15_15-03-43',\n",
       " 'save_dir': './tmp\\\\test_run\\\\checkpoints',\n",
       " 'pred_dir': './tmp\\\\test_run\\\\predictions',\n",
       " 'tensorboard_dir': './tmp\\\\test_run\\\\tb_summaries'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"output_dir\"] = './tmp'\n",
    "config[\"seed\"] = 123\n",
    "config[\"gpu\"] = \"0\" # activate gpu > o, on cpu > -1\n",
    "config[\"n_proc\"] = 1\n",
    "\n",
    "# loading pre-trained model\n",
    "config[\"load_model\"] = None\n",
    "config[\"test_only\"] = None\n",
    "config[\"config_filepath\"] = None\n",
    "\n",
    "# experiment config\n",
    "config[\"task\"] = \"imputation\"\n",
    "config[\"experiment_name\"] = \"test_run\"\n",
    "config[\"no_timestamp\"] = True\n",
    "\n",
    "# dataset settings\n",
    "config[\"limit_size\"] = 300 \n",
    "config[\"data_class\"] = 'kuhar'\n",
    "config[\"data_dir\"] = '../../Data/KU-HAR_time_domain_subsamples_20750x300.csv'\n",
    "config[\"val_ratio\"] = 0.1 \n",
    "config[\"test_ratio\"] = 0.1 \n",
    "config[\"norm_form\"] = False \n",
    "config[\"normalization\"] = 'standardization'\n",
    "config[\"record_file\"] = \"./tmp/Imputation_records.xls\"\n",
    "config[\"records_file\"] = \"./tmp/records.xls\"\n",
    "config[\"num_workers\"] = 0\n",
    "config[\"console\"] = True\n",
    "config[\"save_all\"] = False\n",
    "config[\"comment\"] = \"testing experiment on classification task\"\n",
    "\n",
    "# test, val from seperate files \n",
    "config[\"test_pattern\"] = False \n",
    "config[\"val_pattern\"] = False \n",
    "config[\"test_from\"] = False\n",
    "\n",
    "# data normalization\n",
    "config[\"norm_from\"] = False # add train time normalization to val or inference time\n",
    "config[\"normalization\"] = None#'standardization'\n",
    "\n",
    "# freeze model weight for fine-tunning\n",
    "config[\"freeze\"] = False\n",
    "\n",
    "# if task is a imputation \n",
    "config[\"masking_ratio\"] = 0.15\n",
    "config[\"mean_mask_length\"] = 5\n",
    "config[\"mask_mode\"] = \"seperate\"\n",
    "config[\"mask_distribution\"] = \"geometric\"\n",
    "config[\"exclude_feats\"] = None \n",
    "config[\"mask_feats\"] = '0, 1'\n",
    "config[\"start_hint\"] = 0.0\n",
    "config[\"end_hint\"] = 0.0 \n",
    "config[\"harden\"] = True\n",
    "\n",
    "# model parameters\n",
    "config['model'] = 'transformer'\n",
    "config[\"pos_encoding\"] = \"learnable\"\n",
    "config[\"d_model\"] = 128 \n",
    "config[\"dim_feedforward\"] = 256\n",
    "config[\"num_heads\"] = 8\n",
    "config[\"num_layers\"] = 3\n",
    "config[\"dropout\"] = 0.1\n",
    "config[\"activation\"] = 'relu'\n",
    "config[\"normalization_layer\"] = \"BatchNorm\"\n",
    "config[\"data_window_len\"] = None \n",
    "config[\"max_seq_len\"] = 300\n",
    "\n",
    "# model training parameters\n",
    "config[\"epochs\"] = 100\n",
    "config[\"lr\"] = 0.001\n",
    "config[\"val_interval\"] = 5\n",
    "config[\"lr_step\"] = '100000'\n",
    "config[\"lr_factor\"] = '0.1'\n",
    "config[\"l2_reg\"] = 0\n",
    "config[\"global_reg\"] = 'store_true'\n",
    "config[\"key_metric\"] = \"loss\"\n",
    "config[\"optimizer\"] = \"RAdam\"\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"print_interval\"] = 5\n",
    "\n",
    "setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:03:43,270 | INFO : Running:\n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\ipykernel_launcher.py --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"4130976c-be74-48e1-979b-d2027e910f88\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=c:\\Users\\deela\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-32020jy4q7DtTTfPp.json\n",
      "\n",
      "2022-09-15 15:03:43,354 | INFO : Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run\n",
    "\n",
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:03:43,407 | INFO : Loading and preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "# Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], n_proc=config['n_proc'], limit_size=config['limit_size'], config=config)\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.label.values\n",
    "    print(labels)\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20749"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:05:41,027 | INFO : 16806 samples may be used for training\n",
      "2022-09-15 15:05:41,028 | INFO : 1868 samples will be used for validation\n",
      "2022-09-15 15:05:41,029 | INFO : 2075 samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs\n",
    "\n",
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                                validation_method=validation_method,\n",
    "                                                                n_splits=1,\n",
    "                                                                validation_ratio=config['val_ratio'],\n",
    "                                                                test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                                test_indices=test_indices,\n",
    "                                                                random_seed=1337,\n",
    "                                                                labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []\n",
    "\n",
    "logger.info(\"{} samples may be used for training\".format(len(train_indices)))\n",
    "logger.info(\"{} samples will be used for validation\".format(len(val_indices)))\n",
    "logger.info(\"{} samples will be used for testing\".format(len(test_indices)))\n",
    "\n",
    "with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "    try:\n",
    "        json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                    'val_indices': list(map(int, val_indices)),\n",
    "                    'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "    except ValueError:  # in case indices are non-integers\n",
    "        json.dump({'train_indices': list(train_indices),\n",
    "                    'val_indices': list(val_indices),\n",
    "                    'test_indices': list(test_indices)}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process features\n",
    "normalizer = None\n",
    "if config['norm_from']:\n",
    "    with open(config['norm_from'], 'rb') as f:\n",
    "        norm_dict = pickle.load(f)\n",
    "    normalizer = Normalizer(**norm_dict)\n",
    "elif config['normalization'] is not None:\n",
    "    normalizer = Normalizer(config['normalization'])\n",
    "    my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "    if not config['normalization'].startswith('per_sample'):\n",
    "        # get normalizing values from training set and store for future use\n",
    "        norm_dict = normalizer.__dict__\n",
    "        with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "            pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "if normalizer is not None:\n",
    "    if len(val_indices):\n",
    "        val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "    if len(test_indices):\n",
    "        test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:05:41,221 | INFO : Creating model ...\n",
      "2022-09-15 15:05:42,959 | INFO : Model:\n",
      "TSTransformerEncoderClassiregressor(\n",
      "  (project_inp): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=38400, out_features=18, bias=True)\n",
      ")\n",
      "2022-09-15 15:05:42,960 | INFO : Total number of parameters: 1127954\n",
      "2022-09-15 15:05:42,962 | INFO : Trainable parameters: 1127954\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model ...\")\n",
    "model = model_factory(config, my_data)\n",
    "\n",
    "if config['freeze']:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('output_layer'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "logger.info(\"Model:\\n{}\".format(model))\n",
    "logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "# Load model and optimizer state\n",
    "if config[\"load_model\"]:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                        config['change_output'],\n",
    "                                                        config['lr'],\n",
    "                                                        config['lr_step'],\n",
    "                                                        config['lr_factor'])\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)\n",
    "\n",
    "if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=config['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                num_workers=config['num_workers'],\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "    test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                        print_interval=config['print_interval'], console=config['console'])\n",
    "    aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "    print_str = 'Test Summary: '\n",
    "    for k, v in aggr_metrics_test.items():\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=config['batch_size'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=config['num_workers'],\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=True,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                print_interval=config['print_interval'], console=config['console'])\n",
    "val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                    print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    2,     3,     4,     5,     6,     7,     8,     9,    10,\n",
       "               11,\n",
       "            ...\n",
       "            20741, 20742, 20743, 20744, 20745, 20746, 20747, 20748, 20749,\n",
       "            20750],\n",
       "           dtype='int64', length=20749)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    2,     3,     4,     5,     6,     7,     8,     9,    10,\n",
       "               11,\n",
       "            ...\n",
       "            20741, 20742, 20743, 20744, 20745, 20746, 20747, 20748, 20749,\n",
       "            20750],\n",
       "           dtype='int64', name='ID', length=20749)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.labels_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.feature_df.index.unique() == my_data.labels_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[ 2.0625e-02, -3.7760e-02, -4.7728e-02,  1.4503e-01, -6.2874e-03,\n",
      "          -1.8646e-02],\n",
      "         [-1.5881e-02,  3.8751e-03, -3.7520e-02,  1.4055e-01,  5.2000e-03,\n",
      "          -1.1929e-02],\n",
      "         [-5.9537e-02,  5.3008e-03,  1.6029e-02,  1.2912e-01,  1.7749e-02,\n",
      "          -2.1948e-03],\n",
      "         ...,\n",
      "         [ 9.5954e-02,  1.7656e-01,  1.3830e-02, -1.7332e-01, -4.9133e-02,\n",
      "           8.7466e-03],\n",
      "         [ 1.6388e-01,  1.3931e-01,  3.6172e-02, -1.6408e-01, -4.1609e-02,\n",
      "           1.9685e-02],\n",
      "         [ 2.2031e-01,  1.1850e-01,  3.2531e-02, -1.5569e-01, -3.1441e-02,\n",
      "           2.5508e-02]],\n",
      "\n",
      "        [[ 5.4695e-01, -2.6921e-02,  2.5749e-01,  1.5326e-01, -2.3977e-01,\n",
      "          -2.2483e-01],\n",
      "         [ 7.1426e-01,  4.0404e-02,  2.7667e-01,  1.4127e-01, -2.1876e-01,\n",
      "          -2.0030e-01],\n",
      "         [ 7.6847e-01, -6.6332e-02,  5.5004e-01,  1.1408e-01, -1.7112e-01,\n",
      "          -1.4479e-01],\n",
      "         ...,\n",
      "         [ 2.5176e+00, -7.8178e-01,  6.7975e-01, -1.0716e-01, -6.6033e-01,\n",
      "          -1.2044e-01],\n",
      "         [ 2.7012e+00,  6.8949e-01,  8.8043e-01, -9.7911e-02, -7.7297e-01,\n",
      "          -1.8617e-01],\n",
      "         [ 2.4549e+00,  3.3596e-01,  8.4524e-01, -6.8176e-02, -1.0523e+00,\n",
      "          -3.5211e-01]],\n",
      "\n",
      "        [[-3.9071e-01,  1.2413e-01, -8.8056e-01, -9.9602e-02,  1.7461e-01,\n",
      "          -3.4042e-02],\n",
      "         [-6.5633e-01,  1.1208e-01, -1.1296e+00, -1.6309e-01,  9.4097e-02,\n",
      "           8.6506e-03],\n",
      "         [-7.9460e-01, -1.9522e-02, -9.0204e-01, -1.7767e-01, -3.8456e-04,\n",
      "           2.5252e-02],\n",
      "         ...,\n",
      "         [ 4.3858e-02, -2.1268e-02, -7.7017e-02, -3.4486e-02, -1.2901e-02,\n",
      "           1.4714e-02],\n",
      "         [ 7.7938e-02, -3.5515e-02, -5.6370e-02, -3.5245e-02, -6.7806e-04,\n",
      "           1.3120e-02],\n",
      "         [ 1.0729e-01, -6.4150e-02, -2.0664e-03, -3.0436e-02,  1.6149e-02,\n",
      "           1.2649e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.2003e-03,  2.1938e-01, -1.1168e+00, -1.1650e-01,  9.9504e-01,\n",
      "           2.2379e-01],\n",
      "         [-2.6674e-01,  5.8102e-01, -4.4507e-01, -7.5017e-02,  1.0381e+00,\n",
      "           2.7614e-01],\n",
      "         [-2.6072e-01,  6.4099e-01,  4.6592e-02, -3.6402e-02,  1.0852e+00,\n",
      "           2.8835e-01],\n",
      "         ...,\n",
      "         [-5.8554e+00, -4.3981e-01,  4.4594e+00,  2.9242e-01,  4.9001e-01,\n",
      "           2.6805e-01],\n",
      "         [-2.8762e+00, -1.3209e+00,  9.8197e-01,  3.4867e-01,  5.1286e-02,\n",
      "           1.2858e-01],\n",
      "         [-1.5266e+00, -1.2279e+00, -1.6120e+00,  2.1829e-01, -3.5631e-01,\n",
      "          -6.5572e-02]],\n",
      "\n",
      "        [[ 2.0372e-02, -2.7492e-03,  8.5677e-02,  2.5045e-03,  2.5363e-02,\n",
      "          -1.9548e-04],\n",
      "         [ 2.0855e-02,  9.1800e-03,  1.1540e-01,  6.1084e-05,  3.0250e-02,\n",
      "          -8.0635e-04],\n",
      "         [ 2.6965e-03,  1.2943e-02,  1.0677e-01, -2.3824e-03,  3.5748e-02,\n",
      "          -1.4172e-03],\n",
      "         ...,\n",
      "         [ 1.9227e-02,  5.9135e-02,  1.8128e-01,  8.6132e-03, -8.8453e-03,\n",
      "          -3.2498e-03],\n",
      "         [-2.2992e-03,  3.6738e-02,  1.0235e-01,  6.1697e-03, -1.1900e-02,\n",
      "          -3.8607e-03],\n",
      "         [-2.6513e-02, -6.1113e-03,  1.7460e-02,  8.0023e-03, -1.5565e-02,\n",
      "          -1.4172e-03]],\n",
      "\n",
      "        [[-1.4266e+00, -1.8026e+00, -3.6801e+00, -2.2417e-01, -7.3925e-01,\n",
      "          -4.2919e-01],\n",
      "         [-1.4955e+00, -6.5226e-01, -2.7034e+00, -1.9489e-01, -1.0094e+00,\n",
      "          -2.8839e-01],\n",
      "         [-1.1113e+00,  9.2475e-01, -1.5925e+00, -1.0843e-01, -1.3207e+00,\n",
      "          -5.2253e-02],\n",
      "         ...,\n",
      "         [-3.5574e+00, -1.8054e+00, -4.9620e+00,  4.0436e-01,  3.5207e-01,\n",
      "          -6.1963e-01],\n",
      "         [-3.3658e+00, -2.6499e+00, -6.1743e+00,  2.4835e-01, -2.3146e-01,\n",
      "          -7.3487e-01],\n",
      "         [-3.4957e+00, -3.1291e+00, -7.3364e+00,  6.7136e-02, -6.4540e-01,\n",
      "          -7.4241e-01]]]), tensor([[ 2],\n",
      "        [ 9],\n",
      "        [ 2],\n",
      "        [ 2],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [ 3],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [10],\n",
      "        [12],\n",
      "        [11],\n",
      "        [ 1],\n",
      "        [13],\n",
      "        [ 1],\n",
      "        [10],\n",
      "        [ 5],\n",
      "        [10],\n",
      "        [ 3],\n",
      "        [11],\n",
      "        [ 7],\n",
      "        [15],\n",
      "        [ 4],\n",
      "        [11],\n",
      "        [ 6],\n",
      "        [17],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [ 5],\n",
      "        [12]]), tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]]), [9656, 20727, 10070, 10145, 12698, 17504, 11554, 1137, 2843, 3226, 4420, 5820, 5072, 1967, 6181, 2859, 4049, 16412, 4443, 10970, 5320, 18408, 7232, 13426, 4889, 16757, 8529, 14702, 17312, 4013, 15708, 5679]]\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 15:05:54,737 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 0  93.2% | batch:        55 of        59\t|\tloss: 16.9195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\mvts_transformer-master\\src\\running.py:479: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.softmax(predictions)  # (total_samples, num_classes) est. prob. for each class and sample\n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "2022-09-15 15:06:07,141 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 12.399709701538086 seconds\n",
      "\n",
      "2022-09-15 15:06:07,143 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 12.399709701538086 seconds\n",
      "2022-09-15 15:06:07,146 | INFO : Avg batch val. time: 0.21016457121250992 seconds\n",
      "2022-09-15 15:06:07,148 | INFO : Avg sample val. time: 0.006637960225662787 seconds\n",
      "2022-09-15 15:06:07,152 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 1373.838953 | accuracy: 0.036403 | precision: 0.046809 | \n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\numpy\\lib\\npyio.py:696: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n",
      "2022-09-15 15:06:07,228 | INFO : Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "----------------\n",
      "|   True\\Pred |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |   10 |   11 |   12 |   13 |   14 |   15 |   16 |   17 |\n",
      "|-------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------|\n",
      "|           0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    3 |    0 |    0 |    0 |    0 |  167 |    0 |    0 |\n",
      "|           1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |    0 |    0 |    0 |    0 |  169 |    0 |    0 |\n",
      "|           2 |   0 |   0 |   0 |   0 |   4 |   0 |   0 |  12 |   0 |   0 |   35 |    0 |    0 |    7 |    2 |  101 |    0 |    1 |\n",
      "|           3 |   4 |   0 |   0 |   1 |  11 |   0 |   2 |  40 |   0 |   4 |   56 |    0 |    0 |   12 |   10 |   20 |    4 |    4 |\n",
      "|           4 |  12 |   0 |   0 |   3 |   6 |   0 |   2 |  31 |   2 |   6 |   63 |    0 |    0 |    7 |   23 |   31 |    4 |    6 |\n",
      "|           5 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   1 |   1 |    1 |    0 |    0 |    0 |    0 |  160 |    0 |    0 |\n",
      "|           6 |   6 |   0 |   0 |   2 |   8 |   0 |   3 |  34 |   0 |  10 |   55 |    0 |    0 |    7 |    9 |   12 |    4 |    9 |\n",
      "|           7 |  15 |   0 |   2 |   3 |   9 |   0 |   1 |  14 |   0 |   8 |   26 |    0 |    0 |   11 |   14 |    8 |    5 |    4 |\n",
      "|           8 |   1 |   0 |   0 |  12 |   5 |   0 |   0 |  10 |   0 |   0 |    5 |    0 |    0 |    1 |    4 |   21 |    0 |    1 |\n",
      "|           9 |   4 |   0 |   0 |   2 |   9 |   0 |   1 |   1 |   0 |   1 |   10 |    0 |    0 |    8 |    1 |    3 |    2 |    1 |\n",
      "|          10 |  10 |   0 |   0 |   2 |   3 |   0 |   5 |  16 |   0 |   7 |   26 |    0 |    0 |    2 |    9 |    4 |    4 |    3 |\n",
      "|          11 |  11 |   0 |   0 |   3 |  11 |   0 |   1 |  13 |   0 |   0 |   19 |    0 |    0 |    5 |    0 |   12 |    3 |    1 |\n",
      "|          12 |   2 |   0 |   0 |   4 |   1 |   0 |   0 |   6 |   0 |   0 |    4 |    0 |    0 |    0 |    2 |    9 |    0 |    0 |\n",
      "|          13 |   0 |   0 |   1 |   3 |   2 |   0 |   1 |   5 |   0 |   2 |    3 |    0 |    0 |    1 |    0 |    3 |    0 |    2 |\n",
      "|          14 |   4 |   0 |   2 |  11 |   0 |   0 |   2 |   9 |   0 |   1 |   10 |    0 |    0 |    0 |    4 |    8 |    2 |    1 |\n",
      "|          15 |   1 |   0 |   1 |   2 |   5 |   0 |   0 |  10 |   0 |   2 |   18 |    0 |    0 |    9 |   11 |    9 |    4 |    0 |\n",
      "|          16 |   9 |   0 |   1 |   5 |   6 |   0 |   1 |  10 |   0 |   0 |    9 |    0 |    0 |    3 |    3 |   19 |    3 |    1 |\n",
      "|          17 |   9 |   0 |   3 |   5 |   3 |   0 |   0 |   3 |   0 |   4 |    4 |    0 |    0 |    0 |    3 |    5 |    2 |    0 |\n",
      "\n",
      "\n",
      "Confusion matrix normalized by row\n",
      "----------------------------------\n",
      "|   True\\Pred |         0 |   1 |         2 |          3 |         4 |   5 |          6 |         7 |          8 |          9 |         10 |   11 |   12 |        13 |        14 |        15 |        16 |         17 |\n",
      "|-------------+-----------+-----+-----------+------------+-----------+-----+------------+-----------+------------+------------+------------+------+------+-----------+-----------+-----------+-----------+------------|\n",
      "|           0 | 0         |   0 | 0         | 0          | 0         |   0 | 0          | 0         | 0          | 0          | 0.0176471  |    0 |    0 | 0         | 0         | 0.982353  | 0         | 0          |\n",
      "|           1 | 0         |   0 | 0         | 0          | 0         |   0 | 0          | 0         | 0          | 0          | 0          |    0 |    0 | 0         | 0         | 1         | 0         | 0          |\n",
      "|           2 | 0         |   0 | 0         | 0          | 0.0246914 |   0 | 0          | 0.0740741 | 0          | 0          | 0.216049   |    0 |    0 | 0.0432099 | 0.0123457 | 0.623457  | 0         | 0.00617284 |\n",
      "|           3 | 0.0238095 |   0 | 0         | 0.00595238 | 0.0654762 |   0 | 0.0119048  | 0.238095  | 0          | 0.0238095  | 0.333333   |    0 |    0 | 0.0714286 | 0.0595238 | 0.119048  | 0.0238095 | 0.0238095  |\n",
      "|           4 | 0.0612245 |   0 | 0         | 0.0153061  | 0.0306122 |   0 | 0.0102041  | 0.158163  | 0.0102041  | 0.0306122  | 0.321429   |    0 |    0 | 0.0357143 | 0.117347  | 0.158163  | 0.0204082 | 0.0306122  |\n",
      "|           5 | 0         |   0 | 0         | 0          | 0         |   0 | 0          | 0         | 0.00613497 | 0.00613497 | 0.00613497 |    0 |    0 | 0         | 0         | 0.981595  | 0         | 0          |\n",
      "|           6 | 0.0377358 |   0 | 0         | 0.0125786  | 0.0503145 |   0 | 0.0188679  | 0.213836  | 0          | 0.0628931  | 0.345912   |    0 |    0 | 0.0440252 | 0.0566038 | 0.0754717 | 0.0251572 | 0.0566038  |\n",
      "|           7 | 0.125     |   0 | 0.0166667 | 0.025      | 0.075     |   0 | 0.00833333 | 0.116667  | 0          | 0.0666667  | 0.216667   |    0 |    0 | 0.0916667 | 0.116667  | 0.0666667 | 0.0416667 | 0.0333333  |\n",
      "|           8 | 0.0166667 |   0 | 0         | 0.2        | 0.0833333 |   0 | 0          | 0.166667  | 0          | 0          | 0.0833333  |    0 |    0 | 0.0166667 | 0.0666667 | 0.35      | 0         | 0.0166667  |\n",
      "|           9 | 0.0930233 |   0 | 0         | 0.0465116  | 0.209302  |   0 | 0.0232558  | 0.0232558 | 0          | 0.0232558  | 0.232558   |    0 |    0 | 0.186047  | 0.0232558 | 0.0697674 | 0.0465116 | 0.0232558  |\n",
      "|          10 | 0.10989   |   0 | 0         | 0.021978   | 0.032967  |   0 | 0.0549451  | 0.175824  | 0          | 0.0769231  | 0.285714   |    0 |    0 | 0.021978  | 0.0989011 | 0.043956  | 0.043956  | 0.032967   |\n",
      "|          11 | 0.139241  |   0 | 0         | 0.0379747  | 0.139241  |   0 | 0.0126582  | 0.164557  | 0          | 0          | 0.240506   |    0 |    0 | 0.0632911 | 0         | 0.151899  | 0.0379747 | 0.0126582  |\n",
      "|          12 | 0.0714286 |   0 | 0         | 0.142857   | 0.0357143 |   0 | 0          | 0.214286  | 0          | 0          | 0.142857   |    0 |    0 | 0         | 0.0714286 | 0.321429  | 0         | 0          |\n",
      "|          13 | 0         |   0 | 0.0434783 | 0.130435   | 0.0869565 |   0 | 0.0434783  | 0.217391  | 0          | 0.0869565  | 0.130435   |    0 |    0 | 0.0434783 | 0         | 0.130435  | 0         | 0.0869565  |\n",
      "|          14 | 0.0740741 |   0 | 0.037037  | 0.203704   | 0         |   0 | 0.037037   | 0.166667  | 0          | 0.0185185  | 0.185185   |    0 |    0 | 0         | 0.0740741 | 0.148148  | 0.037037  | 0.0185185  |\n",
      "|          15 | 0.0138889 |   0 | 0.0138889 | 0.0277778  | 0.0694444 |   0 | 0          | 0.138889  | 0          | 0.0277778  | 0.25       |    0 |    0 | 0.125     | 0.152778  | 0.125     | 0.0555556 | 0          |\n",
      "|          16 | 0.128571  |   0 | 0.0142857 | 0.0714286  | 0.0857143 |   0 | 0.0142857  | 0.142857  | 0          | 0          | 0.128571   |    0 |    0 | 0.0428571 | 0.0428571 | 0.271429  | 0.0428571 | 0.0142857  |\n",
      "|          17 | 0.219512  |   0 | 0.0731707 | 0.121951   | 0.0731707 |   0 | 0          | 0.0731707 | 0          | 0.097561   | 0.097561   |    0 |    0 | 0         | 0.0731707 | 0.121951  | 0.0487805 | 0          |\n",
      "\n",
      "\n",
      "Overall accuracy: 0.036\n",
      "\n",
      "              precision     recall   f1-score rel. freq. abs. freq.|\t biggest thieves\n",
      "\n",
      "          4       0.072      0.031      0.043      0.105        196|\t 10: 0.321,\t15: 0.158,\t7: 0.158,\t\n",
      "          0       0.000      0.000      0.000      0.091        170|\t 15: 0.982,\t10: 0.018,\t17: 0.000,\t\n",
      "          1       0.000      0.000      0.000      0.090        169|\t 15: 1.000,\t17: 0.000,\t7: 0.000,\t\n",
      "          3       0.017      0.006      0.009      0.090        168|\t 10: 0.333,\t7: 0.238,\t15: 0.119,\t\n",
      "          5       0.000      0.000      0.000      0.087        163|\t 15: 0.982,\t8: 0.006,\t10: 0.006,\t\n",
      "          2       0.000      0.000      0.000      0.087        162|\t 15: 0.623,\t10: 0.216,\t7: 0.074,\t\n",
      "          6       0.158      0.019      0.034      0.085        159|\t 10: 0.346,\t7: 0.214,\t15: 0.075,\t\n",
      "          7       0.065      0.117      0.084      0.064        120|\t 10: 0.217,\t0: 0.125,\t14: 0.117,\t\n",
      "         10       0.075      0.286      0.119      0.049         91|\t 7: 0.176,\t0: 0.110,\t\n",
      "         11       0.000      0.000      0.000      0.042         79|\t 10: 0.241,\t7: 0.165,\t15: 0.152,\t\n",
      "         15       0.012      0.125      0.022      0.039         72|\t 10: 0.250,\t14: 0.153,\t7: 0.139,\t\n",
      "         16       0.081      0.043      0.056      0.037         70|\t 15: 0.271,\t7: 0.143,\t0: 0.129,\t\n",
      "          8       0.000      0.000      0.000      0.032         60|\t 15: 0.350,\t3: 0.200,\t7: 0.167,\t\n",
      "         14       0.042      0.074      0.054      0.029         54|\t 3: 0.204,\t10: 0.185,\t7: 0.167,\t\n",
      "          9       0.022      0.023      0.022      0.023         43|\t 10: 0.233,\t4: 0.209,\t13: 0.186,\t\n",
      "         17       0.000      0.000      0.000      0.022         41|\t 0: 0.220,\t15: 0.122,\t3: 0.122,\t\n",
      "         12       0.000      0.000      0.000      0.015         28|\t 15: 0.321,\t7: 0.214,\t3: 0.143,\t\n",
      "         13       0.014      0.043      0.021      0.012         23|\t 7: 0.217,\t15: 0.130,\t3: 0.130,\t\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "avg / total       0.036      0.036      0.025        1.0       1868|\t      \n",
      "\n",
      "\n",
      "\n",
      "Distribution of classes with respect to PRECISION: \n",
      "[0.0, 0.2): 18\n",
      "[0.2, 0.4): 0\n",
      "[0.4, 0.6): 0\n",
      "[0.6, 0.7): 0\n",
      "[0.7, 0.8): 0\n",
      "[0.8, 0.9): 0\n",
      "[0.9, 1.0): 0\n",
      "[1.0, 1.1): 0\n",
      "\n",
      "\n",
      "Distribution of classes with respect to RECALL: \n",
      "[0.0, 0.2): 17\n",
      "[0.2, 0.4): 1\n",
      "[0.4, 0.6): 0\n",
      "[0.6, 0.7): 0\n",
      "[0.7, 0.8): 0\n",
      "[0.8, 0.9): 0\n",
      "[0.9, 1.0): 0\n",
      "[1.0, 1.1): 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 1   0.0% | batch:         0 of       526\t|\tloss: 2.99805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\mvts_transformer-master\\src\\optimizers.py:69: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1174.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 1  76.0% | batch:       400 of       526\t|\tloss: 1.779993"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation before training\n",
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                        best_value, epoch=0)\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))\n",
    "\n",
    "logger.info('Starting training...')\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                best_metrics, best_value, epoch)\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    # Difficulty scheduling\n",
    "    if config['harden'] and check_progress(epoch):\n",
    "        train_loader.dataset.update()\n",
    "        val_loader.dataset.update()\n",
    "\n",
    "# Export evolution of metrics over epochs\n",
    "header = metrics_names\n",
    "metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "# Export record metrics to a file accumulating records from all experiments\n",
    "utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "                        best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "logger.info('All Done!')\n",
    "\n",
    "total_runtime = time.time() - total_start_time\n",
    "logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
