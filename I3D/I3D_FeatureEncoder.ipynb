{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iH7043iKpWSx",
    "outputId": "cae29ecf-1d36-4afe-e849-fb91f24c08f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Mon_Sep_13_20:11:50_Pacific_Daylight_Time_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.50\n",
      "Build cuda_11.5.r11.5/compiler.30411180_0\n",
      "gcc (MinGW.org GCC-6.3.0-1) 6.3.0\n",
      "Copyright (C) 2016 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BC6u24fkpaL8",
    "outputId": "d060b4af-2523-4a4a-974f-3c52216aede4"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116 True\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmaction'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39m__version__, torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available())\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Check MMAction2 installation\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmmaction\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(mmaction\u001B[38;5;241m.\u001B[39m__version__)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Check MMCV installation\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'mmaction'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from mmaction.apis import init_recognizer\n",
    "\n",
    "# Choose to use a config and initialize the recognizer\n",
    "config = '../mmaction2/configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "# Initialize the recognizer\n",
    "model = init_recognizer(config, checkpoint, device='cuda:0')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2er4P2npfo5",
    "outputId": "82a9f825-8086-48fb-fc5c-e699f83a097a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs4LW_noJF_Q",
    "outputId": "86868e25-84b2-48d8-dd02-cd276a58ed9a"
   },
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.cls_head.fc_cls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfvi2SPiJF2E",
    "outputId": "0fa33767-10aa-411b-c5e4-10e2ea0fc9b6"
   },
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=2048, out_features=400, bias=True)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import re\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmcv.parallel import collate, scatter\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from mmaction.core import OutputHook\n",
    "from mmaction.datasets.pipelines import Compose\n",
    "from mmaction.models import build_recognizer"
   ],
   "metadata": {
    "id": "_JKfEgHlLmDo"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def inference_recognizer(model, video, outputs=None, as_tensor=True, **kwargs):\n",
    "    \"\"\"Inference a video with the recognizer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The loaded recognizer.\n",
    "        video (str | dict | ndarray): The video file path / url or the\n",
    "            rawframes directory path / results dictionary (the input of\n",
    "            pipeline) / a 4D array T x H x W x 3 (The input video).\n",
    "        outputs (list(str) | tuple(str) | str | None) : Names of layers whose\n",
    "            outputs need to be returned, default: None.\n",
    "        as_tensor (bool): Same as that in ``OutputHook``. Default: True.\n",
    "\n",
    "    Returns:\n",
    "        dict[tuple(str, float)]: Top-5 recognition result dict.\n",
    "        dict[torch.tensor | np.ndarray]:\n",
    "            Output feature maps from layers specified in `outputs`.\n",
    "    \"\"\"\n",
    "    if 'use_frames' in kwargs:\n",
    "        warnings.warn('The argument `use_frames` is deprecated PR #1191. '\n",
    "                      'Now you can use models trained with frames or videos '\n",
    "                      'arbitrarily. ')\n",
    "    if 'label_path' in kwargs:\n",
    "        warnings.warn('The argument `use_frames` is deprecated PR #1191. '\n",
    "                      'Now the label file is not needed in '\n",
    "                      'inference_recognizer. ')\n",
    "\n",
    "    input_flag = None\n",
    "    if isinstance(video, dict):\n",
    "        input_flag = 'dict'\n",
    "    elif isinstance(video, np.ndarray):\n",
    "        assert len(video.shape) == 4, 'The shape should be T x H x W x C'\n",
    "        input_flag = 'array'\n",
    "    elif isinstance(video, str) and video.startswith('http'):\n",
    "        input_flag = 'video'\n",
    "    elif isinstance(video, str) and osp.exists(video):\n",
    "        if osp.isfile(video):\n",
    "            if video.endswith('.npy'):\n",
    "                input_flag = 'audio'\n",
    "            else:\n",
    "                input_flag = 'video'\n",
    "        if osp.isdir(video):\n",
    "            input_flag = 'rawframes'\n",
    "    else:\n",
    "        raise RuntimeError('The type of argument video is not supported: '\n",
    "                           f'{type(video)}')\n",
    "\n",
    "    if isinstance(outputs, str):\n",
    "        outputs = (outputs, )\n",
    "    assert outputs is None or isinstance(outputs, (tuple, list))\n",
    "\n",
    "    cfg = model.cfg\n",
    "    device = next(model.parameters()).device  # model device\n",
    "    # build the data pipeline\n",
    "    test_pipeline = cfg.data.test.pipeline\n",
    "    # Alter data pipelines & prepare inputs\n",
    "    if input_flag == 'dict':\n",
    "        data = video\n",
    "    if input_flag == 'array':\n",
    "        modality_map = {2: 'Flow', 3: 'RGB'}\n",
    "        modality = modality_map.get(video.shape[-1])\n",
    "        data = dict(\n",
    "            total_frames=video.shape[0],\n",
    "            label=-1,\n",
    "            start_index=0,\n",
    "            array=video,\n",
    "            modality=modality)\n",
    "        for i in range(len(test_pipeline)):\n",
    "            if 'Decode' in test_pipeline[i]['type']:\n",
    "                test_pipeline[i] = dict(type='ArrayDecode')\n",
    "        test_pipeline = [x for x in test_pipeline if 'Init' not in x['type']]\n",
    "    if input_flag == 'video':\n",
    "        data = dict(filename=video, label=-1, start_index=0, modality='RGB')\n",
    "        if 'Init' not in test_pipeline[0]['type']:\n",
    "            test_pipeline = [dict(type='OpenCVInit')] + test_pipeline\n",
    "        else:\n",
    "            test_pipeline[0] = dict(type='OpenCVInit')\n",
    "        for i in range(len(test_pipeline)):\n",
    "            if 'Decode' in test_pipeline[i]['type']:\n",
    "                test_pipeline[i] = dict(type='OpenCVDecode')\n",
    "    if input_flag == 'rawframes':\n",
    "        filename_tmpl = cfg.data.test.get('filename_tmpl', 'img_{:05}.jpg')\n",
    "        modality = cfg.data.test.get('modality', 'RGB')\n",
    "        start_index = cfg.data.test.get('start_index', 1)\n",
    "\n",
    "        # count the number of frames that match the format of `filename_tmpl`\n",
    "        # RGB pattern example: img_{:05}.jpg -> ^img_\\d+.jpg$\n",
    "        # Flow patteren example: {}_{:05d}.jpg -> ^x_\\d+.jpg$\n",
    "        pattern = f'^{filename_tmpl}$'\n",
    "        if modality == 'Flow':\n",
    "            pattern = pattern.replace('{}', 'x')\n",
    "        pattern = pattern.replace(\n",
    "            pattern[pattern.find('{'):pattern.find('}') + 1], '\\\\d+')\n",
    "        total_frames = len(\n",
    "            list(\n",
    "                filter(lambda x: re.match(pattern, x) is not None,\n",
    "                       os.listdir(video))))\n",
    "        data = dict(\n",
    "            frame_dir=video,\n",
    "            total_frames=total_frames,\n",
    "            label=-1,\n",
    "            start_index=start_index,\n",
    "            filename_tmpl=filename_tmpl,\n",
    "            modality=modality)\n",
    "        if 'Init' in test_pipeline[0]['type']:\n",
    "            test_pipeline = test_pipeline[1:]\n",
    "        for i in range(len(test_pipeline)):\n",
    "            if 'Decode' in test_pipeline[i]['type']:\n",
    "                test_pipeline[i] = dict(type='RawFrameDecode')\n",
    "    if input_flag == 'audio':\n",
    "        data = dict(\n",
    "            audio_path=video,\n",
    "            total_frames=len(np.load(video)),\n",
    "            start_index=cfg.data.test.get('start_index', 1),\n",
    "            label=-1)\n",
    "\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    data = test_pipeline(data)\n",
    "    data = collate([data], samples_per_gpu=1)\n",
    "\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        # scatter to specified GPU\n",
    "        data = scatter(data, [device])[0]\n",
    "\n",
    "    # forward the model\n",
    "    # with OutputHook(model, outputs=outputs, as_tensor=as_tensor) as h:\n",
    "    # model.cls_head.register_forward_hook(get_activation('dropout'))\n",
    "    # with torch.no_grad():\n",
    "    #     result = model(return_loss=False, **data)\n",
    "    with torch.no_grad():\n",
    "        my_output = None\n",
    "        \n",
    "        def my_hook(module_, input_, output_):\n",
    "            nonlocal my_output\n",
    "            my_output = output_\n",
    "\n",
    "        #a_hook = model.backbone.layer3.register_forward_hook(my_hook)\n",
    "        a_hook = model.cls_head.dropout.register_forward_hook(my_hook)\n",
    "        model(return_loss=False, **data)\n",
    "        a_hook.remove()\n",
    "        # return my_output\n",
    "    #     returned_features = h.layer_outputs if outputs else None\n",
    "\n",
    "    # num_classes = scores.shape[-1]\n",
    "    # score_tuples = tuple(zip(range(num_classes), scores))\n",
    "    # score_sorted = sorted(score_tuples, key=itemgetter(1), reverse=True)\n",
    "\n",
    "    # top5_label = score_sorted[:5]\n",
    "    # if outputs:\n",
    "    #     return top5_label, returned_features\n",
    "    return my_output.squeeze()"
   ],
   "metadata": {
    "id": "s55EEbjSLl5Y"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use the recognizer to do inference\n",
    "video = '../data/nipun_video_dataset/PAMAP2_K10_V1/ascending stairs/as9.mp4'\n",
    "label = '../mmaction2/tools/data/kinetics/label_map_k400.txt'\n",
    "results = inference_recognizer(model, video)\n",
    "\n",
    "# print(results)\n",
    "# labels = open(label).readlines()\n",
    "# labels = [x.strip() for x in labels]\n",
    "# results = [(labels[k[0]], k[1]) for k in results]"
   ],
   "metadata": {
    "id": "6HTnYMeCphzh"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "type(results), results.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erxGMdl-OZ09",
    "outputId": "730be3ce-6306-4301-f1cf-bdd81099fcea"
   },
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Tensor, torch.Size([250, 1024, 14, 14]))"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "id": "t2NRXKdlPqsM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os \n",
    "import glob\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "C1CdnlJVRNzk"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dir_path = '../data/nipun_video_dataset/PAMAP2_K10_V1/'"
   ],
   "metadata": {
    "id": "D64mw7BkQW1Z"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "video_ft_dict = {}\n",
    "for p in tqdm(glob.glob(dir_path+'*/*.mp4',recursive=True)):\n",
    "  action = p.split(os.path.sep)[-2]\n",
    "  ft_vector = inference_recognizer(model, p).cpu().numpy()\n",
    "  try:\n",
    "    video_ft_dict[action].append(ft_vector)\n",
    "  except KeyError:\n",
    "    video_ft_dict[action] = [ft_vector]\n",
    "    \n",
    "\"\"\"\n",
    "for p in glob.glob(dir_path+'*/*.avi',recursive=True):\n",
    "  action = p.split(os.path.sep)[-2]\n",
    "  ft_vector = inference_recognizer(model, p).cpu().numpy()\n",
    "  try:\n",
    "    video_ft_dict[action].append(ft_vector)\n",
    "  except:\n",
    "    video_ft_dict[action] = [ft_vector]\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "4GeTO-MvShBa"
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [15:12<00:00,  5.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\nfor p in glob.glob(dir_path+'*/*.avi',recursive=True):\\n  action = p.split(os.path.sep)[-2]\\n  ft_vector = inference_recognizer(model, p).cpu().numpy()\\n  try:\\n    video_ft_dict[action].append(ft_vector)\\n  except:\\n    video_ft_dict[action] = [ft_vector]\\n\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "video_ft_dict.keys()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jydh_o8Yr3E",
    "outputId": "e59b8eb8-d266-4519-fff6-c75b6db41216"
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['ascending stairs', 'car driving', 'computer work', 'cycling', 'descending stairs', 'folding laundry', 'house cleaning', 'ironing', 'lying', 'Nordic walking', 'playing soccer', 'rope jumping', 'running', 'sitting', 'standing', 'vacuum cleaning', 'walking', 'watching TV'])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "feat_dict = video_ft_dict"
   ],
   "metadata": {
    "id": "ILS4Rm5SYYUU"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def save_file(feat_d:dict,save_loc:str):\n",
    "    __class = []\n",
    "    __features = []\n",
    "    for k,v in feat_d.items():\n",
    "        for feature in v:\n",
    "            __class.append(k)\n",
    "            __features.append(feature)\n",
    "\n",
    "    __class = np.asarray(__class)\n",
    "    __features = np.asarray(__features)\n",
    "\n",
    "    np.savez(save_loc,activity=__class,features=__features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "save_file(feat_dict,\"../data/I3D/video_feat/PAMAP2_K10_V1/feat_dict_400.npz\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
