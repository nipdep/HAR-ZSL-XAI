{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.data import PAMAP2Reader\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "\n",
    "from src.models.ts_transformer import RelativeGlobalAttention\n",
    "from src.models.loss import FeatureLoss, AttributeLoss \n",
    "from src.utils.losses import SupConLoss\n",
    "\n",
    "from src.utils.analysis import action_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/FYP-Group22/Skel-E2E/e/SKEL-5\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"FYP-Group22/Skel-E2E\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"test-001\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"RelativeGlobalAttention\",\n",
    "    \"sem-space\": 'attr',\n",
    "    # model training configs\n",
    "    \"include_attribute_loss\": True, \n",
    "    \"semantic_size\": 64,\n",
    "    \"n_actions\": 18,\n",
    "    \"folding\": True,\n",
    "    \"lr\": 0.001,\n",
    "    \"ae_lr\": 0.0001,\n",
    "    \"imu_lr\": 0.0005,\n",
    "    \"ae_alpha\": 0.7,\n",
    "    \"imu_alpha\": 0.5,\n",
    "    \"n_epochs\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 32,\n",
    "    \"semantic_loss\": \"cosine_distance\",\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    \"feat_size\": 64, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 5, \n",
    "    \"overlap\": 0.5,\n",
    "    \"seq_len\": 50,  # skeleton seq. length\n",
    "    \"seen_split\": 0.2,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['parameters'] = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6a(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, n_classes, max_len=1024, dropout=0.1):\n",
    "        super(Model6a, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu  # _get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.logist = nn.Linear(self.ft_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out1 = self.DenseL2(out)   \n",
    "        out = self.logist(out1)     \n",
    "        return out, out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32, device=device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32)\n",
    "prep_dir = './tmp/random_input_100_epochs.pt'\n",
    "\n",
    "ae_model.load_state_dict(torch.load(prep_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\Pose-AE\\src\\datasets\\data.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "# import PAMAP2 dataset\n",
    "dt = config['dataset']\n",
    "if dt == 'PAMAP2':\n",
    "    dataReader = PAMAP2Reader('./data/PAMAP2_Dataset/Protocol/')\n",
    "\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dt == 'PAMAP2':\n",
    "    fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_data = np.load('./data/skeleton_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'lying'),\n",
       " (2, 'sitting'),\n",
       " (3, 'standing'),\n",
       " (4, 'walking'),\n",
       " (5, 'running'),\n",
       " (6, 'cycling'),\n",
       " (7, 'Nordic walking'),\n",
       " (9, 'watching TV'),\n",
       " (10, 'computer work'),\n",
       " (11, 'car driving'),\n",
       " (12, 'ascending stairs'),\n",
       " (13, 'descending stairs'),\n",
       " (16, 'vacuum cleaning'),\n",
       " (17, 'ironing'),\n",
       " (18, 'folding laundry'),\n",
       " (19, 'house cleaning'),\n",
       " (20, 'playing soccer'),\n",
       " (24, 'rope jumping')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nordic walking', 'ascending stairs', 'car driving',\n",
       "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
       "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
       "       'rope jumping', 'running', 'sitting', 'standing',\n",
       "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(skeleton_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5],\n",
       "             9: [6, 7, 8, 9],\n",
       "             8: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             5: [20, 21, 22, 23, 24],\n",
       "             11: [25, 26, 27, 28, 29, 30],\n",
       "             14: [31, 32, 33, 34, 35, 36, 37],\n",
       "             15: [38, 39, 40, 41, 42, 43, 44],\n",
       "             13: [45, 46, 47, 48, 49, 50],\n",
       "             0: [51, 52],\n",
       "             6: [53, 54, 55, 56, 57, 58, 59],\n",
       "             16: [60, 61, 62, 63, 64],\n",
       "             17: [65, 66],\n",
       "             4: [67, 68, 69, 70, 71],\n",
       "             1: [72, 73, 74, 75, 76, 77],\n",
       "             2: [78, 79, 80, 81, 82],\n",
       "             12: [83, 84, 85, 86, 87, 88],\n",
       "             3: [89, 90, 91, 92],\n",
       "             7: [93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(fold, phase, metrics, model):\n",
    "    for m, v in metrics.items():\n",
    "        if fold == 'global':\n",
    "            run[f'{model}/global/{m}'].log(v)\n",
    "        else:\n",
    "            run[f\"{model}/Fold-{fold}/{phase}/{m}\"].log(v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAMAP2Dataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, attribute_dict, action_classes, seq_len=120):\n",
    "        super(PAMAP2Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attribute_dict = attribute_dict\n",
    "        self.seq_len = seq_len\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_classes = action_classes\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        # extraction semantic space generation skeleton sequences \n",
    "        skel_idx = random.choice(self.attribute_dict[target])\n",
    "        y_feat = self.attributes[skel_idx, ...]\n",
    "        return x, y, y_feat, x_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def getClassAttrs(self):\n",
    "        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n",
    "        ft_mat = self.attributes[sampling_idx, ...]\n",
    "        return ft_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 105, 27])\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=fold_cls_ids[0], seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "sample_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=data_dict['seen_classes'], seq_len=100)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for d in sample_dl:\n",
    "    print(d[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 60, 36])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sample_dt.getClassAttrs()\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60, 36])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = F.pad(input=r, pad=(0, 0, 0, 0, 0, 18), mode='constant', value=0)\n",
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(r[1, ...], 1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_out1, r_out2 = ae_model(torch.transpose(pr, 1, 0).float())\n",
    "# r_out1.shape, r_out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, movements, actions, action_dict, seq_len=60):\n",
    "        super(SkeletonDataset, self).__init__()\n",
    "        self.movements = movements[:, :seq_len, ...]\n",
    "        self.actions = actions\n",
    "        self.action_dict = deepcopy(dict(action_dict))\n",
    "        self.actionsIDs = list(self.action_dict.keys())\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.movements[idx, ...]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        partial_idx = random.sample(self.action_dict[action], k=1)[0]\n",
    "        x2 = self.movements[partial_idx, ...]\n",
    "\n",
    "        label = self.actionsIDs.index(action)\n",
    "        # return np.transpose(x1, (1,0,2)), np.transpose(x2, (1,0,2)), label\n",
    "        return x1, x2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.movements.shape[0]\n",
    "\n",
    "    def getShape(self):\n",
    "        return self.movements[0, ...].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5],\n",
       "             9: [6, 7, 8, 9],\n",
       "             8: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             5: [20, 21, 22, 23, 24],\n",
       "             11: [25, 26, 27, 28, 29, 30],\n",
       "             14: [31, 32, 33, 34, 35, 36, 37],\n",
       "             15: [38, 39, 40, 41, 42, 43, 44],\n",
       "             13: [45, 46, 47, 48, 49, 50],\n",
       "             0: [51, 52],\n",
       "             6: [53, 54, 55, 56, 57, 58, 59],\n",
       "             16: [60, 61, 62, 63, 64],\n",
       "             17: [65, 66],\n",
       "             4: [67, 68, 69, 70, 71],\n",
       "             1: [72, 73, 74, 75, 76, 77],\n",
       "             2: [78, 79, 80, 81, 82],\n",
       "             12: [83, 84, 85, 86, 87, 88],\n",
       "             3: [89, 90, 91, 92],\n",
       "             7: [93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 36])\n"
     ]
    }
   ],
   "source": [
    "sample_dt = SkeletonDataset(skeleton_mov, skeleton_Ids, action_dict, seq_len=50)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "for b in sample_dl:\n",
    "    bx1, bx2, by = b \n",
    "    # bx = torch.transpose(bx1, 1, 0)\n",
    "    bs, seq_len, ft_in = bx1.shape\n",
    "    print(bx1.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 36)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dt.getShape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imu_train_step1(model , ae, dataloader, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    ae = ae.eval()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, padding_masks = batch\n",
    "            # print(X, targets, target_feat, target_attr)\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "            \n",
    "            with torch.set_grad_enabled(False):\n",
    "                _, target_feat = ae(target_feat) # batch first mode\n",
    "\n",
    "            class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "            feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            pred_class = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "            \n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step1(model , dataloader, optimizer, loss_module, device, batch_size, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0 \n",
    "    total_samples = 0 \n",
    "\n",
    "    with tqdm(dataloader, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            x1, x2, labels = batch \n",
    "            # post-process \n",
    "            xa = torch.vstack([x1, x2])  # batch first mode\n",
    "            # device offload \n",
    "            xa = xa.float().to(device)\n",
    "            labels = labels.float()\n",
    "\n",
    "            # set optimizer grad to zero \n",
    "            optimizer.zero_grad()\n",
    "            # get model prediction \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                skel_output, ft_output = model(xa)\n",
    "\n",
    "            # reconstruct the output \n",
    "            f1, f2 = torch.split(ft_output, [batch_size//2, batch_size//2], dim=0)\n",
    "            cons_output = torch.stack([f1.squeeze(1), f2.squeeze(1)], dim=1)\n",
    "            # calc. contrastive loss \n",
    "            con_loss = loss_module['contrast'](cons_output, labels)\n",
    "            # calc. reconstruction loss \n",
    "            l2_loss = loss_module['recons'](xa, skel_output)\n",
    "            # calc. total loss\n",
    "            total_loss = loss_alpha*con_loss + (1-loss_alpha)*l2_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {'contrastive loss ': con_loss.item(), 'reconstruction loss': l2_loss.item()}\n",
    "            with torch.no_grad():\n",
    "                    total_samples += len(labels)\n",
    "                    epoch_loss += total_loss.item()\n",
    "            \n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss/total_samples\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step1(model, ae, dataloader, loss_module, device, class_names, target_feat_met, phase='seen', l2_reg=False, print_report=True, show_plot=True, loss_alpha=0.7, batch_size=32):\n",
    "    model = model.train()\n",
    "    ae = ae.eval()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "\n",
    "    # generate unseen action_semantic from unseen skeleton seq. \n",
    "    ns, _, _ = target_feat_met.shape \n",
    "    padded_mat = F.pad(input=target_feat_met, pad=(0,0,0,0,0,batch_size-ns), mode='constant', value=0)\n",
    "    _, vector_out = ae(padded_mat.float().to(device)) # batch second mode\n",
    "    action_feat_mat = vector_out[:ns, :].cpu().detach().numpy() # batch second mode\n",
    "\n",
    "    # print(\"target feat mat\", target_feat_met.shap e, \"action feat mat\", action_feat_mat.shape)\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, padding_masks = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                _, target_feat = ae(target_feat) # batch second mode\n",
    "                \n",
    "            class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "            feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            if phase == 'seen':\n",
    "                pred_action = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            else:\n",
    "                # print(\"feat output\", feat_output.shape)\n",
    "                feat_numpy = torch.sigmoid(feat_output.cpu().detach())\n",
    "                action_probs = cosine_similarity(feat_numpy, action_feat_mat)\n",
    "                pred_action = np.argmax(action_probs, axis=1)\n",
    "                \n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action)\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Initiate IMU datasets ...\n",
      "Initiate Skeleton dataset ... \n",
      "Initial Models ...\n",
      "Start Models training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 6/6 [00:01<00:00,  5.08batch/s, contrastive loss =3.37, reconstruction loss=0.102]\n",
      "train:   8%|▊         | 44/523 [00:04<00:51,  9.25batch/s, loss=0.995, accuracy=0.719]\n",
      "                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m log(i, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, ae_train_metrics, \u001b[39m'\u001b[39m\u001b[39mskel-ae\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# train IMU-Enc model\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m train_metrics \u001b[39m=\u001b[39m imu_train_step1(imu_model, ae_model, train_dl, imu_optim, imu_loss_module, device, class_names\u001b[39m=\u001b[39;49m[all_classes[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m seen_classes], phase\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, loss_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m)\n\u001b[0;32m     61\u001b[0m log(i, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, train_metrics, \u001b[39m'\u001b[39m\u001b[39mimu-enc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m eval_metrics \u001b[39m=\u001b[39m eval_step1(imu_model, ae_model, eval_dl, imu_loss_module, device\u001b[39m=\u001b[39mdevice, class_names\u001b[39m=\u001b[39m[all_classes[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m seen_classes],  target_feat_met\u001b[39m=\u001b[39meval_dt\u001b[39m.\u001b[39mgetClassAttrs(), phase\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mseen\u001b[39m\u001b[39m'\u001b[39m, print_report\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, loss_alpha\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], show_plot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [28], line 34\u001b[0m, in \u001b[0;36mimu_train_step1\u001b[1;34m(model, ae, dataloader, optimizer, loss_module, device, class_names, phase, l2_reg, loss_alpha)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[39m=\u001b[39m loss_alpha\u001b[39m*\u001b[39mclass_loss\u001b[39m+\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mloss_alpha)\u001b[39m*\u001b[39mfeat_loss\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     37\u001b[0m metrics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: loss\u001b[39m.\u001b[39mitem()}\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prep_dir = './tmp/epoch50_randsample_input_mseloss.pt'\n",
    "\n",
    "fold_metric_scores = []\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    # print(f'Unseen Classes : {fold_classes[i]}')\n",
    "    # separate seen/unseen and train/eval \n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "\n",
    "    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt \n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    print(\"Initiate Skeleton dataset ... \")\n",
    "    # build Skeleton dataset \n",
    "    skel_dt = SkeletonDataset(skeleton_mov, skeleton_Ids, action_dict, seq_len=config['seq_len'])\n",
    "    skel_dl = DataLoader(skel_dt, batch_size=config['batch_size']//2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    skel_n, skel_fts = skel_dt.getShape()\n",
    "    \n",
    "    print(\"Initial Models ...\")\n",
    "    # build IMU Encoder Model \n",
    "    imu_model = Model6a(in_ft=in_ft, d_model=config['d_model'], num_heads=config['num_heads'], ft_size=config['feat_size'], max_len=seq_len, n_classes=len(seen_classes))\n",
    "    imu_model.to(device)\n",
    "\n",
    "    # build AE Model \n",
    "    # ae_model = BiLSTMEncDecModel(input_size=skel_fts, seq_len=skel_n, hidden_size=config['feat_size'], batch_size=config['batch_size'], ae_type='recursive', device=device)\n",
    "    ae_model = BiLSTMEncDecModel(seq_len=skel_n, input_size=skel_fts, hidden_size=512, linear_filters=[128,256,512], embedding_size=config['feat_size'], num_layers=1, bidirectional=True, batch_size=config['batch_size'], device=device)\n",
    "    # ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32, device=device)\n",
    "    # ae_model.load_state_dict(torch.load(prep_dir))\n",
    "    ae_model.to(device)\n",
    "    \n",
    "    # define IMU-Enc run parameters \n",
    "    imu_optim = Adam(imu_model.parameters(), lr=config['imu_lr'])\n",
    "    imu_loss_module = {'class': nn.CrossEntropyLoss(), 'feature': AttributeLoss()}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # define AE run parameters \n",
    "    ae_optim = Adam(ae_model.parameters(), lr=config['ae_lr'])\n",
    "    ae_loss_module = {'contrast': SupConLoss(), 'recons': nn.MSELoss()}\n",
    "\n",
    "    print(\"Start Models training ...\")\n",
    "    # train 2 models E2E\n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        # train AE model \n",
    "        ae_train_metrics = ae_train_step1(ae_model, skel_dl, ae_optim, ae_loss_module, device, config['batch_size'], phase='train', loss_alpha=config['ae_alpha'])\n",
    "        log(i, 'train', ae_train_metrics, 'skel-ae')\n",
    "        # train IMU-Enc model\n",
    "        train_metrics = imu_train_step1(imu_model, ae_model, train_dl, imu_optim, imu_loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=config['imu_alpha'])\n",
    "        log(i, 'train', train_metrics, 'imu-enc')\n",
    "        eval_metrics = eval_step1(imu_model, ae_model, eval_dl, imu_loss_module, device=device, class_names=[all_classes[i] for i in seen_classes],  target_feat_met=eval_dt.getClassAttrs(), phase='seen', print_report=False, loss_alpha=config['imu_alpha'], batch_size=config['batch_size'], show_plot=False)\n",
    "        log(i, 'seen-eval', train_metrics, 'imu-enc')\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['total_accuracy'] > best_acc:\n",
    "            imu_best_model = deepcopy(imu_model.state_dict())\n",
    "\n",
    "    # replace by best model \n",
    "    imu_model.load_state_dict(imu_best_model)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step1(imu_model, ae_model, test_dl, imu_loss_module, device=device, class_names=[all_classes[i] for i in unseen_classes],  target_feat_met=test_dt.getClassAttrs(), phase='unseen', loss_alpha=0.8, batch_size=config['batch_size'])\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    log('test', i, test_metrics, 'imu-enc')\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "seen_score_df.mean()\n",
    "log('global', '', seen_score_df.mean().to_dict(), 'imu-enc')\n",
    "run.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
