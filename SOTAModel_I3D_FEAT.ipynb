{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmaking Experiment 006b\n",
    "- benchmarking model architecture -3 with PAMAP2, DaLiAc and UTD-MHAD datasets\n",
    "- Model : Model4\n",
    "- Dataset : PAMAP2\n",
    "- Semantic Space : Glove50\n",
    "- Cross Validation : 5-fold fixed classes\n",
    "- Feature Loss : MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import numpy.random as random\n",
    "from src.datasets.data import PAMAP2ReaderV2\n",
    "from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.utils.analysis import action_evaluator\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "\n",
    "# from src.running import train_step1, eval_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"test-001\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\",\n",
    "    \"Model\": \"SOTAEmbedding\",\n",
    "    \"model_params\":{\n",
    "        \"linear_filters\":[1024,1024,1024,2048],\n",
    "        \"input_feat\":36,\n",
    "        \"dropout\":0.1,\n",
    "    },\n",
    "    \"folding\": True,\n",
    "    \"lr\": 0.0001,\n",
    "    \"n_epochs\": 15,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 64,\n",
    "    \"seen_split\": 0.2,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model,model_name,unique_name,fold_id):\n",
    "    PATH = f\"{models_saves}/{model_name}\"\n",
    "    os.makedirs(PATH,exist_ok=True)\n",
    "    torch.save({\n",
    "        \"n_epochs\" : config[\"n_epochs\"],\n",
    "        \"model_state_dict\":model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, f\"{PATH}/{unique_name}_{fold_id}.pt\")\n",
    "\n",
    "model_iden = \"fold\"\n",
    "notebook_iden = \"SORTModel_feature\"\n",
    "models_saves = \"model_saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FYP\\HAR-ZSL-XAI\\src\\datasets\\data.py:300: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13 16 17 18 19 20 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2ReaderV2('data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOTAEmbedding(nn.Module):\n",
    "    def __init__(self, linear_filters=[1024,1024,1024,2048],input_feat=36, dropout=0.1):\n",
    "        super(SOTAEmbedding, self).__init__()\n",
    "        self.input_feat = input_feat\n",
    "        self.linear_filters = linear_filters\n",
    "        self.input_feat = input_feat\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.linear1 = nn.Linear(input_feat,linear_filters[0])\n",
    "        self.batch_norm1 = nn.BatchNorm1d(linear_filters[0])\n",
    "        self.linear2 = nn.Linear(linear_filters[0],linear_filters[1])\n",
    "        self.batch_norm2 = nn.BatchNorm1d(linear_filters[1])\n",
    "        self.linear3 = nn.Linear(linear_filters[1],linear_filters[2])\n",
    "        self.batch_norm3 = nn.BatchNorm1d(linear_filters[2])\n",
    "        self.linear4 = nn.Linear(linear_filters[2]+linear_filters[1]+linear_filters[0],linear_filters[3])\n",
    "        self.batch_norm4 = nn.BatchNorm1d(linear_filters[3])\n",
    "        self.act = F.relu\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #layer1\n",
    "        out1 = self.linear1(x)\n",
    "        out1 = self.batch_norm1(out1)\n",
    "        out1 = self.act(out1)\n",
    "\n",
    "        #layer2\n",
    "        out2 = self.linear2(out1)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.act(out2)\n",
    "\n",
    "        #layer3\n",
    "        out3 = self.linear3(out2)\n",
    "        out3 = self.batch_norm3(out3)\n",
    "        out3 = self.act(out3)\n",
    "\n",
    "        concat = torch.cat([out1,out2,out3],-1)\n",
    "\n",
    "        #layer4\n",
    "        out4 = self.linear4(concat)\n",
    "        out4 = self.batch_norm4(out4)\n",
    "        out4 = self.act(out4)\n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SOTAEmbedding(\n",
    "    linear_filters=config[\"model_params\"][\"linear_filters\"],\n",
    "    input_feat=config[\"model_params\"][\"input_feat\"],\n",
    "    dropout=config[\"model_params\"][\"dropout\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = model(torch.randn((32,36)))\n",
    "emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2241, 0.9915, 0.0000,  ..., 0.0000, 0.0000, 0.3455],\n",
       "        [0.0000, 1.1017, 0.5840,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.8130, 0.6843, 0.4609,  ..., 0.5032, 0.7259, 0.0000],\n",
       "        ...,\n",
       "        [0.1859, 0.0000, 1.7942,  ..., 0.6233, 2.5662, 0.0000],\n",
       "        [1.2305, 0.3311, 0.0000,  ..., 2.5456, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.4241,  ..., 0.0000, 0.0000, 0.0778]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = PAMAP2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_data = np.load('data/I3D/video_feat/PAMAP2_K10_V1/feat_dict_2048.npz')\n",
    "video_classes, video_feat = video_data['activity'], video_data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 2048)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
       "        'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
       "        'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
       "        'rope jumping', 'running', 'sitting', 'standing',\n",
       "        'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'),\n",
       " array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(video_classes, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vid_cls_name = np.unique(video_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def selecting_video_prototypes(prototypes:np.array,classes:np.array,vid_class_name:np.array):\n",
    "    selected = []\n",
    "    for tar in vid_class_name:\n",
    "        indexes = np.where(classes == tar)\n",
    "        selected.append(torch.from_numpy(prototypes[random.choice(indexes[0])]))\n",
    "\n",
    "    return torch.stack(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 2048])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selecting_video_prototypes(video_feat,video_classes,vid_cls_name).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'lying'),\n",
       " (2, 'sitting'),\n",
       " (3, 'standing'),\n",
       " (4, 'walking'),\n",
       " (5, 'running'),\n",
       " (6, 'cycling'),\n",
       " (7, 'Nordic walking'),\n",
       " (9, 'watching TV'),\n",
       " (10, 'computer work'),\n",
       " (11, 'car driving'),\n",
       " (12, 'ascending stairs'),\n",
       " (13, 'descending stairs'),\n",
       " (16, 'vacuum cleaning'),\n",
       " (17, 'ironing'),\n",
       " (18, 'folding laundry'),\n",
       " (19, 'house cleaning'),\n",
       " (20, 'playing soccer'),\n",
       " (24, 'rope jumping')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nordic walking', 'ascending stairs', 'car driving',\n",
       "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
       "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
       "       'rope jumping', 'running', 'sitting', 'standing',\n",
       "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(video_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(video_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Main\\envs\\CurrentSOTA\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=fold_cls_ids[0], seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'eval-seen', 'test', 'seen_classes', 'unseen_classes'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16752, 36), (4189, 36), (5608, 36))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape,data_dict['eval-seen']['X'].shape,data_dict[\"test\"][\"X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26549"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape[0]+data_dict['eval-seen']['X'].shape[0]+data_dict[\"test\"][\"X\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['seen_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=data_dict['seen_classes'], seq_len=100)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for d in sample_dl:\n",
    "    print(100*np.isinf(d[0].numpy()).sum()/(d[0].shape[0]*d[0].shape[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 3: 2,\n",
       " 4: 3,\n",
       " 5: 4,\n",
       " 6: 5,\n",
       " 8: 6,\n",
       " 9: 7,\n",
       " 11: 8,\n",
       " 12: 9,\n",
       " 13: 10,\n",
       " 14: 11,\n",
       " 16: 12,\n",
       " 17: 13}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dt.action2Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dt.attribute_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 2048])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sample_dt.getClassAttrs()\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_cross_entropy(\n",
    "        y_pred:torch.Tensor,\n",
    "        cls:torch.Tensor,\n",
    "        selected_features,\n",
    "        loss_fn=nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "):\n",
    "    num_classes = selected_features.size()[0]\n",
    "    #print(selected_features)\n",
    "    cosine_sim_comb = []\n",
    "    for entry in y_pred.unbind():\n",
    "        #print(entry.repeat(num_classes,1),selected_features.size())\n",
    "        cosine_sim = F.softmax(F.cosine_similarity(entry.repeat(num_classes,1),selected_features),dim=-1)\n",
    "        cosine_sim_comb.append(cosine_sim)\n",
    "\n",
    "    cosine_sim_comb = torch.stack(cosine_sim_comb)\n",
    "    loss = loss_fn(cosine_sim_comb,cls)\n",
    "    #print(loss)\n",
    "    return loss\n",
    "\n",
    "def predict_class(\n",
    "        y_pred:torch.Tensor,\n",
    "        selected_features):\n",
    "\n",
    "    num_classes = selected_features.size()[0]\n",
    "\n",
    "    cosine_sim_comb = []\n",
    "    for entry in y_pred.unbind():\n",
    "        cosine_sim = torch.argmax(F.softmax(F.cosine_similarity(entry.repeat(num_classes,1),selected_features),dim=-1))\n",
    "        cosine_sim_comb.append(cosine_sim)\n",
    "\n",
    "    pred = torch.stack(cosine_sim_comb)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_reconstruction_calc(y_pred:torch.Tensor,y_feat:torch.Tensor,loss_fn=nn.L1Loss(reduction=\"sum\")):\n",
    "    bat_size,feature_size = y_pred.size()\n",
    "    loss = loss_fn(y_pred,y_feat)*(1/feature_size)\n",
    "    #print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, dataset:PAMAP2Dataset, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    random_selected_feat = dataset.getClassFeatures().to(device)\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "                class_loss = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn =loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n",
    "\n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss\n",
    "            class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            pred_class = class_output.cpu().detach().numpy()\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader,dataset, loss_module, device, class_names,  phase='seen', l2_reg=False, print_report=True, loss_alpha=0.7):\n",
    "    model = model.eval()\n",
    "    random_selected_feat = dataset.getClassFeatures().to(device)\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat = batch\n",
    "            X = X.float().to(device)\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "                class_loss = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn =loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n",
    "\n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss\n",
    "            class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            if phase == 'seen':\n",
    "                pred_action = class_output\n",
    "            else:\n",
    "                #feat_numpy = torch.sigmoid(feat_output.cpu().detach())\n",
    "                #action_probs = cosine_similarity(feat_numpy, target_feat_met)\n",
    "                pred_action = class_output\n",
    "\n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action.cpu().numpy())\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n",
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n",
      "unseen classes >  [7, 15, 2, 10]\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "train:   0%|          | 0/261 [00:00<?, ?batch/s]\u001b[A\n",
      "train:   0%|          | 0/261 [00:09<?, ?batch/s, loss=30, accuracy=0.125]\u001b[A\n",
      "train:   0%|          | 1/261 [00:09<39:07,  9.03s/batch, loss=30, accuracy=0.125]\u001b[A\n",
      "train:   0%|          | 1/261 [00:09<39:07,  9.03s/batch, loss=28.6, accuracy=0.328]\u001b[A\n",
      "train:   1%|          | 2/261 [00:09<16:53,  3.92s/batch, loss=28.6, accuracy=0.328]\u001b[A\n",
      "train:   1%|          | 2/261 [00:09<16:53,  3.92s/batch, loss=28, accuracy=0.391]  \u001b[A\n",
      "train:   1%|          | 3/261 [00:09<09:48,  2.28s/batch, loss=28, accuracy=0.391]\u001b[A\n",
      "train:   1%|          | 3/261 [00:09<09:48,  2.28s/batch, loss=27.6, accuracy=0.531]\u001b[A\n",
      "train:   2%|▏         | 4/261 [00:09<06:21,  1.48s/batch, loss=27.6, accuracy=0.531]\u001b[A\n",
      "train:   2%|▏         | 4/261 [00:10<06:21,  1.48s/batch, loss=26.6, accuracy=0.625]\u001b[A\n",
      "train:   2%|▏         | 5/261 [00:10<04:23,  1.03s/batch, loss=26.6, accuracy=0.625]\u001b[A\n",
      "train:   2%|▏         | 5/261 [00:10<04:23,  1.03s/batch, loss=25.5, accuracy=0.578]\u001b[A\n",
      "train:   2%|▏         | 6/261 [00:10<03:06,  1.37batch/s, loss=25.5, accuracy=0.578]\u001b[A\n",
      "train:   2%|▏         | 6/261 [00:10<03:06,  1.37batch/s, loss=26.2, accuracy=0.625]\u001b[A\n",
      "train:   3%|▎         | 7/261 [00:10<02:17,  1.85batch/s, loss=26.2, accuracy=0.625]\u001b[A\n",
      "train:   3%|▎         | 7/261 [00:10<02:17,  1.85batch/s, loss=25, accuracy=0.672]  \u001b[A\n",
      "train:   3%|▎         | 8/261 [00:10<01:45,  2.40batch/s, loss=25, accuracy=0.672]\u001b[A\n",
      "train:   3%|▎         | 8/261 [00:10<01:45,  2.40batch/s, loss=24.6, accuracy=0.672]\u001b[A\n",
      "train:   3%|▎         | 9/261 [00:10<01:23,  3.00batch/s, loss=24.6, accuracy=0.672]\u001b[A\n",
      "train:   3%|▎         | 9/261 [00:10<01:23,  3.00batch/s, loss=23.3, accuracy=0.594]\u001b[A\n",
      "train:   4%|▍         | 10/261 [00:10<01:09,  3.62batch/s, loss=23.3, accuracy=0.594]\u001b[A\n",
      "train:   4%|▍         | 10/261 [00:11<01:09,  3.62batch/s, loss=23.1, accuracy=0.672]\u001b[A\n",
      "train:   4%|▍         | 11/261 [00:11<00:59,  4.22batch/s, loss=23.1, accuracy=0.672]\u001b[A\n",
      "train:   4%|▍         | 11/261 [00:11<00:59,  4.22batch/s, loss=22.1, accuracy=0.812]\u001b[A\n",
      "train:   5%|▍         | 12/261 [00:11<00:52,  4.73batch/s, loss=22.1, accuracy=0.812]\u001b[A\n",
      "train:   5%|▍         | 12/261 [00:11<00:52,  4.73batch/s, loss=21.8, accuracy=0.672]\u001b[A\n",
      "train:   5%|▍         | 13/261 [00:11<00:49,  5.01batch/s, loss=21.8, accuracy=0.672]\u001b[A\n",
      "train:   5%|▍         | 13/261 [00:11<00:49,  5.01batch/s, loss=21.8, accuracy=0.625]\u001b[A\n",
      "train:   5%|▌         | 14/261 [00:11<00:45,  5.37batch/s, loss=21.8, accuracy=0.625]\u001b[A\n",
      "train:   5%|▌         | 14/261 [00:11<00:45,  5.37batch/s, loss=21.6, accuracy=0.734]\u001b[A\n",
      "train:   6%|▌         | 15/261 [00:11<00:44,  5.56batch/s, loss=21.6, accuracy=0.734]\u001b[A\n",
      "train:   6%|▌         | 15/261 [00:11<00:44,  5.56batch/s, loss=20.8, accuracy=0.781]\u001b[A\n",
      "train:   6%|▌         | 16/261 [00:11<00:42,  5.79batch/s, loss=20.8, accuracy=0.781]\u001b[A\n",
      "train:   6%|▌         | 16/261 [00:12<00:42,  5.79batch/s, loss=21, accuracy=0.703]  \u001b[A\n",
      "train:   7%|▋         | 17/261 [00:12<00:40,  6.06batch/s, loss=21, accuracy=0.703]\u001b[A\n",
      "train:   7%|▋         | 17/261 [00:12<00:40,  6.06batch/s, loss=20.2, accuracy=0.641]\u001b[A\n",
      "train:   7%|▋         | 18/261 [00:12<00:39,  6.09batch/s, loss=20.2, accuracy=0.641]\u001b[A\n",
      "train:   7%|▋         | 18/261 [00:12<00:39,  6.09batch/s, loss=19.7, accuracy=0.688]\u001b[A\n",
      "train:   7%|▋         | 19/261 [00:12<00:39,  6.18batch/s, loss=19.7, accuracy=0.688]\u001b[A\n",
      "train:   7%|▋         | 19/261 [00:12<00:39,  6.18batch/s, loss=19.8, accuracy=0.688]\u001b[A\n",
      "train:   8%|▊         | 20/261 [00:12<00:38,  6.32batch/s, loss=19.8, accuracy=0.688]\u001b[A\n",
      "train:   8%|▊         | 20/261 [00:12<00:38,  6.32batch/s, loss=20.4, accuracy=0.781]\u001b[A\n",
      "train:   8%|▊         | 21/261 [00:12<00:37,  6.35batch/s, loss=20.4, accuracy=0.781]\u001b[A\n",
      "train:   8%|▊         | 21/261 [00:12<00:37,  6.35batch/s, loss=18.9, accuracy=0.828]\u001b[A\n",
      "train:   8%|▊         | 22/261 [00:12<00:37,  6.41batch/s, loss=18.9, accuracy=0.828]\u001b[A\n",
      "train:   8%|▊         | 22/261 [00:12<00:37,  6.41batch/s, loss=18, accuracy=0.734]  \u001b[A\n",
      "train:   9%|▉         | 23/261 [00:12<00:36,  6.55batch/s, loss=18, accuracy=0.734]\u001b[A\n",
      "train:   9%|▉         | 23/261 [00:13<00:36,  6.55batch/s, loss=18, accuracy=0.703]\u001b[A\n",
      "train:   9%|▉         | 24/261 [00:13<00:39,  6.03batch/s, loss=18, accuracy=0.703]\u001b[A\n",
      "train:   9%|▉         | 24/261 [00:13<00:39,  6.03batch/s, loss=17.3, accuracy=0.781]\u001b[A\n",
      "train:  10%|▉         | 25/261 [00:13<00:41,  5.72batch/s, loss=17.3, accuracy=0.781]\u001b[A\n",
      "train:  10%|▉         | 25/261 [00:13<00:41,  5.72batch/s, loss=18.5, accuracy=0.719]\u001b[A\n",
      "train:  10%|▉         | 26/261 [00:13<00:43,  5.38batch/s, loss=18.5, accuracy=0.719]\u001b[A\n",
      "train:  10%|▉         | 26/261 [00:13<00:43,  5.38batch/s, loss=16.7, accuracy=0.719]\u001b[A\n",
      "train:  10%|█         | 27/261 [00:13<00:42,  5.46batch/s, loss=16.7, accuracy=0.719]\u001b[A\n",
      "train:  10%|█         | 27/261 [00:13<00:42,  5.46batch/s, loss=17.3, accuracy=0.719]\u001b[A\n",
      "train:  11%|█         | 28/261 [00:13<00:42,  5.54batch/s, loss=17.3, accuracy=0.719]\u001b[A\n",
      "train:  11%|█         | 28/261 [00:14<00:42,  5.54batch/s, loss=17, accuracy=0.734]  \u001b[A\n",
      "train:  11%|█         | 29/261 [00:14<00:40,  5.73batch/s, loss=17, accuracy=0.734]\u001b[A\n",
      "train:  11%|█         | 29/261 [00:14<00:40,  5.73batch/s, loss=16.9, accuracy=0.672]\u001b[A\n",
      "train:  11%|█▏        | 30/261 [00:14<00:40,  5.65batch/s, loss=16.9, accuracy=0.672]\u001b[A\n",
      "train:  11%|█▏        | 30/261 [00:14<00:40,  5.65batch/s, loss=16.4, accuracy=0.719]\u001b[A\n",
      "train:  12%|█▏        | 31/261 [00:14<00:39,  5.82batch/s, loss=16.4, accuracy=0.719]\u001b[A\n",
      "train:  12%|█▏        | 31/261 [00:14<00:39,  5.82batch/s, loss=16.6, accuracy=0.812]\u001b[A\n",
      "train:  12%|█▏        | 32/261 [00:14<00:38,  5.99batch/s, loss=16.6, accuracy=0.812]\u001b[A\n",
      "train:  12%|█▏        | 32/261 [00:14<00:38,  5.99batch/s, loss=16.1, accuracy=0.828]\u001b[A\n",
      "train:  13%|█▎        | 33/261 [00:14<00:38,  5.96batch/s, loss=16.1, accuracy=0.828]\u001b[A\n",
      "train:  13%|█▎        | 33/261 [00:14<00:38,  5.96batch/s, loss=16.2, accuracy=0.797]\u001b[A\n",
      "train:  13%|█▎        | 34/261 [00:14<00:38,  5.95batch/s, loss=16.2, accuracy=0.797]\u001b[A\n",
      "train:  13%|█▎        | 34/261 [00:15<00:38,  5.95batch/s, loss=16.2, accuracy=0.625]\u001b[A\n",
      "train:  13%|█▎        | 35/261 [00:15<00:38,  5.82batch/s, loss=16.2, accuracy=0.625]\u001b[A\n",
      "train:  13%|█▎        | 35/261 [00:15<00:38,  5.82batch/s, loss=16.3, accuracy=0.828]\u001b[A\n",
      "train:  14%|█▍        | 36/261 [00:15<00:37,  5.99batch/s, loss=16.3, accuracy=0.828]\u001b[A\n",
      "train:  14%|█▍        | 36/261 [00:15<00:37,  5.99batch/s, loss=15.3, accuracy=0.656]\u001b[A\n",
      "train:  14%|█▍        | 37/261 [00:15<00:36,  6.12batch/s, loss=15.3, accuracy=0.656]\u001b[A\n",
      "train:  14%|█▍        | 37/261 [00:15<00:36,  6.12batch/s, loss=15.2, accuracy=0.797]\u001b[A\n",
      "train:  15%|█▍        | 38/261 [00:15<00:35,  6.25batch/s, loss=15.2, accuracy=0.797]\u001b[A\n",
      "train:  15%|█▍        | 38/261 [00:15<00:35,  6.25batch/s, loss=15.6, accuracy=0.734]\u001b[A\n",
      "train:  15%|█▍        | 39/261 [00:15<00:35,  6.27batch/s, loss=15.6, accuracy=0.734]\u001b[A\n",
      "train:  15%|█▍        | 39/261 [00:15<00:35,  6.27batch/s, loss=15.5, accuracy=0.672]\u001b[A\n",
      "train:  15%|█▌        | 40/261 [00:15<00:35,  6.22batch/s, loss=15.5, accuracy=0.672]\u001b[A\n",
      "train:  15%|█▌        | 40/261 [00:16<00:35,  6.22batch/s, loss=14.8, accuracy=0.719]\u001b[A\n",
      "train:  16%|█▌        | 41/261 [00:16<00:35,  6.29batch/s, loss=14.8, accuracy=0.719]\u001b[A\n",
      "train:  16%|█▌        | 41/261 [00:16<00:35,  6.29batch/s, loss=17.2, accuracy=0.703]\u001b[A\n",
      "train:  16%|█▌        | 42/261 [00:16<00:34,  6.35batch/s, loss=17.2, accuracy=0.703]\u001b[A\n",
      "train:  16%|█▌        | 42/261 [00:16<00:34,  6.35batch/s, loss=15.5, accuracy=0.719]\u001b[A\n",
      "train:  16%|█▋        | 43/261 [00:16<00:34,  6.29batch/s, loss=15.5, accuracy=0.719]\u001b[A\n",
      "train:  16%|█▋        | 43/261 [00:16<00:34,  6.29batch/s, loss=14.3, accuracy=0.75] \u001b[A\n",
      "train:  17%|█▋        | 44/261 [00:16<00:34,  6.33batch/s, loss=14.3, accuracy=0.75]\u001b[A\n",
      "train:  17%|█▋        | 44/261 [00:16<00:34,  6.33batch/s, loss=14.8, accuracy=0.781]\u001b[A\n",
      "train:  17%|█▋        | 45/261 [00:16<00:34,  6.33batch/s, loss=14.8, accuracy=0.781]\u001b[A\n",
      "train:  17%|█▋        | 45/261 [00:16<00:34,  6.33batch/s, loss=13.7, accuracy=0.672]\u001b[A\n",
      "train:  18%|█▊        | 46/261 [00:16<00:34,  6.32batch/s, loss=13.7, accuracy=0.672]\u001b[A\n",
      "train:  18%|█▊        | 46/261 [00:16<00:34,  6.32batch/s, loss=14.6, accuracy=0.719]\u001b[A\n",
      "train:  18%|█▊        | 47/261 [00:16<00:35,  6.05batch/s, loss=14.6, accuracy=0.719]\u001b[A\n",
      "train:  18%|█▊        | 47/261 [00:17<00:35,  6.05batch/s, loss=14.3, accuracy=0.625]\u001b[A\n",
      "train:  18%|█▊        | 48/261 [00:17<00:35,  6.06batch/s, loss=14.3, accuracy=0.625]\u001b[A\n",
      "train:  18%|█▊        | 48/261 [00:17<00:35,  6.06batch/s, loss=14.2, accuracy=0.766]\u001b[A\n",
      "train:  19%|█▉        | 49/261 [00:17<00:34,  6.12batch/s, loss=14.2, accuracy=0.766]\u001b[A\n",
      "train:  19%|█▉        | 49/261 [00:17<00:34,  6.12batch/s, loss=14.1, accuracy=0.656]\u001b[A\n",
      "train:  19%|█▉        | 50/261 [00:17<00:34,  6.19batch/s, loss=14.1, accuracy=0.656]\u001b[A\n",
      "train:  19%|█▉        | 50/261 [00:17<00:34,  6.19batch/s, loss=14.8, accuracy=0.703]\u001b[A\n",
      "train:  20%|█▉        | 51/261 [00:17<00:33,  6.20batch/s, loss=14.8, accuracy=0.703]\u001b[A\n",
      "train:  20%|█▉        | 51/261 [00:17<00:33,  6.20batch/s, loss=14.2, accuracy=0.734]\u001b[A\n",
      "train:  20%|█▉        | 52/261 [00:17<00:33,  6.29batch/s, loss=14.2, accuracy=0.734]\u001b[A\n",
      "train:  20%|█▉        | 52/261 [00:17<00:33,  6.29batch/s, loss=13.9, accuracy=0.719]\u001b[A\n",
      "train:  20%|██        | 53/261 [00:17<00:34,  5.99batch/s, loss=13.9, accuracy=0.719]\u001b[A\n",
      "train:  20%|██        | 53/261 [00:18<00:34,  5.99batch/s, loss=14.1, accuracy=0.656]\u001b[A\n",
      "train:  21%|██        | 54/261 [00:18<00:35,  5.88batch/s, loss=14.1, accuracy=0.656]\u001b[A\n",
      "train:  21%|██        | 54/261 [00:18<00:35,  5.88batch/s, loss=15.1, accuracy=0.672]\u001b[A\n",
      "train:  21%|██        | 55/261 [00:18<00:33,  6.07batch/s, loss=15.1, accuracy=0.672]\u001b[A\n",
      "train:  21%|██        | 55/261 [00:18<00:33,  6.07batch/s, loss=15.3, accuracy=0.703]\u001b[A\n",
      "train:  21%|██▏       | 56/261 [00:18<00:34,  5.99batch/s, loss=15.3, accuracy=0.703]\u001b[A\n",
      "train:  21%|██▏       | 56/261 [00:18<00:34,  5.99batch/s, loss=13.8, accuracy=0.734]\u001b[A\n",
      "train:  22%|██▏       | 57/261 [00:18<00:33,  6.08batch/s, loss=13.8, accuracy=0.734]\u001b[A\n",
      "train:  22%|██▏       | 57/261 [00:18<00:33,  6.08batch/s, loss=14.1, accuracy=0.766]\u001b[A\n",
      "train:  22%|██▏       | 58/261 [00:18<00:32,  6.19batch/s, loss=14.1, accuracy=0.766]\u001b[A\n",
      "train:  22%|██▏       | 58/261 [00:18<00:32,  6.19batch/s, loss=14, accuracy=0.766]  \u001b[A\n",
      "train:  23%|██▎       | 59/261 [00:18<00:33,  6.00batch/s, loss=14, accuracy=0.766]\u001b[A\n",
      "train:  23%|██▎       | 59/261 [00:19<00:33,  6.00batch/s, loss=13.2, accuracy=0.734]\u001b[A\n",
      "train:  23%|██▎       | 60/261 [00:19<00:33,  6.00batch/s, loss=13.2, accuracy=0.734]\u001b[A\n",
      "train:  23%|██▎       | 60/261 [00:19<00:33,  6.00batch/s, loss=14.5, accuracy=0.766]\u001b[A\n",
      "train:  23%|██▎       | 61/261 [00:19<00:34,  5.82batch/s, loss=14.5, accuracy=0.766]\u001b[A\n",
      "train:  23%|██▎       | 61/261 [00:19<00:34,  5.82batch/s, loss=13.6, accuracy=0.703]\u001b[A\n",
      "train:  24%|██▍       | 62/261 [00:19<00:33,  5.90batch/s, loss=13.6, accuracy=0.703]\u001b[A\n",
      "train:  24%|██▍       | 62/261 [00:19<00:33,  5.90batch/s, loss=14, accuracy=0.812]  \u001b[A\n",
      "train:  24%|██▍       | 63/261 [00:19<00:33,  5.92batch/s, loss=14, accuracy=0.812]\u001b[A\n",
      "train:  24%|██▍       | 63/261 [00:19<00:33,  5.92batch/s, loss=12.9, accuracy=0.75]\u001b[A\n",
      "train:  25%|██▍       | 64/261 [00:19<00:32,  6.15batch/s, loss=12.9, accuracy=0.75]\u001b[A\n",
      "train:  25%|██▍       | 64/261 [00:19<00:32,  6.15batch/s, loss=13.7, accuracy=0.766]\u001b[A\n",
      "train:  25%|██▍       | 65/261 [00:19<00:31,  6.17batch/s, loss=13.7, accuracy=0.766]\u001b[A\n",
      "train:  25%|██▍       | 65/261 [00:20<00:31,  6.17batch/s, loss=12.9, accuracy=0.75] \u001b[A\n",
      "train:  25%|██▌       | 66/261 [00:20<00:30,  6.30batch/s, loss=12.9, accuracy=0.75]\u001b[A\n",
      "train:  25%|██▌       | 66/261 [00:20<00:30,  6.30batch/s, loss=13.2, accuracy=0.75]\u001b[A\n",
      "train:  26%|██▌       | 67/261 [00:20<00:30,  6.38batch/s, loss=13.2, accuracy=0.75]\u001b[A\n",
      "train:  26%|██▌       | 67/261 [00:20<00:30,  6.38batch/s, loss=13.6, accuracy=0.734]\u001b[A\n",
      "train:  26%|██▌       | 68/261 [00:20<00:29,  6.48batch/s, loss=13.6, accuracy=0.734]\u001b[A\n",
      "train:  26%|██▌       | 68/261 [00:20<00:29,  6.48batch/s, loss=14.4, accuracy=0.766]\u001b[A\n",
      "train:  26%|██▋       | 69/261 [00:20<00:29,  6.41batch/s, loss=14.4, accuracy=0.766]\u001b[A\n",
      "train:  26%|██▋       | 69/261 [00:20<00:29,  6.41batch/s, loss=14, accuracy=0.859]  \u001b[A\n",
      "train:  27%|██▋       | 70/261 [00:20<00:30,  6.22batch/s, loss=14, accuracy=0.859]\u001b[A\n",
      "train:  27%|██▋       | 70/261 [00:20<00:30,  6.22batch/s, loss=13.4, accuracy=0.766]\u001b[A\n",
      "train:  27%|██▋       | 71/261 [00:20<00:31,  6.03batch/s, loss=13.4, accuracy=0.766]\u001b[A\n",
      "train:  27%|██▋       | 71/261 [00:21<00:31,  6.03batch/s, loss=13, accuracy=0.781]  \u001b[A\n",
      "train:  28%|██▊       | 72/261 [00:21<00:32,  5.78batch/s, loss=13, accuracy=0.781]\u001b[A\n",
      "train:  28%|██▊       | 72/261 [00:21<00:32,  5.78batch/s, loss=12.9, accuracy=0.828]\u001b[A\n",
      "train:  28%|██▊       | 73/261 [00:21<00:31,  5.88batch/s, loss=12.9, accuracy=0.828]\u001b[A\n",
      "train:  28%|██▊       | 73/261 [00:21<00:31,  5.88batch/s, loss=13.6, accuracy=0.812]\u001b[A\n",
      "train:  28%|██▊       | 74/261 [00:21<00:33,  5.60batch/s, loss=13.6, accuracy=0.812]\u001b[A\n",
      "train:  28%|██▊       | 74/261 [00:21<00:33,  5.60batch/s, loss=12.9, accuracy=0.734]\u001b[A\n",
      "train:  29%|██▊       | 75/261 [00:21<00:31,  5.81batch/s, loss=12.9, accuracy=0.734]\u001b[A\n",
      "train:  29%|██▊       | 75/261 [00:21<00:31,  5.81batch/s, loss=13.5, accuracy=0.688]\u001b[A\n",
      "train:  29%|██▉       | 76/261 [00:21<00:31,  5.80batch/s, loss=13.5, accuracy=0.688]\u001b[A\n",
      "train:  29%|██▉       | 76/261 [00:21<00:31,  5.80batch/s, loss=13.1, accuracy=0.812]\u001b[A\n",
      "train:  30%|██▉       | 77/261 [00:21<00:30,  5.96batch/s, loss=13.1, accuracy=0.812]\u001b[A\n",
      "train:  30%|██▉       | 77/261 [00:22<00:30,  5.96batch/s, loss=13.5, accuracy=0.703]\u001b[A\n",
      "train:  30%|██▉       | 78/261 [00:22<00:30,  5.97batch/s, loss=13.5, accuracy=0.703]\u001b[A\n",
      "train:  30%|██▉       | 78/261 [00:22<00:30,  5.97batch/s, loss=13.1, accuracy=0.797]\u001b[A\n",
      "train:  30%|███       | 79/261 [00:22<00:30,  5.94batch/s, loss=13.1, accuracy=0.797]\u001b[A\n",
      "train:  30%|███       | 79/261 [00:22<00:30,  5.94batch/s, loss=13.2, accuracy=0.766]\u001b[A\n",
      "train:  31%|███       | 80/261 [00:22<00:30,  5.99batch/s, loss=13.2, accuracy=0.766]\u001b[A\n",
      "train:  31%|███       | 80/261 [00:22<00:30,  5.99batch/s, loss=12.7, accuracy=0.719]\u001b[A\n",
      "train:  31%|███       | 81/261 [00:22<00:29,  6.06batch/s, loss=12.7, accuracy=0.719]\u001b[A\n",
      "train:  31%|███       | 81/261 [00:22<00:29,  6.06batch/s, loss=13.2, accuracy=0.781]\u001b[A\n",
      "train:  31%|███▏      | 82/261 [00:22<00:28,  6.18batch/s, loss=13.2, accuracy=0.781]\u001b[A\n",
      "train:  31%|███▏      | 82/261 [00:22<00:28,  6.18batch/s, loss=12.9, accuracy=0.734]\u001b[A\n",
      "train:  32%|███▏      | 83/261 [00:22<00:28,  6.15batch/s, loss=12.9, accuracy=0.734]\u001b[A\n",
      "train:  32%|███▏      | 83/261 [00:23<00:28,  6.15batch/s, loss=12.8, accuracy=0.812]\u001b[A\n",
      "train:  32%|███▏      | 84/261 [00:23<00:28,  6.19batch/s, loss=12.8, accuracy=0.812]\u001b[A\n",
      "train:  32%|███▏      | 84/261 [00:23<00:28,  6.19batch/s, loss=13.1, accuracy=0.812]\u001b[A\n",
      "train:  33%|███▎      | 85/261 [00:23<00:30,  5.85batch/s, loss=13.1, accuracy=0.812]\u001b[A\n",
      "train:  33%|███▎      | 85/261 [00:23<00:30,  5.85batch/s, loss=13.5, accuracy=0.781]\u001b[A\n",
      "train:  33%|███▎      | 86/261 [00:23<00:28,  6.05batch/s, loss=13.5, accuracy=0.781]\u001b[A\n",
      "train:  33%|███▎      | 86/261 [00:23<00:28,  6.05batch/s, loss=13.4, accuracy=0.75] \u001b[A\n",
      "train:  33%|███▎      | 87/261 [00:23<00:28,  6.12batch/s, loss=13.4, accuracy=0.75]\u001b[A\n",
      "train:  33%|███▎      | 87/261 [00:23<00:28,  6.12batch/s, loss=12.7, accuracy=0.75]\u001b[A\n",
      "train:  34%|███▎      | 88/261 [00:23<00:28,  6.11batch/s, loss=12.7, accuracy=0.75]\u001b[A\n",
      "train:  34%|███▎      | 88/261 [00:23<00:28,  6.11batch/s, loss=13.5, accuracy=0.844]\u001b[A\n",
      "train:  34%|███▍      | 89/261 [00:23<00:27,  6.19batch/s, loss=13.5, accuracy=0.844]\u001b[A\n",
      "train:  34%|███▍      | 89/261 [00:24<00:27,  6.19batch/s, loss=14.1, accuracy=0.797]\u001b[A\n",
      "train:  34%|███▍      | 90/261 [00:24<00:27,  6.31batch/s, loss=14.1, accuracy=0.797]\u001b[A\n",
      "train:  34%|███▍      | 90/261 [00:24<00:27,  6.31batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  35%|███▍      | 91/261 [00:24<00:27,  6.08batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  35%|███▍      | 91/261 [00:24<00:27,  6.08batch/s, loss=12.5, accuracy=0.734]\u001b[A\n",
      "train:  35%|███▌      | 92/261 [00:24<00:27,  6.13batch/s, loss=12.5, accuracy=0.734]\u001b[A\n",
      "train:  35%|███▌      | 92/261 [00:24<00:27,  6.13batch/s, loss=13, accuracy=0.781]  \u001b[A\n",
      "train:  36%|███▌      | 93/261 [00:24<00:27,  6.13batch/s, loss=13, accuracy=0.781]\u001b[A\n",
      "train:  36%|███▌      | 93/261 [00:24<00:27,  6.13batch/s, loss=12.9, accuracy=0.766]\u001b[A\n",
      "train:  36%|███▌      | 94/261 [00:24<00:27,  6.13batch/s, loss=12.9, accuracy=0.766]\u001b[A\n",
      "train:  36%|███▌      | 94/261 [00:24<00:27,  6.13batch/s, loss=12.6, accuracy=0.75] \u001b[A\n",
      "train:  36%|███▋      | 95/261 [00:24<00:26,  6.18batch/s, loss=12.6, accuracy=0.75]\u001b[A\n",
      "train:  36%|███▋      | 95/261 [00:25<00:26,  6.18batch/s, loss=13.1, accuracy=0.75]\u001b[A\n",
      "train:  37%|███▋      | 96/261 [00:25<00:26,  6.24batch/s, loss=13.1, accuracy=0.75]\u001b[A\n",
      "train:  37%|███▋      | 96/261 [00:25<00:26,  6.24batch/s, loss=12.8, accuracy=0.75]\u001b[A\n",
      "train:  37%|███▋      | 97/261 [00:25<00:26,  6.27batch/s, loss=12.8, accuracy=0.75]\u001b[A\n",
      "train:  37%|███▋      | 97/261 [00:25<00:26,  6.27batch/s, loss=12.7, accuracy=0.703]\u001b[A\n",
      "train:  38%|███▊      | 98/261 [00:25<00:26,  6.23batch/s, loss=12.7, accuracy=0.703]\u001b[A\n",
      "train:  38%|███▊      | 98/261 [00:25<00:26,  6.23batch/s, loss=12.8, accuracy=0.875]\u001b[A\n",
      "train:  38%|███▊      | 99/261 [00:25<00:29,  5.54batch/s, loss=12.8, accuracy=0.875]\u001b[A\n",
      "train:  38%|███▊      | 99/261 [00:25<00:29,  5.54batch/s, loss=14.1, accuracy=0.75] \u001b[A\n",
      "train:  38%|███▊      | 100/261 [00:25<00:29,  5.53batch/s, loss=14.1, accuracy=0.75]\u001b[A\n",
      "train:  38%|███▊      | 100/261 [00:25<00:29,  5.53batch/s, loss=13.2, accuracy=0.922]\u001b[A\n",
      "train:  39%|███▊      | 101/261 [00:25<00:27,  5.82batch/s, loss=13.2, accuracy=0.922]\u001b[A\n",
      "train:  39%|███▊      | 101/261 [00:26<00:27,  5.82batch/s, loss=12.3, accuracy=0.812]\u001b[A\n",
      "train:  39%|███▉      | 102/261 [00:26<00:27,  5.86batch/s, loss=12.3, accuracy=0.812]\u001b[A\n",
      "train:  39%|███▉      | 102/261 [00:26<00:27,  5.86batch/s, loss=12.9, accuracy=0.859]\u001b[A\n",
      "train:  39%|███▉      | 103/261 [00:26<00:26,  6.00batch/s, loss=12.9, accuracy=0.859]\u001b[A\n",
      "train:  39%|███▉      | 103/261 [00:26<00:26,  6.00batch/s, loss=12.3, accuracy=0.875]\u001b[A\n",
      "train:  40%|███▉      | 104/261 [00:26<00:25,  6.13batch/s, loss=12.3, accuracy=0.875]\u001b[A\n",
      "train:  40%|███▉      | 104/261 [00:26<00:25,  6.13batch/s, loss=12.8, accuracy=0.75] \u001b[A\n",
      "train:  40%|████      | 105/261 [00:26<00:25,  6.07batch/s, loss=12.8, accuracy=0.75]\u001b[A\n",
      "train:  40%|████      | 105/261 [00:26<00:25,  6.07batch/s, loss=12.5, accuracy=0.859]\u001b[A\n",
      "train:  41%|████      | 106/261 [00:26<00:25,  6.11batch/s, loss=12.5, accuracy=0.859]\u001b[A\n",
      "train:  41%|████      | 106/261 [00:26<00:25,  6.11batch/s, loss=12, accuracy=0.812]  \u001b[A\n",
      "train:  41%|████      | 107/261 [00:26<00:25,  6.04batch/s, loss=12, accuracy=0.812]\u001b[A\n",
      "train:  41%|████      | 107/261 [00:27<00:25,  6.04batch/s, loss=13.2, accuracy=0.797]\u001b[A\n",
      "train:  41%|████▏     | 108/261 [00:27<00:25,  6.02batch/s, loss=13.2, accuracy=0.797]\u001b[A\n",
      "train:  41%|████▏     | 108/261 [00:27<00:25,  6.02batch/s, loss=12.8, accuracy=0.781]\u001b[A\n",
      "train:  42%|████▏     | 109/261 [00:27<00:24,  6.12batch/s, loss=12.8, accuracy=0.781]\u001b[A\n",
      "train:  42%|████▏     | 109/261 [00:27<00:24,  6.12batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  42%|████▏     | 110/261 [00:27<00:24,  6.24batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  42%|████▏     | 110/261 [00:27<00:24,  6.24batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  43%|████▎     | 111/261 [00:27<00:24,  6.05batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  43%|████▎     | 111/261 [00:27<00:24,  6.05batch/s, loss=12.6, accuracy=0.844]\u001b[A\n",
      "train:  43%|████▎     | 112/261 [00:27<00:24,  6.20batch/s, loss=12.6, accuracy=0.844]\u001b[A\n",
      "train:  43%|████▎     | 112/261 [00:27<00:24,  6.20batch/s, loss=12.5, accuracy=0.859]\u001b[A\n",
      "train:  43%|████▎     | 113/261 [00:27<00:23,  6.30batch/s, loss=12.5, accuracy=0.859]\u001b[A\n",
      "train:  43%|████▎     | 113/261 [00:28<00:23,  6.30batch/s, loss=13.2, accuracy=0.812]\u001b[A\n",
      "train:  44%|████▎     | 114/261 [00:28<00:23,  6.31batch/s, loss=13.2, accuracy=0.812]\u001b[A\n",
      "train:  44%|████▎     | 114/261 [00:28<00:23,  6.31batch/s, loss=12.8, accuracy=0.734]\u001b[A\n",
      "train:  44%|████▍     | 115/261 [00:28<00:23,  6.22batch/s, loss=12.8, accuracy=0.734]\u001b[A\n",
      "train:  44%|████▍     | 115/261 [00:28<00:23,  6.22batch/s, loss=13.4, accuracy=0.734]\u001b[A\n",
      "train:  44%|████▍     | 116/261 [00:28<00:23,  6.27batch/s, loss=13.4, accuracy=0.734]\u001b[A\n",
      "train:  44%|████▍     | 116/261 [00:28<00:23,  6.27batch/s, loss=13.1, accuracy=0.797]\u001b[A\n",
      "train:  45%|████▍     | 117/261 [00:28<00:22,  6.36batch/s, loss=13.1, accuracy=0.797]\u001b[A\n",
      "train:  45%|████▍     | 117/261 [00:28<00:22,  6.36batch/s, loss=12.6, accuracy=0.812]\u001b[A\n",
      "train:  45%|████▌     | 118/261 [00:28<00:23,  6.19batch/s, loss=12.6, accuracy=0.812]\u001b[A\n",
      "train:  45%|████▌     | 118/261 [00:28<00:23,  6.19batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  46%|████▌     | 119/261 [00:28<00:23,  5.96batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  46%|████▌     | 119/261 [00:29<00:23,  5.96batch/s, loss=13.3, accuracy=0.828]\u001b[A\n",
      "train:  46%|████▌     | 120/261 [00:29<00:23,  5.92batch/s, loss=13.3, accuracy=0.828]\u001b[A\n",
      "train:  46%|████▌     | 120/261 [00:29<00:23,  5.92batch/s, loss=11.9, accuracy=0.781]\u001b[A\n",
      "train:  46%|████▋     | 121/261 [00:29<00:24,  5.71batch/s, loss=11.9, accuracy=0.781]\u001b[A\n",
      "train:  46%|████▋     | 121/261 [00:29<00:24,  5.71batch/s, loss=13.2, accuracy=0.875]\u001b[A\n",
      "train:  47%|████▋     | 122/261 [00:29<00:23,  5.85batch/s, loss=13.2, accuracy=0.875]\u001b[A\n",
      "train:  47%|████▋     | 122/261 [00:29<00:23,  5.85batch/s, loss=13.1, accuracy=0.828]\u001b[A\n",
      "train:  47%|████▋     | 123/261 [00:29<00:22,  6.08batch/s, loss=13.1, accuracy=0.828]\u001b[A\n",
      "train:  47%|████▋     | 123/261 [00:29<00:22,  6.08batch/s, loss=13.4, accuracy=0.891]\u001b[A\n",
      "train:  48%|████▊     | 124/261 [00:29<00:22,  6.12batch/s, loss=13.4, accuracy=0.891]\u001b[A\n",
      "train:  48%|████▊     | 124/261 [00:29<00:22,  6.12batch/s, loss=13.6, accuracy=0.781]\u001b[A\n",
      "train:  48%|████▊     | 125/261 [00:29<00:22,  6.17batch/s, loss=13.6, accuracy=0.781]\u001b[A\n",
      "train:  48%|████▊     | 125/261 [00:29<00:22,  6.17batch/s, loss=12.2, accuracy=0.875]\u001b[A\n",
      "train:  48%|████▊     | 126/261 [00:30<00:21,  6.34batch/s, loss=12.2, accuracy=0.875]\u001b[A\n",
      "train:  48%|████▊     | 126/261 [00:30<00:21,  6.34batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  49%|████▊     | 127/261 [00:30<00:20,  6.52batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  49%|████▊     | 127/261 [00:30<00:20,  6.52batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  49%|████▉     | 128/261 [00:30<00:20,  6.41batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  49%|████▉     | 128/261 [00:30<00:20,  6.41batch/s, loss=12.1, accuracy=0.891]\u001b[A\n",
      "train:  49%|████▉     | 129/261 [00:30<00:20,  6.33batch/s, loss=12.1, accuracy=0.891]\u001b[A\n",
      "train:  49%|████▉     | 129/261 [00:30<00:20,  6.33batch/s, loss=11.7, accuracy=0.75] \u001b[A\n",
      "train:  50%|████▉     | 130/261 [00:30<00:20,  6.31batch/s, loss=11.7, accuracy=0.75]\u001b[A\n",
      "train:  50%|████▉     | 130/261 [00:30<00:20,  6.31batch/s, loss=12.7, accuracy=0.797]\u001b[A\n",
      "train:  50%|█████     | 131/261 [00:30<00:20,  6.27batch/s, loss=12.7, accuracy=0.797]\u001b[A\n",
      "train:  50%|█████     | 131/261 [00:30<00:20,  6.27batch/s, loss=12, accuracy=0.797]  \u001b[A\n",
      "train:  51%|█████     | 132/261 [00:30<00:21,  5.89batch/s, loss=12, accuracy=0.797]\u001b[A\n",
      "train:  51%|█████     | 132/261 [00:31<00:21,  5.89batch/s, loss=12.4, accuracy=0.922]\u001b[A\n",
      "train:  51%|█████     | 133/261 [00:31<00:21,  6.00batch/s, loss=12.4, accuracy=0.922]\u001b[A\n",
      "train:  51%|█████     | 133/261 [00:31<00:21,  6.00batch/s, loss=12.3, accuracy=0.781]\u001b[A\n",
      "train:  51%|█████▏    | 134/261 [00:31<00:20,  6.09batch/s, loss=12.3, accuracy=0.781]\u001b[A\n",
      "train:  51%|█████▏    | 134/261 [00:31<00:20,  6.09batch/s, loss=13.6, accuracy=0.797]\u001b[A\n",
      "train:  52%|█████▏    | 135/261 [00:31<00:20,  6.07batch/s, loss=13.6, accuracy=0.797]\u001b[A\n",
      "train:  52%|█████▏    | 135/261 [00:31<00:20,  6.07batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  52%|█████▏    | 136/261 [00:31<00:20,  6.03batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  52%|█████▏    | 136/261 [00:31<00:20,  6.03batch/s, loss=12.4, accuracy=0.812]\u001b[A\n",
      "train:  52%|█████▏    | 137/261 [00:31<00:19,  6.21batch/s, loss=12.4, accuracy=0.812]\u001b[A\n",
      "train:  52%|█████▏    | 137/261 [00:31<00:19,  6.21batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  53%|█████▎    | 138/261 [00:31<00:19,  6.34batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  53%|█████▎    | 138/261 [00:32<00:19,  6.34batch/s, loss=12.3, accuracy=0.828]\u001b[A\n",
      "train:  53%|█████▎    | 139/261 [00:32<00:19,  6.23batch/s, loss=12.3, accuracy=0.828]\u001b[A\n",
      "train:  53%|█████▎    | 139/261 [00:32<00:19,  6.23batch/s, loss=12.2, accuracy=0.75] \u001b[A\n",
      "train:  54%|█████▎    | 140/261 [00:32<00:19,  6.34batch/s, loss=12.2, accuracy=0.75]\u001b[A\n",
      "train:  54%|█████▎    | 140/261 [00:32<00:19,  6.34batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  54%|█████▍    | 141/261 [00:32<00:18,  6.47batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  54%|█████▍    | 141/261 [00:32<00:18,  6.47batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  54%|█████▍    | 142/261 [00:32<00:18,  6.31batch/s, loss=12.2, accuracy=0.844]\u001b[A\n",
      "train:  54%|█████▍    | 142/261 [00:32<00:18,  6.31batch/s, loss=12.4, accuracy=0.938]\u001b[A\n",
      "train:  55%|█████▍    | 143/261 [00:32<00:18,  6.30batch/s, loss=12.4, accuracy=0.938]\u001b[A\n",
      "train:  55%|█████▍    | 143/261 [00:32<00:18,  6.30batch/s, loss=12.3, accuracy=0.828]\u001b[A\n",
      "train:  55%|█████▌    | 144/261 [00:32<00:18,  6.30batch/s, loss=12.3, accuracy=0.828]\u001b[A\n",
      "train:  55%|█████▌    | 144/261 [00:33<00:18,  6.30batch/s, loss=12.7, accuracy=0.812]\u001b[A\n",
      "train:  56%|█████▌    | 145/261 [00:33<00:18,  6.20batch/s, loss=12.7, accuracy=0.812]\u001b[A\n",
      "train:  56%|█████▌    | 145/261 [00:33<00:18,  6.20batch/s, loss=12.1, accuracy=0.859]\u001b[A\n",
      "train:  56%|█████▌    | 146/261 [00:33<00:18,  6.25batch/s, loss=12.1, accuracy=0.859]\u001b[A\n",
      "train:  56%|█████▌    | 146/261 [00:33<00:18,  6.25batch/s, loss=12.7, accuracy=0.812]\u001b[A\n",
      "train:  56%|█████▋    | 147/261 [00:33<00:19,  5.99batch/s, loss=12.7, accuracy=0.812]\u001b[A\n",
      "train:  56%|█████▋    | 147/261 [00:33<00:19,  5.99batch/s, loss=13.2, accuracy=0.75] \u001b[A\n",
      "train:  57%|█████▋    | 148/261 [00:33<00:18,  6.09batch/s, loss=13.2, accuracy=0.75]\u001b[A\n",
      "train:  57%|█████▋    | 148/261 [00:33<00:18,  6.09batch/s, loss=13.8, accuracy=0.797]\u001b[A\n",
      "train:  57%|█████▋    | 149/261 [00:33<00:18,  5.94batch/s, loss=13.8, accuracy=0.797]\u001b[A\n",
      "train:  57%|█████▋    | 149/261 [00:33<00:18,  5.94batch/s, loss=12.5, accuracy=0.906]\u001b[A\n",
      "train:  57%|█████▋    | 150/261 [00:33<00:18,  6.13batch/s, loss=12.5, accuracy=0.906]\u001b[A\n",
      "train:  57%|█████▋    | 150/261 [00:34<00:18,  6.13batch/s, loss=12.1, accuracy=0.781]\u001b[A\n",
      "train:  58%|█████▊    | 151/261 [00:34<00:17,  6.21batch/s, loss=12.1, accuracy=0.781]\u001b[A\n",
      "train:  58%|█████▊    | 151/261 [00:34<00:17,  6.21batch/s, loss=12.8, accuracy=0.891]\u001b[A\n",
      "train:  58%|█████▊    | 152/261 [00:34<00:17,  6.06batch/s, loss=12.8, accuracy=0.891]\u001b[A\n",
      "train:  58%|█████▊    | 152/261 [00:34<00:17,  6.06batch/s, loss=12.1, accuracy=0.891]\u001b[A\n",
      "train:  59%|█████▊    | 153/261 [00:34<00:17,  6.02batch/s, loss=12.1, accuracy=0.891]\u001b[A\n",
      "train:  59%|█████▊    | 153/261 [00:34<00:17,  6.02batch/s, loss=11.9, accuracy=0.859]\u001b[A\n",
      "train:  59%|█████▉    | 154/261 [00:34<00:17,  5.99batch/s, loss=11.9, accuracy=0.859]\u001b[A\n",
      "train:  59%|█████▉    | 154/261 [00:34<00:17,  5.99batch/s, loss=12.7, accuracy=0.75] \u001b[A\n",
      "train:  59%|█████▉    | 155/261 [00:34<00:19,  5.50batch/s, loss=12.7, accuracy=0.75]\u001b[A\n",
      "train:  59%|█████▉    | 155/261 [00:34<00:19,  5.50batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  60%|█████▉    | 156/261 [00:34<00:18,  5.71batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  60%|█████▉    | 156/261 [00:35<00:18,  5.71batch/s, loss=12.3, accuracy=0.891]\u001b[A\n",
      "train:  60%|██████    | 157/261 [00:35<00:17,  5.91batch/s, loss=12.3, accuracy=0.891]\u001b[A\n",
      "train:  60%|██████    | 157/261 [00:35<00:17,  5.91batch/s, loss=12.2, accuracy=0.812]\u001b[A\n",
      "train:  61%|██████    | 158/261 [00:35<00:17,  6.01batch/s, loss=12.2, accuracy=0.812]\u001b[A\n",
      "train:  61%|██████    | 158/261 [00:35<00:17,  6.01batch/s, loss=12.3, accuracy=0.875]\u001b[A\n",
      "train:  61%|██████    | 159/261 [00:35<00:17,  5.88batch/s, loss=12.3, accuracy=0.875]\u001b[A\n",
      "train:  61%|██████    | 159/261 [00:35<00:17,  5.88batch/s, loss=12.6, accuracy=0.781]\u001b[A\n",
      "train:  61%|██████▏   | 160/261 [00:35<00:17,  5.84batch/s, loss=12.6, accuracy=0.781]\u001b[A\n",
      "train:  61%|██████▏   | 160/261 [00:35<00:17,  5.84batch/s, loss=12, accuracy=0.891]  \u001b[A\n",
      "train:  62%|██████▏   | 161/261 [00:35<00:16,  5.89batch/s, loss=12, accuracy=0.891]\u001b[A\n",
      "train:  62%|██████▏   | 161/261 [00:35<00:16,  5.89batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  62%|██████▏   | 162/261 [00:35<00:16,  5.90batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  62%|██████▏   | 162/261 [00:36<00:16,  5.90batch/s, loss=12.3, accuracy=0.906]\u001b[A\n",
      "train:  62%|██████▏   | 163/261 [00:36<00:16,  5.86batch/s, loss=12.3, accuracy=0.906]\u001b[A\n",
      "train:  62%|██████▏   | 163/261 [00:36<00:16,  5.86batch/s, loss=11.8, accuracy=0.812]\u001b[A\n",
      "train:  63%|██████▎   | 164/261 [00:36<00:16,  5.84batch/s, loss=11.8, accuracy=0.812]\u001b[A\n",
      "train:  63%|██████▎   | 164/261 [00:36<00:16,  5.84batch/s, loss=12.3, accuracy=0.766]\u001b[A\n",
      "train:  63%|██████▎   | 165/261 [00:36<00:16,  5.88batch/s, loss=12.3, accuracy=0.766]\u001b[A\n",
      "train:  63%|██████▎   | 165/261 [00:36<00:16,  5.88batch/s, loss=11.7, accuracy=0.828]\u001b[A\n",
      "train:  64%|██████▎   | 166/261 [00:36<00:16,  5.86batch/s, loss=11.7, accuracy=0.828]\u001b[A\n",
      "train:  64%|██████▎   | 166/261 [00:36<00:16,  5.86batch/s, loss=11.8, accuracy=0.828]\u001b[A\n",
      "train:  64%|██████▍   | 167/261 [00:36<00:16,  5.86batch/s, loss=11.8, accuracy=0.828]\u001b[A\n",
      "train:  64%|██████▍   | 167/261 [00:36<00:16,  5.86batch/s, loss=13, accuracy=0.859]  \u001b[A\n",
      "train:  64%|██████▍   | 168/261 [00:36<00:16,  5.76batch/s, loss=13, accuracy=0.859]\u001b[A\n",
      "train:  64%|██████▍   | 168/261 [00:37<00:16,  5.76batch/s, loss=13, accuracy=0.891]\u001b[A\n",
      "train:  65%|██████▍   | 169/261 [00:37<00:15,  5.81batch/s, loss=13, accuracy=0.891]\u001b[A\n",
      "train:  65%|██████▍   | 169/261 [00:37<00:15,  5.81batch/s, loss=14.5, accuracy=0.859]\u001b[A\n",
      "train:  65%|██████▌   | 170/261 [00:37<00:15,  5.94batch/s, loss=14.5, accuracy=0.859]\u001b[A\n",
      "train:  65%|██████▌   | 170/261 [00:37<00:15,  5.94batch/s, loss=11.6, accuracy=0.906]\u001b[A\n",
      "train:  66%|██████▌   | 171/261 [00:37<00:15,  5.96batch/s, loss=11.6, accuracy=0.906]\u001b[A\n",
      "train:  66%|██████▌   | 171/261 [00:37<00:15,  5.96batch/s, loss=11.6, accuracy=0.875]\u001b[A\n",
      "train:  66%|██████▌   | 172/261 [00:37<00:14,  5.93batch/s, loss=11.6, accuracy=0.875]\u001b[A\n",
      "train:  66%|██████▌   | 172/261 [00:37<00:14,  5.93batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  66%|██████▋   | 173/261 [00:37<00:14,  6.08batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  66%|██████▋   | 173/261 [00:37<00:14,  6.08batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  67%|██████▋   | 174/261 [00:37<00:14,  6.21batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  67%|██████▋   | 174/261 [00:38<00:14,  6.21batch/s, loss=12, accuracy=0.906]  \u001b[A\n",
      "train:  67%|██████▋   | 175/261 [00:38<00:14,  6.12batch/s, loss=12, accuracy=0.906]\u001b[A\n",
      "train:  67%|██████▋   | 175/261 [00:38<00:14,  6.12batch/s, loss=11.5, accuracy=0.844]\u001b[A\n",
      "train:  67%|██████▋   | 176/261 [00:38<00:13,  6.21batch/s, loss=11.5, accuracy=0.844]\u001b[A\n",
      "train:  67%|██████▋   | 176/261 [00:38<00:13,  6.21batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train:  68%|██████▊   | 177/261 [00:38<00:13,  6.19batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train:  68%|██████▊   | 177/261 [00:38<00:13,  6.19batch/s, loss=12.5, accuracy=0.906]\u001b[A\n",
      "train:  68%|██████▊   | 178/261 [00:38<00:13,  6.32batch/s, loss=12.5, accuracy=0.906]\u001b[A\n",
      "train:  68%|██████▊   | 178/261 [00:38<00:13,  6.32batch/s, loss=11.8, accuracy=0.828]\u001b[A\n",
      "train:  69%|██████▊   | 179/261 [00:38<00:13,  6.28batch/s, loss=11.8, accuracy=0.828]\u001b[A\n",
      "train:  69%|██████▊   | 179/261 [00:38<00:13,  6.28batch/s, loss=12, accuracy=0.844]  \u001b[A\n",
      "train:  69%|██████▉   | 180/261 [00:38<00:12,  6.31batch/s, loss=12, accuracy=0.844]\u001b[A\n",
      "train:  69%|██████▉   | 180/261 [00:39<00:12,  6.31batch/s, loss=12.3, accuracy=0.859]\u001b[A\n",
      "train:  69%|██████▉   | 181/261 [00:39<00:12,  6.19batch/s, loss=12.3, accuracy=0.859]\u001b[A\n",
      "train:  69%|██████▉   | 181/261 [00:39<00:12,  6.19batch/s, loss=12, accuracy=0.797]  \u001b[A\n",
      "train:  70%|██████▉   | 182/261 [00:39<00:12,  6.11batch/s, loss=12, accuracy=0.797]\u001b[A\n",
      "train:  70%|██████▉   | 182/261 [00:39<00:12,  6.11batch/s, loss=11.8, accuracy=0.906]\u001b[A\n",
      "train:  70%|███████   | 183/261 [00:39<00:12,  6.15batch/s, loss=11.8, accuracy=0.906]\u001b[A\n",
      "train:  70%|███████   | 183/261 [00:39<00:12,  6.15batch/s, loss=11.7, accuracy=0.844]\u001b[A\n",
      "train:  70%|███████   | 184/261 [00:39<00:12,  6.24batch/s, loss=11.7, accuracy=0.844]\u001b[A\n",
      "train:  70%|███████   | 184/261 [00:39<00:12,  6.24batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  71%|███████   | 185/261 [00:39<00:12,  6.21batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  71%|███████   | 185/261 [00:39<00:12,  6.21batch/s, loss=12.5, accuracy=0.875]\u001b[A\n",
      "train:  71%|███████▏  | 186/261 [00:39<00:12,  6.01batch/s, loss=12.5, accuracy=0.875]\u001b[A\n",
      "train:  71%|███████▏  | 186/261 [00:40<00:12,  6.01batch/s, loss=11.7, accuracy=0.828]\u001b[A\n",
      "train:  72%|███████▏  | 187/261 [00:40<00:12,  6.09batch/s, loss=11.7, accuracy=0.828]\u001b[A\n",
      "train:  72%|███████▏  | 187/261 [00:40<00:12,  6.09batch/s, loss=11.7, accuracy=0.859]\u001b[A\n",
      "train:  72%|███████▏  | 188/261 [00:40<00:11,  6.22batch/s, loss=11.7, accuracy=0.859]\u001b[A\n",
      "train:  72%|███████▏  | 188/261 [00:40<00:11,  6.22batch/s, loss=12.2, accuracy=0.875]\u001b[A\n",
      "train:  72%|███████▏  | 189/261 [00:40<00:11,  6.14batch/s, loss=12.2, accuracy=0.875]\u001b[A\n",
      "train:  72%|███████▏  | 189/261 [00:40<00:11,  6.14batch/s, loss=15.2, accuracy=0.781]\u001b[A\n",
      "train:  73%|███████▎  | 190/261 [00:40<00:11,  5.92batch/s, loss=15.2, accuracy=0.781]\u001b[A\n",
      "train:  73%|███████▎  | 190/261 [00:40<00:11,  5.92batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  73%|███████▎  | 191/261 [00:40<00:11,  5.95batch/s, loss=12.3, accuracy=0.844]\u001b[A\n",
      "train:  73%|███████▎  | 191/261 [00:40<00:11,  5.95batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train:  74%|███████▎  | 192/261 [00:40<00:11,  6.00batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train:  74%|███████▎  | 192/261 [00:41<00:11,  6.00batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  74%|███████▍  | 193/261 [00:41<00:11,  6.08batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  74%|███████▍  | 193/261 [00:41<00:11,  6.08batch/s, loss=11.5, accuracy=0.844]\u001b[A\n",
      "train:  74%|███████▍  | 194/261 [00:41<00:11,  5.94batch/s, loss=11.5, accuracy=0.844]\u001b[A\n",
      "train:  74%|███████▍  | 194/261 [00:41<00:11,  5.94batch/s, loss=12.8, accuracy=0.891]\u001b[A\n",
      "train:  75%|███████▍  | 195/261 [00:41<00:10,  6.04batch/s, loss=12.8, accuracy=0.891]\u001b[A\n",
      "train:  75%|███████▍  | 195/261 [00:41<00:10,  6.04batch/s, loss=12.2, accuracy=0.891]\u001b[A\n",
      "train:  75%|███████▌  | 196/261 [00:41<00:10,  6.11batch/s, loss=12.2, accuracy=0.891]\u001b[A\n",
      "train:  75%|███████▌  | 196/261 [00:41<00:10,  6.11batch/s, loss=10.9, accuracy=0.969]\u001b[A\n",
      "train:  75%|███████▌  | 197/261 [00:41<00:10,  6.02batch/s, loss=10.9, accuracy=0.969]\u001b[A\n",
      "train:  75%|███████▌  | 197/261 [00:41<00:10,  6.02batch/s, loss=13.2, accuracy=0.859]\u001b[A\n",
      "train:  76%|███████▌  | 198/261 [00:41<00:10,  6.04batch/s, loss=13.2, accuracy=0.859]\u001b[A\n",
      "train:  76%|███████▌  | 198/261 [00:42<00:10,  6.04batch/s, loss=12.3, accuracy=0.812]\u001b[A\n",
      "train:  76%|███████▌  | 199/261 [00:42<00:10,  6.12batch/s, loss=12.3, accuracy=0.812]\u001b[A\n",
      "train:  76%|███████▌  | 199/261 [00:42<00:10,  6.12batch/s, loss=13, accuracy=0.922]  \u001b[A\n",
      "train:  77%|███████▋  | 200/261 [00:42<00:09,  6.26batch/s, loss=13, accuracy=0.922]\u001b[A\n",
      "train:  77%|███████▋  | 200/261 [00:42<00:09,  6.26batch/s, loss=11.4, accuracy=0.953]\u001b[A\n",
      "train:  77%|███████▋  | 201/261 [00:42<00:09,  6.36batch/s, loss=11.4, accuracy=0.953]\u001b[A\n",
      "train:  77%|███████▋  | 201/261 [00:42<00:09,  6.36batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  77%|███████▋  | 202/261 [00:42<00:09,  6.35batch/s, loss=12.4, accuracy=0.828]\u001b[A\n",
      "train:  77%|███████▋  | 202/261 [00:42<00:09,  6.35batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  78%|███████▊  | 203/261 [00:42<00:09,  6.34batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  78%|███████▊  | 203/261 [00:42<00:09,  6.34batch/s, loss=12, accuracy=0.906]  \u001b[A\n",
      "train:  78%|███████▊  | 204/261 [00:42<00:08,  6.36batch/s, loss=12, accuracy=0.906]\u001b[A\n",
      "train:  78%|███████▊  | 204/261 [00:42<00:08,  6.36batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  79%|███████▊  | 205/261 [00:42<00:09,  6.04batch/s, loss=12.6, accuracy=0.891]\u001b[A\n",
      "train:  79%|███████▊  | 205/261 [00:43<00:09,  6.04batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  79%|███████▉  | 206/261 [00:43<00:08,  6.14batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  79%|███████▉  | 206/261 [00:43<00:08,  6.14batch/s, loss=11.6, accuracy=0.812]\u001b[A\n",
      "train:  79%|███████▉  | 207/261 [00:43<00:08,  6.27batch/s, loss=11.6, accuracy=0.812]\u001b[A\n",
      "train:  79%|███████▉  | 207/261 [00:43<00:08,  6.27batch/s, loss=12.1, accuracy=0.844]\u001b[A\n",
      "train:  80%|███████▉  | 208/261 [00:43<00:08,  5.97batch/s, loss=12.1, accuracy=0.844]\u001b[A\n",
      "train:  80%|███████▉  | 208/261 [00:43<00:08,  5.97batch/s, loss=13.5, accuracy=0.859]\u001b[A\n",
      "train:  80%|████████  | 209/261 [00:43<00:08,  6.02batch/s, loss=13.5, accuracy=0.859]\u001b[A\n",
      "train:  80%|████████  | 209/261 [00:43<00:08,  6.02batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  80%|████████  | 210/261 [00:43<00:08,  6.04batch/s, loss=12.7, accuracy=0.875]\u001b[A\n",
      "train:  80%|████████  | 210/261 [00:43<00:08,  6.04batch/s, loss=12.3, accuracy=0.922]\u001b[A\n",
      "train:  81%|████████  | 211/261 [00:43<00:08,  6.19batch/s, loss=12.3, accuracy=0.922]\u001b[A\n",
      "train:  81%|████████  | 211/261 [00:44<00:08,  6.19batch/s, loss=12, accuracy=0.922]  \u001b[A\n",
      "train:  81%|████████  | 212/261 [00:44<00:07,  6.23batch/s, loss=12, accuracy=0.922]\u001b[A\n",
      "train:  81%|████████  | 212/261 [00:44<00:07,  6.23batch/s, loss=12.1, accuracy=0.844]\u001b[A\n",
      "train:  82%|████████▏ | 213/261 [00:44<00:07,  6.29batch/s, loss=12.1, accuracy=0.844]\u001b[A\n",
      "train:  82%|████████▏ | 213/261 [00:44<00:07,  6.29batch/s, loss=12.7, accuracy=0.797]\u001b[A\n",
      "train:  82%|████████▏ | 214/261 [00:44<00:07,  6.31batch/s, loss=12.7, accuracy=0.797]\u001b[A\n",
      "train:  82%|████████▏ | 214/261 [00:44<00:07,  6.31batch/s, loss=11.4, accuracy=0.906]\u001b[A\n",
      "train:  82%|████████▏ | 215/261 [00:44<00:07,  6.16batch/s, loss=11.4, accuracy=0.906]\u001b[A\n",
      "train:  82%|████████▏ | 215/261 [00:44<00:07,  6.16batch/s, loss=11.5, accuracy=0.891]\u001b[A\n",
      "train:  83%|████████▎ | 216/261 [00:44<00:07,  6.17batch/s, loss=11.5, accuracy=0.891]\u001b[A\n",
      "train:  83%|████████▎ | 216/261 [00:44<00:07,  6.17batch/s, loss=11.6, accuracy=0.969]\u001b[A\n",
      "train:  83%|████████▎ | 217/261 [00:44<00:07,  5.97batch/s, loss=11.6, accuracy=0.969]\u001b[A\n",
      "train:  83%|████████▎ | 217/261 [00:45<00:07,  5.97batch/s, loss=12.1, accuracy=0.875]\u001b[A\n",
      "train:  84%|████████▎ | 218/261 [00:45<00:07,  6.09batch/s, loss=12.1, accuracy=0.875]\u001b[A\n",
      "train:  84%|████████▎ | 218/261 [00:45<00:07,  6.09batch/s, loss=11.4, accuracy=0.875]\u001b[A\n",
      "train:  84%|████████▍ | 219/261 [00:45<00:07,  5.83batch/s, loss=11.4, accuracy=0.875]\u001b[A\n",
      "train:  84%|████████▍ | 219/261 [00:45<00:07,  5.83batch/s, loss=12.8, accuracy=0.906]\u001b[A\n",
      "train:  84%|████████▍ | 220/261 [00:45<00:06,  6.00batch/s, loss=12.8, accuracy=0.906]\u001b[A\n",
      "train:  84%|████████▍ | 220/261 [00:45<00:06,  6.00batch/s, loss=11.3, accuracy=0.906]\u001b[A\n",
      "train:  85%|████████▍ | 221/261 [00:45<00:06,  6.15batch/s, loss=11.3, accuracy=0.906]\u001b[A\n",
      "train:  85%|████████▍ | 221/261 [00:45<00:06,  6.15batch/s, loss=12.2, accuracy=0.781]\u001b[A\n",
      "train:  85%|████████▌ | 222/261 [00:45<00:06,  6.19batch/s, loss=12.2, accuracy=0.781]\u001b[A\n",
      "train:  85%|████████▌ | 222/261 [00:45<00:06,  6.19batch/s, loss=11.2, accuracy=0.891]\u001b[A\n",
      "train:  85%|████████▌ | 223/261 [00:45<00:06,  6.07batch/s, loss=11.2, accuracy=0.891]\u001b[A\n",
      "train:  85%|████████▌ | 223/261 [00:46<00:06,  6.07batch/s, loss=11.5, accuracy=0.875]\u001b[A\n",
      "train:  86%|████████▌ | 224/261 [00:46<00:06,  6.10batch/s, loss=11.5, accuracy=0.875]\u001b[A\n",
      "train:  86%|████████▌ | 224/261 [00:46<00:06,  6.10batch/s, loss=12.6, accuracy=0.828]\u001b[A\n",
      "train:  86%|████████▌ | 225/261 [00:46<00:05,  6.20batch/s, loss=12.6, accuracy=0.828]\u001b[A\n",
      "train:  86%|████████▌ | 225/261 [00:46<00:05,  6.20batch/s, loss=14, accuracy=0.859]  \u001b[A\n",
      "train:  87%|████████▋ | 226/261 [00:46<00:05,  6.17batch/s, loss=14, accuracy=0.859]\u001b[A\n",
      "train:  87%|████████▋ | 226/261 [00:46<00:05,  6.17batch/s, loss=11.9, accuracy=0.875]\u001b[A\n",
      "train:  87%|████████▋ | 227/261 [00:46<00:05,  6.15batch/s, loss=11.9, accuracy=0.875]\u001b[A\n",
      "train:  87%|████████▋ | 227/261 [00:46<00:05,  6.15batch/s, loss=12.7, accuracy=0.906]\u001b[A\n",
      "train:  87%|████████▋ | 228/261 [00:46<00:05,  6.09batch/s, loss=12.7, accuracy=0.906]\u001b[A\n",
      "train:  87%|████████▋ | 228/261 [00:46<00:05,  6.09batch/s, loss=11.7, accuracy=0.859]\u001b[A\n",
      "train:  88%|████████▊ | 229/261 [00:46<00:05,  5.97batch/s, loss=11.7, accuracy=0.859]\u001b[A\n",
      "train:  88%|████████▊ | 229/261 [00:47<00:05,  5.97batch/s, loss=11.5, accuracy=0.953]\u001b[A\n",
      "train:  88%|████████▊ | 230/261 [00:47<00:05,  5.99batch/s, loss=11.5, accuracy=0.953]\u001b[A\n",
      "train:  88%|████████▊ | 230/261 [00:47<00:05,  5.99batch/s, loss=11.2, accuracy=0.906]\u001b[A\n",
      "train:  89%|████████▊ | 231/261 [00:47<00:04,  6.07batch/s, loss=11.2, accuracy=0.906]\u001b[A\n",
      "train:  89%|████████▊ | 231/261 [00:47<00:04,  6.07batch/s, loss=11.6, accuracy=0.906]\u001b[A\n",
      "train:  89%|████████▉ | 232/261 [00:47<00:04,  6.23batch/s, loss=11.6, accuracy=0.906]\u001b[A\n",
      "train:  89%|████████▉ | 232/261 [00:47<00:04,  6.23batch/s, loss=11.9, accuracy=0.781]\u001b[A\n",
      "train:  89%|████████▉ | 233/261 [00:47<00:04,  6.48batch/s, loss=11.9, accuracy=0.781]\u001b[A\n",
      "train:  89%|████████▉ | 233/261 [00:47<00:04,  6.48batch/s, loss=11.5, accuracy=0.859]\u001b[A\n",
      "train:  90%|████████▉ | 234/261 [00:47<00:04,  6.67batch/s, loss=11.5, accuracy=0.859]\u001b[A\n",
      "train:  90%|████████▉ | 234/261 [00:47<00:04,  6.67batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  90%|█████████ | 235/261 [00:47<00:03,  6.80batch/s, loss=12.4, accuracy=0.844]\u001b[A\n",
      "train:  90%|█████████ | 235/261 [00:47<00:03,  6.80batch/s, loss=12, accuracy=0.859]  \u001b[A\n",
      "train:  90%|█████████ | 236/261 [00:47<00:03,  6.76batch/s, loss=12, accuracy=0.859]\u001b[A\n",
      "train:  90%|█████████ | 236/261 [00:48<00:03,  6.76batch/s, loss=11.2, accuracy=0.938]\u001b[A\n",
      "train:  91%|█████████ | 237/261 [00:48<00:03,  6.59batch/s, loss=11.2, accuracy=0.938]\u001b[A\n",
      "train:  91%|█████████ | 237/261 [00:48<00:03,  6.59batch/s, loss=11.8, accuracy=0.875]\u001b[A\n",
      "train:  91%|█████████ | 238/261 [00:48<00:03,  6.13batch/s, loss=11.8, accuracy=0.875]\u001b[A\n",
      "train:  91%|█████████ | 238/261 [00:48<00:03,  6.13batch/s, loss=12.1, accuracy=0.875]\u001b[A\n",
      "train:  92%|█████████▏| 239/261 [00:48<00:03,  6.27batch/s, loss=12.1, accuracy=0.875]\u001b[A\n",
      "train:  92%|█████████▏| 239/261 [00:48<00:03,  6.27batch/s, loss=11.5, accuracy=0.891]\u001b[A\n",
      "train:  92%|█████████▏| 240/261 [00:48<00:03,  6.51batch/s, loss=11.5, accuracy=0.891]\u001b[A\n",
      "train:  92%|█████████▏| 240/261 [00:48<00:03,  6.51batch/s, loss=11.8, accuracy=0.891]\u001b[A\n",
      "train:  92%|█████████▏| 241/261 [00:48<00:03,  6.56batch/s, loss=11.8, accuracy=0.891]\u001b[A\n",
      "train:  92%|█████████▏| 241/261 [00:48<00:03,  6.56batch/s, loss=13, accuracy=0.922]  \u001b[A\n",
      "train:  93%|█████████▎| 242/261 [00:48<00:03,  6.22batch/s, loss=13, accuracy=0.922]\u001b[A\n",
      "train:  93%|█████████▎| 242/261 [00:49<00:03,  6.22batch/s, loss=12.5, accuracy=0.844]\u001b[A\n",
      "train:  93%|█████████▎| 243/261 [00:49<00:03,  5.90batch/s, loss=12.5, accuracy=0.844]\u001b[A\n",
      "train:  93%|█████████▎| 243/261 [00:49<00:03,  5.90batch/s, loss=11.7, accuracy=0.938]\u001b[A\n",
      "train:  93%|█████████▎| 244/261 [00:49<00:02,  6.00batch/s, loss=11.7, accuracy=0.938]\u001b[A\n",
      "train:  93%|█████████▎| 244/261 [00:49<00:02,  6.00batch/s, loss=11.9, accuracy=0.891]\u001b[A\n",
      "train:  94%|█████████▍| 245/261 [00:49<00:02,  5.76batch/s, loss=11.9, accuracy=0.891]\u001b[A\n",
      "train:  94%|█████████▍| 245/261 [00:49<00:02,  5.76batch/s, loss=12.5, accuracy=0.891]\u001b[A\n",
      "train:  94%|█████████▍| 246/261 [00:49<00:02,  5.70batch/s, loss=12.5, accuracy=0.891]\u001b[A\n",
      "train:  94%|█████████▍| 246/261 [00:49<00:02,  5.70batch/s, loss=12.1, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▍| 247/261 [00:49<00:02,  5.75batch/s, loss=12.1, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▍| 247/261 [00:49<00:02,  5.75batch/s, loss=12.8, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▌| 248/261 [00:49<00:02,  6.00batch/s, loss=12.8, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▌| 248/261 [00:50<00:02,  6.00batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▌| 249/261 [00:50<00:01,  6.18batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  95%|█████████▌| 249/261 [00:50<00:01,  6.18batch/s, loss=12.2, accuracy=0.906]\u001b[A\n",
      "train:  96%|█████████▌| 250/261 [00:50<00:01,  6.53batch/s, loss=12.2, accuracy=0.906]\u001b[A\n",
      "train:  96%|█████████▌| 250/261 [00:50<00:01,  6.53batch/s, loss=12.6, accuracy=0.906]\u001b[A\n",
      "train:  96%|█████████▌| 251/261 [00:50<00:01,  6.24batch/s, loss=12.6, accuracy=0.906]\u001b[A\n",
      "train:  96%|█████████▌| 251/261 [00:50<00:01,  6.24batch/s, loss=11.4, accuracy=0.875]\u001b[A\n",
      "train:  97%|█████████▋| 252/261 [00:50<00:01,  5.80batch/s, loss=11.4, accuracy=0.875]\u001b[A\n",
      "train:  97%|█████████▋| 252/261 [00:50<00:01,  5.80batch/s, loss=11.5, accuracy=0.859]\u001b[A\n",
      "train:  97%|█████████▋| 253/261 [00:50<00:01,  6.14batch/s, loss=11.5, accuracy=0.859]\u001b[A\n",
      "train:  97%|█████████▋| 253/261 [00:50<00:01,  6.14batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  97%|█████████▋| 254/261 [00:50<00:01,  6.08batch/s, loss=12.8, accuracy=0.859]\u001b[A\n",
      "train:  97%|█████████▋| 254/261 [00:51<00:01,  6.08batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  98%|█████████▊| 255/261 [00:51<00:00,  6.22batch/s, loss=11.9, accuracy=0.922]\u001b[A\n",
      "train:  98%|█████████▊| 255/261 [00:51<00:00,  6.22batch/s, loss=11.4, accuracy=0.906]\u001b[A\n",
      "train:  98%|█████████▊| 256/261 [00:51<00:00,  6.23batch/s, loss=11.4, accuracy=0.906]\u001b[A\n",
      "train:  98%|█████████▊| 256/261 [00:51<00:00,  6.23batch/s, loss=11.5, accuracy=0.875]\u001b[A\n",
      "train:  98%|█████████▊| 257/261 [00:51<00:00,  6.47batch/s, loss=11.5, accuracy=0.875]\u001b[A\n",
      "train:  98%|█████████▊| 257/261 [00:51<00:00,  6.47batch/s, loss=11.5, accuracy=0.922]\u001b[A\n",
      "train:  99%|█████████▉| 258/261 [00:51<00:00,  6.52batch/s, loss=11.5, accuracy=0.922]\u001b[A\n",
      "train:  99%|█████████▉| 258/261 [00:51<00:00,  6.52batch/s, loss=11.9, accuracy=0.891]\u001b[A\n",
      "train:  99%|█████████▉| 259/261 [00:51<00:00,  6.68batch/s, loss=11.9, accuracy=0.891]\u001b[A\n",
      "train:  99%|█████████▉| 259/261 [00:51<00:00,  6.68batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train: 100%|█████████▉| 260/261 [00:51<00:00,  6.37batch/s, loss=12.1, accuracy=0.906]\u001b[A\n",
      "train: 100%|█████████▉| 260/261 [00:52<00:00,  6.37batch/s, loss=12.3, accuracy=0.922]\u001b[A\n",
      "train: 100%|██████████| 261/261 [00:52<00:00,  5.02batch/s, loss=12.3, accuracy=0.922]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "seen:   0%|          | 0/65 [00:00<?, ?batch/s]\u001b[A\n",
      "seen:   0%|          | 0/65 [00:00<?, ?batch/s, loss=11.3]\u001b[A\n",
      "seen:   2%|▏         | 1/65 [00:00<00:46,  1.37batch/s, loss=11.3]\u001b[A\n",
      "seen:   2%|▏         | 1/65 [00:00<00:46,  1.37batch/s, loss=11.7]\u001b[A\n",
      "seen:   2%|▏         | 1/65 [00:00<00:46,  1.37batch/s, loss=11.7]\u001b[A\n",
      "seen:   5%|▍         | 3/65 [00:00<00:14,  4.25batch/s, loss=11.7]\u001b[A\n",
      "seen:   5%|▍         | 3/65 [00:00<00:14,  4.25batch/s, loss=11.5]\u001b[A\n",
      "seen:   5%|▍         | 3/65 [00:00<00:14,  4.25batch/s, loss=11.4]\u001b[A\n",
      "seen:   8%|▊         | 5/65 [00:00<00:08,  7.04batch/s, loss=11.4]\u001b[A\n",
      "seen:   8%|▊         | 5/65 [00:01<00:08,  7.04batch/s, loss=11.4]\u001b[A\n",
      "seen:   8%|▊         | 5/65 [00:01<00:08,  7.04batch/s, loss=11.1]\u001b[A\n",
      "seen:   8%|▊         | 5/65 [00:01<00:08,  7.04batch/s, loss=11.6]\u001b[A\n",
      "seen:  12%|█▏        | 8/65 [00:01<00:05, 10.70batch/s, loss=11.6]\u001b[A\n",
      "seen:  12%|█▏        | 8/65 [00:01<00:05, 10.70batch/s, loss=10.8]\u001b[A\n",
      "seen:  12%|█▏        | 8/65 [00:01<00:05, 10.70batch/s, loss=11.3]\u001b[A\n",
      "seen:  12%|█▏        | 8/65 [00:01<00:05, 10.70batch/s, loss=11.5]\u001b[A\n",
      "seen:  17%|█▋        | 11/65 [00:01<00:04, 13.31batch/s, loss=11.5]\u001b[A\n",
      "seen:  17%|█▋        | 11/65 [00:01<00:04, 13.31batch/s, loss=11.5]\u001b[A\n",
      "seen:  17%|█▋        | 11/65 [00:01<00:04, 13.31batch/s, loss=11.7]\u001b[A\n",
      "seen:  20%|██        | 13/65 [00:01<00:03, 14.20batch/s, loss=11.7]\u001b[A\n",
      "seen:  20%|██        | 13/65 [00:01<00:03, 14.20batch/s, loss=10.6]\u001b[A\n",
      "seen:  20%|██        | 13/65 [00:01<00:03, 14.20batch/s, loss=11.3]\u001b[A\n",
      "seen:  23%|██▎       | 15/65 [00:01<00:03, 14.70batch/s, loss=11.3]\u001b[A\n",
      "seen:  23%|██▎       | 15/65 [00:01<00:03, 14.70batch/s, loss=11.5]\u001b[A\n",
      "seen:  23%|██▎       | 15/65 [00:01<00:03, 14.70batch/s, loss=11]  \u001b[A\n",
      "seen:  26%|██▌       | 17/65 [00:01<00:03, 15.04batch/s, loss=11]\u001b[A\n",
      "seen:  26%|██▌       | 17/65 [00:01<00:03, 15.04batch/s, loss=11.3]\u001b[A\n",
      "seen:  26%|██▌       | 17/65 [00:01<00:03, 15.04batch/s, loss=11]  \u001b[A\n",
      "seen:  29%|██▉       | 19/65 [00:01<00:02, 15.85batch/s, loss=11]\u001b[A\n",
      "seen:  29%|██▉       | 19/65 [00:01<00:02, 15.85batch/s, loss=11.2]\u001b[A\n",
      "seen:  29%|██▉       | 19/65 [00:01<00:02, 15.85batch/s, loss=11]  \u001b[A\n",
      "seen:  32%|███▏      | 21/65 [00:01<00:02, 15.35batch/s, loss=11]\u001b[A\n",
      "seen:  32%|███▏      | 21/65 [00:01<00:02, 15.35batch/s, loss=11.4]\u001b[A\n",
      "seen:  32%|███▏      | 21/65 [00:01<00:02, 15.35batch/s, loss=11.8]\u001b[A\n",
      "seen:  35%|███▌      | 23/65 [00:01<00:02, 16.09batch/s, loss=11.8]\u001b[A\n",
      "seen:  35%|███▌      | 23/65 [00:02<00:02, 16.09batch/s, loss=11]  \u001b[A\n",
      "seen:  35%|███▌      | 23/65 [00:02<00:02, 16.09batch/s, loss=11.2]\u001b[A\n",
      "seen:  38%|███▊      | 25/65 [00:02<00:02, 15.87batch/s, loss=11.2]\u001b[A\n",
      "seen:  38%|███▊      | 25/65 [00:02<00:02, 15.87batch/s, loss=10.9]\u001b[A\n",
      "seen:  38%|███▊      | 25/65 [00:02<00:02, 15.87batch/s, loss=10.9]\u001b[A\n",
      "seen:  42%|████▏     | 27/65 [00:02<00:02, 15.97batch/s, loss=10.9]\u001b[A\n",
      "seen:  42%|████▏     | 27/65 [00:02<00:02, 15.97batch/s, loss=12.2]\u001b[A\n",
      "seen:  42%|████▏     | 27/65 [00:02<00:02, 15.97batch/s, loss=11.2]\u001b[A\n",
      "seen:  45%|████▍     | 29/65 [00:02<00:02, 16.18batch/s, loss=11.2]\u001b[A\n",
      "seen:  45%|████▍     | 29/65 [00:02<00:02, 16.18batch/s, loss=11.5]\u001b[A\n",
      "seen:  45%|████▍     | 29/65 [00:02<00:02, 16.18batch/s, loss=11.5]\u001b[A\n",
      "seen:  48%|████▊     | 31/65 [00:02<00:02, 16.73batch/s, loss=11.5]\u001b[A\n",
      "seen:  48%|████▊     | 31/65 [00:02<00:02, 16.73batch/s, loss=12.7]\u001b[A\n",
      "seen:  48%|████▊     | 31/65 [00:02<00:02, 16.73batch/s, loss=10.4]\u001b[A\n",
      "seen:  51%|█████     | 33/65 [00:02<00:01, 16.71batch/s, loss=10.4]\u001b[A\n",
      "seen:  51%|█████     | 33/65 [00:02<00:01, 16.71batch/s, loss=11.1]\u001b[A\n",
      "seen:  51%|█████     | 33/65 [00:02<00:01, 16.71batch/s, loss=11.1]\u001b[A\n",
      "seen:  54%|█████▍    | 35/65 [00:02<00:01, 15.99batch/s, loss=11.1]\u001b[A\n",
      "seen:  54%|█████▍    | 35/65 [00:02<00:01, 15.99batch/s, loss=11.4]\u001b[A\n",
      "seen:  54%|█████▍    | 35/65 [00:02<00:01, 15.99batch/s, loss=11.3]\u001b[A\n",
      "seen:  57%|█████▋    | 37/65 [00:02<00:01, 16.72batch/s, loss=11.3]\u001b[A\n",
      "seen:  57%|█████▋    | 37/65 [00:02<00:01, 16.72batch/s, loss=10.5]\u001b[A\n",
      "seen:  57%|█████▋    | 37/65 [00:02<00:01, 16.72batch/s, loss=11.5]\u001b[A\n",
      "seen:  60%|██████    | 39/65 [00:02<00:01, 16.29batch/s, loss=11.5]\u001b[A\n",
      "seen:  60%|██████    | 39/65 [00:03<00:01, 16.29batch/s, loss=11.3]\u001b[A\n",
      "seen:  60%|██████    | 39/65 [00:03<00:01, 16.29batch/s, loss=11.1]\u001b[A\n",
      "seen:  63%|██████▎   | 41/65 [00:03<00:01, 16.67batch/s, loss=11.1]\u001b[A\n",
      "seen:  63%|██████▎   | 41/65 [00:03<00:01, 16.67batch/s, loss=11.2]\u001b[A\n",
      "seen:  63%|██████▎   | 41/65 [00:03<00:01, 16.67batch/s, loss=10.8]\u001b[A\n",
      "seen:  66%|██████▌   | 43/65 [00:03<00:01, 16.67batch/s, loss=10.8]\u001b[A\n",
      "seen:  66%|██████▌   | 43/65 [00:03<00:01, 16.67batch/s, loss=12.1]\u001b[A\n",
      "seen:  66%|██████▌   | 43/65 [00:03<00:01, 16.67batch/s, loss=10.9]\u001b[A\n",
      "seen:  69%|██████▉   | 45/65 [00:03<00:01, 16.75batch/s, loss=10.9]\u001b[A\n",
      "seen:  69%|██████▉   | 45/65 [00:03<00:01, 16.75batch/s, loss=11.5]\u001b[A\n",
      "seen:  69%|██████▉   | 45/65 [00:03<00:01, 16.75batch/s, loss=10.4]\u001b[A\n",
      "seen:  72%|███████▏  | 47/65 [00:03<00:01, 16.56batch/s, loss=10.4]\u001b[A\n",
      "seen:  72%|███████▏  | 47/65 [00:03<00:01, 16.56batch/s, loss=11.8]\u001b[A\n",
      "seen:  72%|███████▏  | 47/65 [00:03<00:01, 16.56batch/s, loss=10.9]\u001b[A\n",
      "seen:  75%|███████▌  | 49/65 [00:03<00:00, 17.02batch/s, loss=10.9]\u001b[A\n",
      "seen:  75%|███████▌  | 49/65 [00:03<00:00, 17.02batch/s, loss=11.5]\u001b[A\n",
      "seen:  75%|███████▌  | 49/65 [00:03<00:00, 17.02batch/s, loss=11.7]\u001b[A\n",
      "seen:  78%|███████▊  | 51/65 [00:03<00:00, 14.99batch/s, loss=11.7]\u001b[A\n",
      "seen:  78%|███████▊  | 51/65 [00:03<00:00, 14.99batch/s, loss=10.7]\u001b[A\n",
      "seen:  78%|███████▊  | 51/65 [00:03<00:00, 14.99batch/s, loss=11.1]\u001b[A\n",
      "seen:  82%|████████▏ | 53/65 [00:03<00:00, 15.42batch/s, loss=11.1]\u001b[A\n",
      "seen:  82%|████████▏ | 53/65 [00:03<00:00, 15.42batch/s, loss=11.1]\u001b[A\n",
      "seen:  82%|████████▏ | 53/65 [00:03<00:00, 15.42batch/s, loss=11.6]\u001b[A\n",
      "seen:  85%|████████▍ | 55/65 [00:03<00:00, 16.16batch/s, loss=11.6]\u001b[A\n",
      "seen:  85%|████████▍ | 55/65 [00:04<00:00, 16.16batch/s, loss=11.7]\u001b[A\n",
      "seen:  85%|████████▍ | 55/65 [00:04<00:00, 16.16batch/s, loss=12.5]\u001b[A\n",
      "seen:  88%|████████▊ | 57/65 [00:04<00:00, 16.72batch/s, loss=12.5]\u001b[A\n",
      "seen:  88%|████████▊ | 57/65 [00:04<00:00, 16.72batch/s, loss=12]  \u001b[A\n",
      "seen:  88%|████████▊ | 57/65 [00:04<00:00, 16.72batch/s, loss=11]\u001b[A\n",
      "seen:  91%|█████████ | 59/65 [00:04<00:00, 17.13batch/s, loss=11]\u001b[A\n",
      "seen:  91%|█████████ | 59/65 [00:04<00:00, 17.13batch/s, loss=11.3]\u001b[A\n",
      "seen:  91%|█████████ | 59/65 [00:04<00:00, 17.13batch/s, loss=11.4]\u001b[A\n",
      "seen:  94%|█████████▍| 61/65 [00:04<00:00, 16.72batch/s, loss=11.4]\u001b[A\n",
      "seen:  94%|█████████▍| 61/65 [00:04<00:00, 16.72batch/s, loss=11]  \u001b[A\n",
      "seen:  94%|█████████▍| 61/65 [00:04<00:00, 16.72batch/s, loss=11.7]\u001b[A\n",
      "seen:  97%|█████████▋| 63/65 [00:04<00:00, 16.59batch/s, loss=11.7]\u001b[A\n",
      "seen:  97%|█████████▋| 63/65 [00:04<00:00, 16.59batch/s, loss=11.5]\u001b[A\n",
      "seen:  97%|█████████▋| 63/65 [00:04<00:00, 16.59batch/s, loss=11.9]\u001b[A\n",
      "seen: 100%|██████████| 65/65 [00:04<00:00, 14.30batch/s, loss=11.9]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "fold_metric_scores = []\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    train_n, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    model = SOTAEmbedding(\n",
    "        linear_filters=config[\"model_params\"][\"linear_filters\"],\n",
    "        input_feat=config[\"model_params\"][\"input_feat\"],\n",
    "        dropout=config[\"model_params\"][\"dropout\"]\n",
    "    )\n",
    "    # model = Model1(feat_dim=in_ft, max_len=seq_len, d_model=config['d_model'], n_heads=config['num_heads'], num_layers=2, dim_feedforward=128, ft_size=feat_size)\n",
    "    model.to(device)\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'],weight_decay=1e-5)\n",
    "    loss_module = {'class': nn.CrossEntropyLoss(reduction=\"sum\"), 'feature': nn.L1Loss(reduction=\"sum\")}\n",
    "    best_acc = 0.0\n",
    "    # train the model \n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        train_metrics = train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.0001)\n",
    "        eval_metrics = eval_step(model, eval_dl, eval_dt,loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='seen', print_report=True, loss_alpha=0.0001)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "    save_model(model,notebook_iden,model_iden,i)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step(model, test_dl,test_dt, loss_module, device, class_names=[all_classes[i] for i in unseen_classes],   phase='unseen', loss_alpha=0.0001)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "seen_score_df.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
