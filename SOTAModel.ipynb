{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmaking Experiment 006b\n",
    "- benchmarking model architecture -3 with PAMAP2, DaLiAc and UTD-MHAD datasets\n",
    "- Model : Model4\n",
    "- Dataset : PAMAP2\n",
    "- Semantic Space : Glove50\n",
    "- Cross Validation : 5-fold fixed classes\n",
    "- Feature Loss : MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import numpy.random as random\n",
    "from src.datasets.data import PAMAP2Reader\n",
    "from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "\n",
    "# from src.running import train_step1, eval_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"test-001\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"SOTAEmbedding\",\n",
    "    \"model_params\":{\n",
    "        \"linear_filters\":[1024,1024,1024,2048],\n",
    "        \"input_feat\":102,\n",
    "        \"dropout\":0.1,\n",
    "    },\n",
    "    \"sem-space\": 'attr',\n",
    "    # model training configs\n",
    "    \"include_attribute_loss\": True, \n",
    "    \"semantic_size\": 64,\n",
    "    \"n_actions\": 18,\n",
    "    \"folding\": True,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_epochs\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 32,\n",
    "    \"semantic_loss\": \"cosine_distance\",\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    # dataset configs\n",
    "    \"window_size\": 5, \n",
    "    \"overlap\": 0.5,\n",
    "    \"seq_len\": 200,\n",
    "    \"seen_split\": 0.2,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOTAEmbedding(nn.Module):\n",
    "    def __init__(self, linear_filters=[1024,1024,1024,2048],input_feat=102, dropout=0.1):\n",
    "        super(SOTAEmbedding, self).__init__()\n",
    "        self.input_feat = input_feat\n",
    "        self.linear_filters = linear_filters\n",
    "        self.input_feat = input_feat\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.linear1 = nn.Linear(input_feat,linear_filters[0])\n",
    "        self.batch_norm1 = nn.BatchNorm1d(linear_filters[0])\n",
    "        self.linear2 = nn.Linear(linear_filters[0],linear_filters[1])\n",
    "        self.batch_norm2 = nn.BatchNorm1d(linear_filters[1])\n",
    "        self.linear3 = nn.Linear(linear_filters[1],linear_filters[2])\n",
    "        self.batch_norm3 = nn.BatchNorm1d(linear_filters[2])\n",
    "        self.linear4 = nn.Linear(linear_filters[2]+linear_filters[1]+linear_filters[0],linear_filters[3])\n",
    "        self.batch_norm4 = nn.BatchNorm1d(linear_filters[3])\n",
    "        self.act = F.relu\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #layer1\n",
    "        out1 = self.linear1(x)\n",
    "        out1 = self.batch_norm1(out1)\n",
    "        out1 = self.act(out1)\n",
    "\n",
    "        #layer2\n",
    "        out2 = self.linear2(out1)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.act(out2)\n",
    "\n",
    "        #layer3\n",
    "        out3 = self.linear3(out2)\n",
    "        out3 = self.batch_norm3(out3)\n",
    "        out3 = self.act(out3)\n",
    "\n",
    "        concat = torch.cat([out1,out2,out3],-1)\n",
    "\n",
    "        #layer4\n",
    "        out4 = self.linear4(concat)\n",
    "        out4 = self.batch_norm4(out4)\n",
    "        out4 = self.act(out4)\n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model = SOTAEmbedding(\n",
    "    linear_filters=config[\"model_params\"][\"linear_filters\"],\n",
    "    input_feat=config[\"model_params\"][\"input_feat\"],\n",
    "    dropout=config[\"model_params\"][\"dropout\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 2048])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = model(torch.randn((32,102)))\n",
    "emb.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\FYP\\HAR-ZSL-XAI\\src\\datasets\\data.py:79: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  9 10 11 12 13 16 17 18 19 20 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2Reader('data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(124,)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = PAMAP2Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "video_data = np.load('data/I3D/video_feat/PAMAP2_K10_V1/feat_dict.npz')\n",
    "video_classes, video_feat = video_data['activity'], video_data['features']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(180, 2048)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_feat.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(array(['Nordic walking', 'ascending stairs', 'car driving',\n        'computer work', 'cycling', 'descending stairs', 'folding laundry',\n        'house cleaning', 'ironing', 'lying', 'playing soccer',\n        'rope jumping', 'running', 'sitting', 'standing',\n        'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'),\n array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n        10], dtype=int64))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(video_classes, return_counts=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "vid_cls_name = np.unique(video_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def selecting_video_prototypes(prototypes:np.array,classes:np.array,vid_class_name:np.array):\n",
    "    selected = []\n",
    "    for tar in vid_class_name:\n",
    "        indexes = np.where(classes == tar)\n",
    "        selected.append(torch.from_numpy(prototypes[random.choice(indexes[0])]))\n",
    "\n",
    "    return torch.stack(selected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([18, 2048])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selecting_video_prototypes(video_feat,video_classes,vid_cls_name).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 'lying'),\n (2, 'sitting'),\n (3, 'standing'),\n (4, 'walking'),\n (5, 'running'),\n (6, 'cycling'),\n (7, 'Nordic walking'),\n (9, 'watching TV'),\n (10, 'computer work'),\n (11, 'car driving'),\n (12, 'ascending stairs'),\n (13, 'descending stairs'),\n (16, 'vacuum cleaning'),\n (17, 'ironing'),\n (18, 'folding laundry'),\n (19, 'house cleaning'),\n (20, 'playing soccer'),\n (24, 'rope jumping')]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.label_map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Nordic walking', 'ascending stairs', 'car driving',\n       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n       'house cleaning', 'ironing', 'lying', 'playing soccer',\n       'rope jumping', 'running', 'sitting', 'standing',\n       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(video_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(video_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Main\\envs\\CurrentSOTA\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "D:\\Anaconda\\Main\\envs\\CurrentSOTA\\lib\\site-packages\\numpy\\core\\_methods.py:213: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=fold_cls_ids[0], seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "((16752, 102), (16752,))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape,data_dict['train']['y'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['seen_classes']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 102])\n"
     ]
    }
   ],
   "source": [
    "sample_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=data_dict['seen_classes'], seq_len=100)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for d in sample_dl:\n",
    "    print(d[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 0,\n 1: 1,\n 3: 2,\n 4: 3,\n 5: 4,\n 6: 5,\n 8: 6,\n 9: 7,\n 11: 8,\n 12: 9,\n 13: 10,\n 14: 11,\n 16: 12,\n 17: 13}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dt.action2Id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dt.attribute_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([14, 2048])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sample_dt.getClassAttrs()\n",
    "r.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_cross_entropy(\n",
    "        y_pred:torch.Tensor,\n",
    "        cls:torch.Tensor,\n",
    "        selected_features,\n",
    "        loss_fn=nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "):\n",
    "    num_classes = selected_features.size()[0]\n",
    "\n",
    "    cosine_sim_comb = []\n",
    "    for entry in y_pred.unbind():\n",
    "        cosine_sim = F.cosine_similarity(entry.repeat(num_classes,1),selected_features)\n",
    "        cosine_sim_comb.append(cosine_sim)\n",
    "\n",
    "    cosine_sim_comb = torch.stack(cosine_sim_comb)\n",
    "    loss = loss_fn(cosine_sim_comb,cls)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_reconstruction_calc(y_pred:torch.Tensor,y_feat:torch.Tensor,loss_fn=nn.L1Loss(reduction=\"sum\")):\n",
    "\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, dataset:PAMAP2Dataset, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat = batch\n",
    "            # print(X, targets, target_feat, target_attr)\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            random_selected_feat = dataset.getClassAttrs() #selecting_video_prototypes(video_feat,video_classes,train_dt.Id2action)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "                loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn =loss_module['class'] )\n",
    "                class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "                feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            pred_class = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader,dataset, loss_module, device, class_names, target_feat_met, phase='seen', l2_reg=False, print_report=True, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, target_attr, padding_masks = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            target_attr = target_attr.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "                class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "                feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            if phase == 'seen':\n",
    "                pred_action = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            else:\n",
    "                feat_numpy = torch.sigmoid(feat_output.cpu().detach())\n",
    "                action_probs = cosine_similarity(feat_numpy, target_feat_met)\n",
    "                pred_action = np.argmax(action_probs, axis=1)\n",
    "\n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action)\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report)\n",
    "    return metrics_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "per class count :  {'lying': 8, 'sitting': 8, 'standing': 8, 'walking': 8, 'running': 6, 'cycling': 7, 'Nordic walking': 7, 'watching TV': 1, 'computer work': 4, 'car driving': 1, 'ascending stairs': 16, 'descending stairs': 17, 'vacuum cleaning': 8, 'ironing': 8, 'folding laundry': 4, 'house cleaning': 5, 'playing soccer': 2, 'rope jumping': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Main\\envs\\CurrentSOTA\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "D:\\Anaconda\\Main\\envs\\CurrentSOTA\\lib\\site-packages\\numpy\\core\\_methods.py:213: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n",
      "unseen classes >  [7, 15, 2, 10]\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "train:   0%|          | 0/523 [00:00<?, ?batch/s]\u001B[A\n",
      "                                                      \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 39\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# train the model \u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining Epoch\u001B[39m\u001B[38;5;124m'\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 39\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclass_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mall_classes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mseen_classes\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_alpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m     eval_metrics \u001B[38;5;241m=\u001B[39m eval_step(model, eval_dl, loss_module, device, class_names\u001B[38;5;241m=\u001B[39m[all_classes[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m seen_classes],  target_feat_met\u001B[38;5;241m=\u001B[39meval_dt\u001B[38;5;241m.\u001B[39mtarget_feat, phase\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseen\u001B[39m\u001B[38;5;124m'\u001B[39m, print_report\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, loss_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m)\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;66;03m# print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\u001B[39;00m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;66;03m# print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[30], line 9\u001B[0m, in \u001B[0;36mtrain_step\u001B[1;34m(model, dataloader, optimizer, loss_module, device, class_names, phase, l2_reg, loss_alpha)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(dataloader, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m, desc\u001B[38;5;241m=\u001B[39mphase) \u001B[38;5;28;01mas\u001B[39;00m tepoch:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tepoch:\n\u001B[1;32m----> 9\u001B[0m         X, targets, target_feat, target_attr, padding_masks \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     10\u001B[0m         \u001B[38;5;66;03m# print(X, targets, target_feat, target_attr)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m         X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "fold_metric_scores = []\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    train_n, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=video_feat, attribute_dict=action_dict, action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    model = SOTAEmbedding(\n",
    "        linear_filters=config[\"model_params\"][\"linear_filters\"],\n",
    "        input_feat=config[\"model_params\"][\"input_feat\"],\n",
    "        dropout=config[\"model_params\"][\"dropout\"]\n",
    "    )\n",
    "    # model = Model1(feat_dim=in_ft, max_len=seq_len, d_model=config['d_model'], n_heads=config['num_heads'], num_layers=2, dim_feedforward=128, ft_size=feat_size)\n",
    "    model.to(device)\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "    loss_module = {'class': nn.CrossEntropyLoss(), 'feature': nn.MSELoss(reduction=\"sum\")}\n",
    "    best_acc = 0.0\n",
    "    # train the model \n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        train_metrics = train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.8)\n",
    "        eval_metrics = eval_step(model, eval_dl, eval_dt,loss_module, device, class_names=[all_classes[i] for i in seen_classes],  target_feat_met=eval_dt.target_feat, phase='seen', print_report=False, loss_alpha=0.8)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['total_accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    if best_acc == 0.0:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step(model, test_dl,test_dt, loss_module, device, class_names=[all_classes[i] for i in unseen_classes],  target_feat_met=test_dt.target_feat, phase='unseen', loss_alpha=0.8)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "seen_score_df.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
