{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss,L1Loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from pytorch_metric_learning.losses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.data import PAMAP2Reader\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "\n",
    "from src.models.ts_transformer import RelativeGlobalAttention\n",
    "from src.models.loss import FeatureLoss, AttributeLoss \n",
    "from src.utils.losses import SupConLoss\n",
    "\n",
    "from src.utils.analysis import action_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"test-001\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"RelativeGlobalAttention\",\n",
    "    \"sem-space\": 'attr',\n",
    "    # model training configs\n",
    "    \"include_attribute_loss\": True, \n",
    "    \"semantic_size\": 64,\n",
    "    \"n_actions\": 18,\n",
    "    \"folding\": True,\n",
    "    \"lr\": 0.001,\n",
    "    \"ae_lr\": 0.0001,\n",
    "    \"imu_lr\": 0.0004,\n",
    "    \"ae_alpha\": 0.7,\n",
    "    \"imu_alpha\": 0.3,\n",
    "    \"n_epochs\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 32,\n",
    "    \"semantic_loss\": \"cosine_distance\",\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    \"feat_size\": 64, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 5, \n",
    "    \"overlap\": 0.5,\n",
    "    \"seq_len\": 50,  # skeleton seq. length\n",
    "    \"seen_split\": 0.2,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6a(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, n_classes, max_len=1024, dropout=0.1):\n",
    "        super(Model6a, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu  # _get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.logist = nn.Linear(self.ft_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out1 = self.DenseL2(out)   \n",
    "        out = self.logist(out1)     \n",
    "        return out, out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32, device=device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32)\n",
    "prep_dir = './tmp/random_input_100_epochs.pt'\n",
    "\n",
    "ae_model.load_state_dict(torch.load(prep_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PAMAP2 dataset\n",
    "dt = config['dataset']\n",
    "if dt == 'PAMAP2':\n",
    "    dataReader = PAMAP2Reader('./data/PAMAP2_Dataset/Protocol/')\n",
    "\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dt == 'PAMAP2':\n",
    "    fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_data = np.load('./data/skeleton_k10_v7_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_mov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(skeleton_classes, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataReader.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(skeleton_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAMAP2Dataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, attribute_dict, action_classes, seq_len=120):\n",
    "        super(PAMAP2Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attribute_dict = attribute_dict\n",
    "        self.seq_len = seq_len\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_classes = action_classes\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        # extraction semantic space generation skeleton sequences \n",
    "        skel_idx = random.choice(self.attribute_dict[target])\n",
    "        y_feat = self.attributes[skel_idx, ...]\n",
    "        return x, y, y_feat, x_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def getClassAttrs(self):\n",
    "        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n",
    "        ft_mat = self.attributes[sampling_idx, ...]\n",
    "        return ft_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=fold_cls_ids[0], seen_ratio=0.2, unseen_ratio=0.8, window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "sample_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=data_dict['seen_classes'], seq_len=100)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for d in sample_dl:\n",
    "    print(d[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sample_dt.getClassAttrs()\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = F.pad(input=r, pad=(0, 0, 0, 0, 0, 18), mode='constant', value=0)\n",
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(r[1, ...], 1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_out1, r_out2 = ae_model(torch.transpose(pr, 1, 0).float())\n",
    "# r_out1.shape, r_out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, movements, actions, action_dict, seq_len=60):\n",
    "        super(SkeletonDataset, self).__init__()\n",
    "        self.movements = torch.from_numpy(movements[:, :seq_len, ...]) #@\n",
    "        self.actions = actions\n",
    "        self.action_dict = deepcopy(dict(action_dict))\n",
    "        self.actionsIDs = list(self.action_dict.keys())\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.movements[idx, ...]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        partial_idx = random.sample(self.action_dict[action], k=1)[0]\n",
    "        x2 = self.movements[partial_idx, ...]\n",
    "\n",
    "        label = self.actionsIDs.index(action)\n",
    "        # return np.transpose(x1, (1,0,2)), np.transpose(x2, (1,0,2)), label\n",
    "        return x1, x2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.movements.shape[0]\n",
    "\n",
    "    def getShape(self):\n",
    "        return self.movements[0, ...].shape\n",
    "\n",
    "    def get_actions(self, label):\n",
    "        return self.movements[self.action_dict[label], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dt = SkeletonDataset(skeleton_mov, skeleton_Ids, action_dict, seq_len=50)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "for b in sample_dl:\n",
    "    bx1, bx2, by = b \n",
    "    # bx = torch.transpose(bx1, 1, 0)\n",
    "    bs, seq_len, ft_in = bx1.shape\n",
    "    print(bx1.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dt.getShape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    print(sample_dt.get_actions(i).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_avg(data, model, device, bs=32):\n",
    "    ns, _, _ = data.shape \n",
    "    padded_mat = F.pad(input=data, pad=(0,0,0,0,0,bs-ns), mode='constant', value=0)\n",
    "    _, vector_out = model(padded_mat.float().to(device)) # batch second mode\n",
    "    action_feat_mat = vector_out[:ns, :].cpu().detach().numpy()\n",
    "    avg_vector = np.mean(action_feat_mat, axis=0)\n",
    "    # print(action_feat_mat.shape, avg_vector.shape)\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imu_train_step1(model, ae, dataloader, skel_dt, optimizer, loss_module, device, class_names, class_ids, phase='train', l2_reg=False, loss_alpha=0.7, batch_size=32):\n",
    "    model = model.train()\n",
    "    ae = ae.eval()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "\n",
    "    sem_space = []\n",
    "    for c in class_ids:\n",
    "        action_mat = skel_dt.get_actions(c)\n",
    "        avg_vector = get_class_avg(action_mat, ae, device, batch_size)\n",
    "        sem_space.append(avg_vector)\n",
    "\n",
    "    sem_space = np.array(sem_space)\n",
    "    sem_space = torch.from_numpy(sem_space)\n",
    "    # print(sem_space.shape, \"< sem space\")\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, _, padding_masks = batch\n",
    "            # print(X, targets, target_feat, target_attr)\n",
    "            X = X.float().to(device)\n",
    "            # target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "\n",
    "            target_feat = torch.squeeze(sem_space[targets]).float().to(device) #@\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "            \n",
    "            # print(feat_output.shape, target_feat.shape)\n",
    "            class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "            feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            pred_class = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "            \n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step1(model , dataloader, optimizer, loss_module, device, batch_size, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_loss = 0 \n",
    "    total_samples = 0 \n",
    "\n",
    "    with tqdm(dataloader, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            x1, x2, labels = batch \n",
    "            # post-process \n",
    "            xa = torch.vstack([x1, x2])  # batch first mode\n",
    "            # device offload \n",
    "            xa = xa.float().to(device)\n",
    "            labels = labels.float()\n",
    "\n",
    "            # set optimizer grad to zero \n",
    "            optimizer.zero_grad()\n",
    "            # get model prediction \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                skel_output, ft_output = model(xa)\n",
    "\n",
    "            # reconstruct the output \n",
    "            f1, f2 = torch.split(ft_output, [batch_size//2, batch_size//2], dim=0)\n",
    "            cons_output = torch.stack([f1.squeeze(1), f2.squeeze(1)], dim=1)\n",
    "            # calc. contrastive loss \n",
    "            con_loss = loss_module['contrast'](cons_output, labels)\n",
    "            # calc. reconstruction loss \n",
    "            l2_loss = loss_module['recons'](xa, skel_output)\n",
    "            # calc. total loss\n",
    "            total_loss = loss_alpha*con_loss + (1-loss_alpha)*l2_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {'contrastive loss ': con_loss.item(), 'reconstruction loss': l2_loss.item()}\n",
    "            with torch.no_grad():\n",
    "                    total_samples += len(labels)\n",
    "                    epoch_loss += total_loss.item()\n",
    "            \n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss/total_samples\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tnse(X, y, Id2Label):\n",
    "    tnse = TSNE(n_components=2, init='random', learning_rate='auto', perplexity=15, n_iter=1000)\n",
    "    cmp_data = tnse.fit_transform(X)\n",
    "\n",
    "    cmp_df = pd.DataFrame(data=cmp_data, columns=['x', 'y'])\n",
    "    cmp_df['label'] = y\n",
    "    cmp_df['action'] = cmp_df['label'].map(Id2Label)\n",
    "\n",
    "    fig = px.scatter(cmp_df, x='x', y='y', color='action', width=800, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step1(model, ae, dataloader, skel_dt, loss_module, device, class_names, class_ids, target_feat_met, phase='seen', l2_reg=False, print_report=True, show_plot=True, loss_alpha=0.7, batch_size=32):\n",
    "    model = model.train()\n",
    "    ae = ae.eval()\n",
    "    label_dict = {c:i for i,c in enumerate(class_ids)}\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "\n",
    "    # generate unseen action_semantic from unseen skeleton seq. \n",
    "    all_actions = []\n",
    "    all_labels = []\n",
    "    for c in class_ids:\n",
    "        action_mat = skel_dt.get_actions(c)\n",
    "        ns, _, _ = action_mat.shape \n",
    "        class_labels = [label_dict[c] for _ in range(ns)]\n",
    "        padded_mat = F.pad(input=action_mat, pad=(0,0,0,0,0,batch_size-ns), mode='constant', value=0)\n",
    "        _, vector_out = ae(padded_mat.float().to(device)) # batch second mode\n",
    "        action_feat_mat = vector_out[:ns, :].cpu().detach().numpy() # batch second mode\n",
    "\n",
    "        all_actions.append(action_feat_mat)\n",
    "        all_labels.append(class_labels)\n",
    "\n",
    "    all_actions = np.concatenate(all_actions)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    # build knn model on know unseen samples \n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(all_actions, all_labels)\n",
    "    # print(\"unseen labels\", all_labels)\n",
    "    # plot t-SNE on unseen vector \n",
    "    Id2Label = {i:l for i,l in enumerate(class_names)}\n",
    "    plot_tnse(all_actions, all_labels, Id2Label)\n",
    "    \n",
    "    # print(\"target feat mat\", target_feat_met.shap e, \"action feat mat\", action_feat_mat.shape)\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, padding_masks = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            \n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                _, target_feat = ae(target_feat) # batch second mode\n",
    "                \n",
    "            class_loss = 0 #loss_module['class'](class_output, targets.squeeze())\n",
    "            feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            if phase == 'seen':\n",
    "                pred_action = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            else:\n",
    "                # print(\"targets > \", targets)\n",
    "                # print(\"feat output\", feat_output.shape)\n",
    "                feat_numpy = feat_output.cpu().detach().numpy()\n",
    "                pred_action = knn.predict(feat_numpy)\n",
    "                \n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action)\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    \n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    # print(\"predictions > \", all_preds, \"ground truth > \", all_targets[:, 0])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_dir = './tmp/epoch50_randsample_input_mseloss.pt'\n",
    "\n",
    "fold_metric_scores = []\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    # print(f'Unseen Classes : {fold_classes[i]}')\n",
    "    # separate seen/unseen and train/eval \n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "\n",
    "    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt \n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=skeleton_mov, attribute_dict=action_dict, action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    print(\"Initiate Skeleton dataset ... \")\n",
    "    # build Skeleton dataset \n",
    "    skel_dt = SkeletonDataset(skeleton_mov, skeleton_Ids, action_dict, seq_len=config['seq_len'])\n",
    "    skel_dl = DataLoader(skel_dt, batch_size=config['batch_size']//2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    skel_n, skel_fts = skel_dt.getShape()\n",
    "    \n",
    "    print(\"Initial Models ...\")\n",
    "    # build IMU Encoder Model \n",
    "    imu_model = Model6a(in_ft=in_ft, d_model=config['d_model'], num_heads=config['num_heads'], ft_size=config['feat_size'], max_len=seq_len, n_classes=len(seen_classes))\n",
    "    imu_model.to(device)\n",
    "\n",
    "    # build AE Model \n",
    "    # ae_model = BiLSTMEncDecModel(input_size=skel_fts, seq_len=skel_n, hidden_size=config['feat_size'], batch_size=config['batch_size'], ae_type='recursive', device=device)\n",
    "    ae_model = BiLSTMEncDecModel(seq_len=skel_n, input_size=skel_fts, hidden_size=512, linear_filters=[128,256,512], embedding_size=config['feat_size'], num_layers=1, bidirectional=True, batch_size=config['batch_size'], device=device)\n",
    "    # ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32, device=device)\n",
    "    # ae_model.load_state_dict(torch.load(prep_dir))\n",
    "    ae_model.to(device)\n",
    "    \n",
    "    # define IMU-Enc run parameters \n",
    "    imu_optim = Adam(imu_model.parameters(), lr=config['imu_lr'])\n",
    "    imu_loss_module = {'class': nn.CrossEntropyLoss(), 'feature': L1Loss()}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # define AE run parameters \n",
    "    ae_optim = Adam(ae_model.parameters(), lr=config['ae_lr'])\n",
    "    ae_loss_module = {'contrast': SupConLoss(contrast_mode='one'), 'recons': nn.MSELoss()}\n",
    "\n",
    "    print(\"Start Models training ...\")\n",
    "    # train 2 models E2E\n",
    "    # train AE model \n",
    "    for epoch in tqdm(range(config['n_epochs']//2), desc='Training Epoch', leave=False):\n",
    "        ae_train_metrics = ae_train_step1(ae_model, skel_dl, ae_optim, ae_loss_module, device, config['batch_size'], phase='train', loss_alpha=config['ae_alpha'])\n",
    "        \n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        \n",
    "        # train IMU-Enc model\n",
    "        train_metrics = imu_train_step1(imu_model, ae_model, train_dl, skel_dt, imu_optim, imu_loss_module, device, class_names=[all_classes[i] for i in seen_classes], class_ids=seen_classes, phase='train', loss_alpha=config['imu_alpha'], batch_size=32)\n",
    "        eval_metrics = eval_step1(imu_model, ae_model, eval_dl, skel_dt, imu_loss_module, device=device, class_names=[all_classes[i] for i in seen_classes], class_ids=seen_classes,  target_feat_met=eval_dt.getClassAttrs(), phase='seen', print_report=False, loss_alpha=config['imu_alpha'], batch_size=config['batch_size'], show_plot=True)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['total_accuracy'] > best_acc:\n",
    "            imu_best_model = deepcopy(imu_model.state_dict())\n",
    "\n",
    "    # replace by best model \n",
    "    imu_model.load_state_dict(imu_best_model)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step1(imu_model, ae_model, test_dl, skel_dt, imu_loss_module, device=device, class_names=[all_classes[i] for i in unseen_classes], class_ids=unseen_classes,  target_feat_met=test_dt.getClassAttrs(), phase='unseen', loss_alpha=0.8, batch_size=config['batch_size'])\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "seen_score_df.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
