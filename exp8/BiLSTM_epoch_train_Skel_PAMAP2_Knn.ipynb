{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAMAP2 dataset | Epoch wise training Static Skeleton Dataset | BiLSTM IMU Encoder Model | Knn unseen inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipun\\AppData\\Local\\Temp\\ipykernel_17312\\1929646555.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "# import neptune\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.utils.analysis import action_evaluator\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "from src.utils.losses import *\n",
    "from src.utils.analysis import action_evaluator\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"Experiment_section-1\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"BiLSTM\",\n",
    "    \"sem-space\": 'Skeleton',\n",
    "    # model training configs\n",
    "    \"lr\": 0.0001,\n",
    "    \"imu_alpha\": 0.0001,\n",
    "    \"n_epochs\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    'neighs': 10,\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    \"feat_size\": 512, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 5.21, \n",
    "    \"overlap\": 4.21,\n",
    "    \"seq_len\": 20,  # skeleton seq. length\n",
    "    \"seen_split\": 0.1,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\Pose-AE\\exp8\\..\\src\\datasets\\data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2ReaderV2('../data/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAMAP2Dataset(Dataset):\n",
    "    def __init__(self, data, actions, action_classes, seq_len=120):\n",
    "        super(PAMAP2Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        # self.attribute_dict = attribute_dict\n",
    "        self.seq_len = seq_len\n",
    "        # self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_classes = action_classes\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        # extraction semantic space generation skeleton sequences\n",
    "        # vid_idx = random.choice(self.attribute_dict[target])\n",
    "        # y_feat = self.attributes[y, ...]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def getClassAttrs(self):\n",
    "        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n",
    "        ft_mat = self.attributes[sampling_idx, ...]\n",
    "        return ft_mat\n",
    "\n",
    "    def getClassFeatures(self):\n",
    "        cls_feat = []\n",
    "        for c in self.action_classes:\n",
    "            idx = self.attribute_dict[c]\n",
    "            cls_feat.append(torch.mean(self.attributes[idx, ...], dim=0))\n",
    "\n",
    "        cls_feat = torch.vstack(cls_feat)\n",
    "        # print(cls_feat.size())\n",
    "        return cls_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, ft_size, n_classes, num_heads=1, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_ft,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n",
    "        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_forward = out[:, self.max_len - 1, :self.d_model]\n",
    "        out_reverse = out[:, 0, self.d_model:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        out = self.drop(out_reduced)\n",
    "        out = self.act(out)\n",
    "        out = self.fcLayer1(out)\n",
    "        # out = self.fcLayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "              \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        return x\n",
    "\n",
    "class SkeletonAE(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(SkeletonAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32, device=device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 60, 36)\n",
      "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
      "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
      "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
      "       'rope jumping', 'running', 'sitting', 'standing',\n",
      "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10], dtype=int64))\n",
      "[(1, 'lying'), (2, 'sitting'), (3, 'standing'), (4, 'walking'), (5, 'running'), (6, 'cycling'), (7, 'Nordic walking'), (9, 'watching TV'), (10, 'computer work'), (11, 'car driving'), (12, 'ascending stairs'), (13, 'descending stairs'), (16, 'vacuum cleaning'), (17, 'ironing'), (18, 'folding laundry'), (19, 'house cleaning'), (20, 'playing soccer'), (24, 'rope jumping')]\n",
      "['Nordic walking' 'ascending stairs' 'car driving' 'computer work'\n",
      " 'cycling' 'descending stairs' 'folding laundry' 'house cleaning'\n",
      " 'ironing' 'lying' 'playing soccer' 'rope jumping' 'running' 'sitting'\n",
      " 'standing' 'vacuum cleaning' 'walking' 'watching TV']\n"
     ]
    }
   ],
   "source": [
    "skeleton_data = np.load('../data/skeleton_k10_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\n",
    "\n",
    "print(skeleton_mov.shape)\n",
    "print(np.unique(skeleton_classes, return_counts=True))\n",
    "print(dataReader.label_map)\n",
    "print(np.unique(skeleton_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting_video_prototypes(prototypes:np.array,classes:np.array,vid_class_name:np.array):\n",
    "    selected = []\n",
    "    for tar in vid_class_name:\n",
    "        indexes = np.where(classes == tar)\n",
    "        selected.append(torch.from_numpy(prototypes[random.choice(indexes[0])]))\n",
    "\n",
    "    return torch.stack(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, movements, actions, action_dict, seq_len=60):\n",
    "        super(SkeletonDataset, self).__init__()\n",
    "        # new_fts = [i for i in range(movements.shape[-1]) if i%3 != 2]\n",
    "        # self.movements = torch.from_numpy(movements[:, :seq_len, new_fts])\n",
    "        self.movements = torch.from_numpy(movements[:, :seq_len, ...])\n",
    "        self.actions = actions\n",
    "        self.action_dict = deepcopy(dict(action_dict))\n",
    "        self.actionsIDs = list(self.action_dict.keys())  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.movements[idx, ...]\n",
    "        action = self.actions[idx]\n",
    "        # partial_idx = random.sample(self.action_dict[action], k=1)[0]\n",
    "        # x2 = self.movements[partial_idx, ...]\n",
    "        label = self.actionsIDs.index(action)\n",
    "        # return np.transpose(x1, (1,0,2)), np.transpose(x2, (1,0,2)), label\n",
    "        return x1, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.movements.shape[0]\n",
    "\n",
    "    def getShape(self):\n",
    "        return self.movements[0, ...].shape\n",
    "\n",
    "    def get_actions(self, label):\n",
    "        return self.movements[self.action_dict[label], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_vector(ae, skel_dt, device, class_ids, class_names, batch_size=32):\n",
    "    # generate unseen action_semantic from unseen skeleton seq. \n",
    "    label_dict = {c:i for i,c in enumerate(class_ids)}\n",
    "    all_actions = []\n",
    "    all_labels = []\n",
    "    for c in class_ids:\n",
    "        action_mat = skel_dt.get_actions(c)\n",
    "        ns, _, _ = action_mat.shape \n",
    "        class_labels = [label_dict[c] for _ in range(ns)]\n",
    "        padded_mat = F.pad(input=action_mat, pad=(0,0,0,0,0,batch_size-ns), mode='constant', value=0)\n",
    "        _, vector_out = ae(padded_mat.float().to(device)) # batch second mode\n",
    "        action_feat_mat = vector_out[:ns, :].cpu().detach().numpy() # batch second mode\n",
    "\n",
    "        all_actions.append(action_feat_mat)\n",
    "        all_labels.append(class_labels)\n",
    "\n",
    "    all_actions = np.concatenate(all_actions)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    Id2Label = {i:l for i,l in enumerate(class_names)}\n",
    "    return all_actions, all_labels, Id2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])\n",
    "\n",
    "action_dict = OrderedDict(list(action_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(y_pred, y, feat, loss_fn):\n",
    "    # print(f\"y_pred : {y_pred.shape} | feat : {feat.shape}\")\n",
    "    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n",
    "    feat_norm = torch.norm(feat, p=2, dim=1)\n",
    "    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n",
    "    softmax_vec = torch.softmax(norm_vec, dim=1)\n",
    "    output = loss_fn(softmax_vec, y)\n",
    "    pred = torch.argmax(softmax_vec, dim=-1)\n",
    "    return output, pred\n",
    "\n",
    "def loss_reconstruction_calc(y_pred, y, feat, loss_fn=nn.L1Loss(reduction=\"sum\")):\n",
    "    y_feat = feat[y, ...]\n",
    "    loss = loss_fn(y_pred,y_feat)\n",
    "    return loss\n",
    "\n",
    "def predict_class(y_pred, feat):\n",
    "    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n",
    "    feat_norm = torch.norm(feat, p=2, dim=1)\n",
    "    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n",
    "    softmax_vec = torch.softmax(norm_vec, dim=1)\n",
    "    pred = torch.argmax(softmax_vec, dim=-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, semantic_space, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets = batch\n",
    "            X = X.float().to(device)\n",
    "            # target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "                class_loss, class_output = loss_cross_entropy(feat_output, targets.squeeze(), semantic_space, loss_fn =loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output, targets.squeeze(), semantic_space, loss_fn=loss_module[\"feature\"])\n",
    "\n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss\n",
    "            # class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            pred_class = class_output.cpu().detach().numpy()\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader, semantic_space, loss_module, device, class_names,  phase='seen', l2_reg=False, print_report=False, show_plot=False, loss_alpha=0.7):\n",
    "    model = model.eval()\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets = batch\n",
    "            X = X.float().to(device)\n",
    "            X = X.float().to(device)\n",
    "            # target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "                class_loss, class_output = loss_cross_entropy(feat_output, targets.squeeze(), semantic_space, loss_fn =loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output, targets.squeeze(), semantic_space, loss_fn=loss_module[\"feature\"])\n",
    "            \n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss\n",
    "            # class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            pred_action = class_output\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metrics['samples'] += len(targets)\n",
    "                metrics['loss'] += loss.item()  # add total loss of batch\n",
    "                metrics['feat. loss'] += feat_loss.item()\n",
    "                metrics['classi. loss'] += class_loss.item()\n",
    "\n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action.cpu().numpy())\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    metrics_dict.update(metrics)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_semantic_space(idx_dict, labels, feats):\n",
    "    action_ft = []\n",
    "\n",
    "    class_dict = {c: i for i,c in enumerate(labels)}\n",
    "    n_cls = len(labels)\n",
    "    action_label = []\n",
    "    for l in labels:\n",
    "        idxs = idx_dict[l] # class_dict[k]\n",
    "        for i in idxs:\n",
    "            action_ft.append(feats[i])\n",
    "            action_label.append(l)\n",
    "\n",
    "    action_label = np.array(action_label)\n",
    "    return action_ft, action_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_eval_step(model, dataloader, sem_actions, sem_labels, loss_module, device, class_names, class_ids, phase='seen', l2_reg=False, print_report=False, show_plot=False, loss_alpha=0.7, neighs=5):\n",
    "    model = model.eval()\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': [], 'feat': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n",
    "    \n",
    "\n",
    "    ld = dict(zip(class_ids, range(len(class_ids))))\n",
    "    all_actions = sem_actions\n",
    "    all_labels = np.array([ld[e] for e in sem_labels])\n",
    "    # build knn model on know unseen samples \n",
    "    clf = KNeighborsClassifier(n_neighbors=neighs, algorithm='auto', metric='cosine', metric_params=None, n_jobs=None, weights='distance')\n",
    "    clf.fit(sem_actions, all_labels)\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # forward track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output = model(X)\n",
    "\n",
    "            # convert feature vector into action class using cosine\n",
    "            feat_numpy = feat_output.cpu().detach().numpy()\n",
    "            pred_action = clf.predict(feat_numpy)\n",
    "  \n",
    "            with torch.no_grad():\n",
    "                metrics['samples'] += len(targets)\n",
    "\n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action)\n",
    "            per_batch['feat'].append(feat_numpy)\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    all_pred_ft = np.concatenate(per_batch[\"feat\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    metrics_dict.update(metrics)\n",
    "    label2Id = dict(zip(range(len(class_ids)), class_names))\n",
    "    return metrics_dict, sem_actions, all_labels, all_pred_ft, all_preds, label2Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_plot_preds(gt_actions, gt_labels, pred_actions, pred_labels, Id2Label):\n",
    "    gt_n = gt_actions.shape[0]\n",
    "    pred_n = pred_actions.shape[0]\n",
    "\n",
    "    all_actions = np.concatenate([gt_actions, pred_actions])\n",
    "    all_labels = np.concatenate([gt_labels, pred_labels])\n",
    "    all_types = ['GT',]*gt_n + ['pred',]*pred_n\n",
    "\n",
    "    tnse = TSNE(n_components=2, init='random', learning_rate='auto', perplexity=15, n_iter=1000)\n",
    "    gt_data = tnse.fit_transform(all_actions)\n",
    "\n",
    "    gt_df = pd.DataFrame(data=gt_data, columns=['x', 'y'])\n",
    "    gt_df['label'] = all_labels \n",
    "    gt_df['action'] = gt_df['label'].map(Id2Label)\n",
    "    gt_df['type'] = all_types\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(data=gt_df, x='x', y='y', hue='action', style='type')\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step1(model, dataloader, optimizer, loss_module, device, batch_size, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    const_loss = 0 \n",
    "    recons_loss = 0\n",
    "    epoch_loss = 0 \n",
    "\n",
    "    with tqdm(dataloader, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            xa, labels = batch \n",
    "            # post-process \n",
    "            # xa = torch.vstack([x1, x2])  # batch first mode\n",
    "            # device offload \n",
    "            xa = xa.float().to(device)\n",
    "            labels = labels.float()\n",
    "\n",
    "            # set optimizer grad to zero \n",
    "            optimizer.zero_grad()\n",
    "            # get model prediction \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                skel_output, ft_output = model(xa)\n",
    "\n",
    "            # reconstruct the output \n",
    "            # f1, f2 = torch.split(ft_output, [batch_size//2, batch_size//2], dim=0)\n",
    "            # cons_output = torch.stack([f1.squeeze(1), f2.squeeze(1)], dim=1)\n",
    "            # calc. contrastive loss \n",
    "            con_loss = loss_module['contrast'](ft_output, labels)()\n",
    "            # calc. reconstruction loss \n",
    "            l2_loss = loss_module['recons'](xa, skel_output)\n",
    "            # calc. total loss\n",
    "            # total_loss = con_loss #\n",
    "            total_loss = loss_alpha*con_loss + (1-loss_alpha)*l2_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {'contrastive loss ': con_loss.item(), 'reconstruction loss': l2_loss.item()}\n",
    "            with torch.no_grad():\n",
    "                    const_loss += con_loss.item()    \n",
    "                    recons_loss += l2_loss.item()\n",
    "                    epoch_loss += total_loss.item()\n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    m = len(dataloader)\n",
    "    epoch_metrics = {'total_loss': epoch_loss/m, 'contrastive_loss': const_loss/m, 'reconstruction_loss':recons_loss/m}\n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(df):\n",
    "    df['loss'] = df['loss']/df['samples']\n",
    "    df['feat. loss'] = df['feat. loss']/df['samples']\n",
    "    df['classi. loss'] = df['classi. loss']/df['samples']\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=4)\n",
    "    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(fold, phase, metrics):\n",
    "    for m, v in metrics.items():\n",
    "        if fold == 'global':\n",
    "            run[f'global/{m}'].log(v)\n",
    "        else:\n",
    "            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = neptune.init_run(\n",
    "#     project=\"FYP-Group22/ICANN-Logs\",\n",
    "#     api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
    "# )  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n",
      "unseen classes >  [7, 15, 2, 10]\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'skel_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitiate IMU datasets ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39m# build IMU datasets\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_dt \u001b[39m=\u001b[39m PAMAP2Dataset(data\u001b[39m=\u001b[39mdata_dict[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m], actions\u001b[39m=\u001b[39mdata_dict[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m], attributes\u001b[39m=\u001b[39mskel_actions, attribute_dict\u001b[39m=\u001b[39maction_dict, action_classes\u001b[39m=\u001b[39mseen_classes, seq_len\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m     19\u001b[0m train_dl \u001b[39m=\u001b[39m DataLoader(train_dt, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[39m# build seen eval_dt\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'skel_actions' is not defined"
     ]
    }
   ],
   "source": [
    "# run['parameters'] = config\n",
    "fold_metric_scores = []\n",
    "\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=12, window_overlap=10, resample_freq=50)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build Skeleton dataset \n",
    "    skel_dt = SkeletonDataset(skeleton_mov, skeleton_Ids, action_dict, seq_len=config['seq_len'])\n",
    "    skel_dl = DataLoader(skel_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    skel_n, skel_fts = skel_dt.getShape()\n",
    "\n",
    "    # build model\n",
    "    imu_config = {\n",
    "        'in_ft':in_ft, \n",
    "        'd_model':config['d_model'], \n",
    "        'num_heads':config['num_heads'], \n",
    "        'ft_size':config['feat_size'], \n",
    "        'max_len':seq_len, \n",
    "        'n_classes':len(seen_classes)\n",
    "    }\n",
    "    model = IMUEncoder(**imu_config)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    prep_dir = '../data/Pre-trained Skeleton models/PAMAP2/epoch100_emb512.pt'\n",
    "    model_config = torch.load(prep_dir)['model_config']\n",
    "    ae_config = {\n",
    "        'seq_len': skel_n, \n",
    "        'input_size': skel_fts, \n",
    "        'hidden_size': model_config['hidden_size'], \n",
    "        'linear_filters': model_config['linear_filters'], \n",
    "        'embedding_size': model_config['hidden_size'],\n",
    "        # 'num_classes': 138,\n",
    "        'num_layers': 1,\n",
    "        'bidirectional': True,\n",
    "        'batch_size':config['batch_size']\n",
    "    }\n",
    "    ae_model = SkeletonAE(device=device, **ae_config)\n",
    "    # ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32, device=device)\n",
    "    ae_model.load_state_dict(torch.load(prep_dir)['model_state_dict'], strict=False)\n",
    "    ae_model.to(device)\n",
    "\n",
    "\n",
    "    seen_action_ft, seen_action_label = gen_semantic_space(action_dict, seen_classes, skel_actions)\n",
    "    unseen_action_ft, unseen_action_label = gen_semantic_space(action_dict, unseen_classes, skel_actions)\n",
    "\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'], weight_decay=1e-6)\n",
    "    loss_module = {'class': nn.CrossEntropyLoss(reduction=\"sum\"), 'feature': nn.L1Loss(reduction=\"sum\")}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # define AE run parameters \n",
    "    ae_optim = Adam(ae_model.parameters(), lr=config['ae_lr'])\n",
    "    ae_loss_module = {'contrast': SoftNearestNeighbours, 'recons': nn.MSELoss()}\n",
    "\n",
    "    # train the model \n",
    "    train_data = []\n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        \n",
    "        ae_train_metrics = ae_train_step1(ae_model, skel_dl, ae_optim, ae_loss_module, device, config['batch_size'], phase='train', loss_alpha=config['ae_alpha'])\n",
    "        ae_train_metrics['epoch'] = epoch\n",
    "\n",
    "        skel_actions, skel_labels, skel_lm = get_action_vector(ae_model, skel_dt, device, class_ids=range(18), class_names=dataReader.idToLabel)\n",
    "        semantic_space = np.array([skel_actions[action_dict[c],:].mean(axis=0) for c in range(len(actionList))])\n",
    "\n",
    "        train_metrics = train_step(model, train_dl, semantic_space, optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.0001)\n",
    "        train_metrics['epoch'] = epoch\n",
    "        train_metrics['phase'] = 'train'\n",
    "        train_data.append(train_metrics)\n",
    "        # log(i, 'train', train_metrics)\n",
    "\n",
    "        eval_metrics = eval_step(model, eval_dl, semantic_space, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='seen', loss_alpha=0.0001, print_report=False, show_plot=False)\n",
    "        eval_metrics['epoch'] = epoch \n",
    "        eval_metrics['phase'] = 'valid'\n",
    "        train_data.append(eval_metrics)\n",
    "        # log(i, 'eval', eval_metrics)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    train_df = pd.DataFrame().from_records(train_data)\n",
    "    plot_curves(train_df)\n",
    "\n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "    # save_model(model,notebook_iden,model_iden,i)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    skel_actions, skel_labels, skel_lm = get_action_vector(ae_model, skel_dt, device, class_ids=range(18), class_names=dataReader.idToLabel)\n",
    "    unseen_action_ft, unseen_action_label = gen_semantic_space(action_dict, unseen_classes, skel_actions)\n",
    "    test_metrics, a,b,c,d,e = unseen_eval_step(model, test_dl, unseen_action_ft, unseen_action_label, loss_module, device, class_names=[all_classes[i] for i in unseen_classes], class_ids=unseen_classes, phase='unseen', loss_alpha=config['imu_alpha'], print_report=True, show_plot=True, neighs=config['neighs'])\n",
    "    # over_plot_preds(a,b,c,d,e)\n",
    "    # over_plot_preds(a,b,c[:30, :],d[:30],e)\n",
    "    test_metrics['N'] = len(unseen_classes)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    # log('test', i, test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "weighted_score_df = seen_score_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].multiply(seen_score_df[\"N\"], axis=\"index\")\n",
    "final_results = weighted_score_df.sum()/seen_score_df['N'].sum()\n",
    "print(final_results)\n",
    "# log('global', '',final_results.to_dict())\n",
    "# run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
