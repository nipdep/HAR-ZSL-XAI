{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/DaLiAc_Dataset/dataset_1.txt', 'r') as pf:\n",
    "    all_data = [l.rstrip('\\n').split(',') for l in pf.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['1', '10', '11', '12', '13', '2', '3', '4', '5', '6', '7', '8',\n",
       "        '9'], dtype='<U11'),\n",
       " array([12290, 25397, 24577, 24577, 12907, 12289, 12289, 24578, 12289,\n",
       "        18433, 52021,  6964,  6965], dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data = np.array(all_data)\n",
    "all_labels = np_data[:, -1]\n",
    "np.unique(all_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'sitting'),\n",
       " (2, 'lying'),\n",
       " (3, 'standing'),\n",
       " (4, 'washing dishes'),\n",
       " (5, 'vacuuming'),\n",
       " (6, 'sweeping'),\n",
       " (7, 'walking'),\n",
       " (8, 'ascending stairs'),\n",
       " (9, 'descending stairs'),\n",
       " (10, 'treadmill running'),\n",
       " (11, '50W cycling'),\n",
       " (12, '100W cycling'),\n",
       " (13, 'rope jumping')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (1, 'sitting'),\n",
    "    (2, 'lying'),\n",
    "    (3, 'standing'),\n",
    "    (4, 'washing dishes'),\n",
    "    (5, 'vacuuming'),\n",
    "    (6, 'sweeping'),\n",
    "    (7, 'walking'),\n",
    "    (8, 'ascending stairs'),\n",
    "    (9, 'descending stairs'),\n",
    "    (10, 'treadmill running'),\n",
    "    (11, '50W cycling'),\n",
    "    (12, '100W cycling'),\n",
    "    (13, 'rope jumping')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaLiAcReader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readDaliac()\n",
    "\n",
    "    def readFile(self, file_path):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        action_ID = 0\n",
    "\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.rstrip('\\n').split(',')\n",
    "            act = int(s[-1])\n",
    "            if act == 12:\n",
    "                act = 11\n",
    "            elif act == 13:\n",
    "                act = 12\n",
    "\n",
    "            if (prev_action != act):\n",
    "                if not(starting):\n",
    "                    # df = pd.DataFrame(action_seq)\n",
    "                    # intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                    # intep_data = intep_df.values \n",
    "                    intep_data = action_seq\n",
    "                    all_data['data'][action_ID] = np.array(intep_data)\n",
    "                    all_data['target'][action_ID] = prev_action\n",
    "                    action_ID+=1\n",
    "                action_seq = []\n",
    "            else:\n",
    "                starting = False\n",
    "            data_seq = np.array(s[:-1]).astype(np.float16)\n",
    "            # data_seq[np.isnan(data_seq)] = 0\n",
    "            action_seq.append(data_seq)\n",
    "            prev_action = act\n",
    "            \n",
    "            # print(prev_action)\n",
    "            all_data['collection'].append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                df = pd.DataFrame(action_seq)\n",
    "                intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                intep_data = intep_df.values\n",
    "                all_data['data'][action_ID] = np.array(intep_data)\n",
    "                all_data['target'][action_ID] = prev_action\n",
    "        return all_data\n",
    "\n",
    "    def readDaliAcFiles(self, filelist, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        collection = []\n",
    "        for i, filename in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "            collection.extend(file_data['collection'])\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readDaliac(self):\n",
    "        files = [f'dataset_{i}.txt' for i in range(1, 20)]\n",
    "            \n",
    "        label_map = [\n",
    "            (1, 'sitting'),\n",
    "            (2, 'lying'),\n",
    "            (3, 'standing'),\n",
    "            (4, 'washing dishes'),\n",
    "            (5, 'vacuuming'),\n",
    "            (6, 'sweeping'),\n",
    "            (7, 'walking'),\n",
    "            (8, 'ascending stairs'),\n",
    "            (9, 'descending stairs'),\n",
    "            (10, 'treadmill running'),\n",
    "            (11, 'cycling'),\n",
    "            (12, 'rope jumping')\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        cols = [\n",
    "                1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "                35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53\n",
    "                ]\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readDaliAcFiles(files, labelToId)\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def dataTableOptimizerUpdated(self, mat_file):\n",
    "        our_data = mat_file['d_iner']\n",
    "        data = []\n",
    "        frame_size = len(our_data[0][0])-1\n",
    "        for each in range(0,frame_size):\n",
    "            data_flatten = our_data[:,:,each].flatten()\n",
    "            data_flatten = data_flatten\n",
    "            data.append(data_flatten)\n",
    "        return data,frame_size\n",
    "\n",
    "    def resample(self, signal, freq=10):\n",
    "        step_size = int(100/freq)\n",
    "        seq_len, _ = signal.shape \n",
    "        resample_indx = np.arange(0, seq_len, step_size)\n",
    "        resampled_sig = signal[resample_indx, :]\n",
    "        return resampled_sig\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*50) # 100Hz compensation \n",
    "        overlap_len = int(overlap*50) # 100Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = self.resample(w, resample_freq)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, window_size=5.21, window_overlap=1, resample_freq=20, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "        \n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "        print(f\"data shape : {self.data.shape}, seen_data shape : {seen_data.shape}\")\n",
    "        ids, cnts = np.unique(self.targets, return_counts=True)\n",
    "        print({self.idToLabel[ids[e]]: cnts[e] for e in range(len(ids))})\n",
    "        \n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "       # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        # print(fst_index)\n",
    "        # print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val = seen_data[fst_index, :], seen_data[sec_index, :]\n",
    "        y_seen_train, y_seen_val = seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "        # val-test split\n",
    "        unseen_index = list(range(len(unseen_targets)))\n",
    "        random.shuffle(unseen_index)\n",
    "        split_point = int((1-unseen_ratio)*len(unseen_index))\n",
    "        fst_index, sec_index = unseen_index[:split_point], unseen_index[split_point:]\n",
    "\n",
    "        X_unseen_val, X_unseen_test = unseen_data[fst_index, :], unseen_data[sec_index, :]\n",
    "        y_unseen_val, y_unseen_test = unseen_targets[fst_index], unseen_targets[sec_index]\n",
    "\n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'eval-unseen':{\n",
    "                        'X': X_unseen_val,\n",
    "                        'y': y_unseen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': X_unseen_test,\n",
    "                        'y': y_unseen_test\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 19\n",
      "Reading file 2 of 19\n",
      "Reading file 3 of 19\n",
      "Reading file 4 of 19\n",
      "Reading file 5 of 19\n",
      "Reading file 6 of 19\n",
      "Reading file 7 of 19\n",
      "Reading file 8 of 19\n",
      "Reading file 9 of 19\n",
      "Reading file 10 of 19\n",
      "Reading file 11 of 19\n",
      "Reading file 12 of 19\n",
      "Reading file 13 of 19\n",
      "Reading file 14 of 19\n",
      "Reading file 15 of 19\n",
      "Reading file 16 of 19\n",
      "Reading file 17 of 19\n",
      "Reading file 18 of 19\n",
      "Reading file 19 of 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_58048\\1888737570.py:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = DaLiAcReader(root_path='../data/DaLiAc_Dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (247,), seen_data shape : (190,)\n",
      "{'sitting': 19, 'lying': 19, 'standing': 19, 'washing dishes': 19, 'vacuuming': 19, 'sweeping': 19, 'walking': 38, 'ascending stairs': 19, 'descending stairs': 19, 'treadmill running': 19, 'cycling': 19, 'rope jumping': 19}\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1, 4, 7], window_size=5.21, window_overlap=1, resample_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sitting',\n",
       " 'lying',\n",
       " 'standing',\n",
       " 'washing dishes',\n",
       " 'vacuuming',\n",
       " 'sweeping',\n",
       " 'walking',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'treadmill running',\n",
       " 'cycling',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(d1, d2):\n",
    "    return {k: d1[k]+d2[k] for k in d1.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  15177\n",
      "per class count :  {0: 848, 2: 846, 3: 1798, 5: 1422, 6: 3953, 8: 509, 9: 1761, 10: 3550, 11: 490}\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "train_X, train_y = data_dict['train']['X'], data_dict['train']['y']\n",
    "print(\"number of training samples : \", len(train_y))\n",
    "s = np.unique(train_y, return_counts=True)\n",
    "std = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  3795\n",
      "per class count :  {0: 232, 2: 228, 3: 450, 5: 351, 6: 965, 8: 120, 9: 421, 10: 910, 11: 118}\n"
     ]
    }
   ],
   "source": [
    "# Seen Evaluation dataset\n",
    "Seval_X, Seval_y = data_dict['eval-seen']['X'], data_dict['eval-seen']['y']\n",
    "print(\"number of training samples : \", len(Seval_y))\n",
    "s = np.unique(Seval_y, return_counts=True)\n",
    "sed = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1080,\n",
       " 2: 1074,\n",
       " 3: 2248,\n",
       " 5: 1773,\n",
       " 6: 4918,\n",
       " 8: 629,\n",
       " 9: 2182,\n",
       " 10: 4460,\n",
       " 11: 608}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_dict(std, sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  581\n",
      "per class count :  {1: 215, 4: 221, 7: 145}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "Ueval_X, Ueval_y = data_dict['eval-unseen']['X'], data_dict['eval-unseen']['y']\n",
    "print(\"number of training samples : \", len(Ueval_y))\n",
    "s = np.unique(Ueval_y, return_counts=True)\n",
    "utd = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", utd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  2329\n",
      "per class count :  {1: 871, 4: 863, 7: 595}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "test_X, test_y = data_dict['test']['X'], data_dict['test']['y']\n",
    "print(\"number of training samples : \", len(test_y))\n",
    "s = np.unique(test_y, return_counts=True)\n",
    "ued = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", ued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1086, 4: 1084, 7: 740}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_dict(utd, ued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2329"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict['test']['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points :  21844\n",
      "Total number of unseen data :  2910\n",
      "Total number of seen data :  18934\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points : \", len(test_y)+len(Ueval_y)+len(Seval_y)+len(train_y))\n",
    "print(\"Total number of unseen data : \", len(test_y)+len(Ueval_y))\n",
    "print(\"Total number of seen data : \", len(Seval_y)+len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = dataReader.idToLabel\n",
    "seen_classes = data_dict['seen_classes']\n",
    "unseen_classes = data_dict['unseen_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15147, 130, 24)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaLiAcDataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, action_feats, action_classes, seq_len=120):\n",
    "        super(DaLiAcDataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_feats = torch.from_numpy(action_feats)\n",
    "        self.target_feat = torch.from_numpy(action_feats[action_classes, :])\n",
    "        self.seq_len = seq_len\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        y_feat = self.action_feats[target, ...]\n",
    "        attr = self.attributes[target, ...]\n",
    "        return x, y, y_feat, attr, x_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = dataReader.idToLabel\n",
    "seen_classes = data_dict['seen_classes']\n",
    "unseen_classes = data_dict['unseen_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_mat = np.zeros((13, 32))\n",
    "feat_mat = np.zeros((13, 42))\n",
    "train_dt = DaLiAcDataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=seen_classes, seq_len=120)\n",
    "train_dl = DataLoader(train_dt, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 130, 24])\n"
     ]
    }
   ],
   "source": [
    "for b in train_dl:\n",
    "    x, y, yf, attr, xm = b\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
