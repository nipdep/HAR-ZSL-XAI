{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmaking Experiment 006b\n",
    "- benchmarking model architecture -3 with PAMAP2, DaLiAc and UTD-MHAD datasets\n",
    "- Model : Model4\n",
    "- Dataset : PAMAP2\n",
    "- Semantic Space : Glove50\n",
    "- Cross Validation : 5-fold fixed classes\n",
    "- Feature Loss : MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from src.datasets.data import PAMAP2Reader, DaLiAcReader, UTDReader\n",
    "from src.datasets.dataset import PAMAP2Dataset, DaLiAcDataset, UTDDataset\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.ts_transformer import TSTransformerEncoderClassiregressor, RelativeGlobalAttention\n",
    "from src.models.loss import FeatureLoss, AttributeLoss \n",
    "\n",
    "from src.utils.analysis import action_evaluator\n",
    "# from src.running import train_step1, eval_step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"test-001\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"RelativeGlobalAttention\",\n",
    "    \"sem-space\": 'attr',\n",
    "    # model training configs\n",
    "    \"include_attribute_loss\": True, \n",
    "    \"semantic_size\": 64,\n",
    "    \"n_actions\": 18,\n",
    "    \"folding\": True,\n",
    "    \"lr\": 0.001,\n",
    "    \"n_epochs\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 32,\n",
    "    \"semantic_loss\": \"cosine_distance\",\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    # dataset configs\n",
    "    \"window_size\": 5, \n",
    "    \"overlap\": 0.5,\n",
    "    \"seq_len\": 200,\n",
    "    \"seen_split\": 0.2,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6a(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, n_classes, max_len=1024, dropout=0.1):\n",
    "        super(Model6a, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu#_get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.logist = nn.Linear(self.ft_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out1 = self.DenseL2(out)   \n",
    "        out = self.logist(out1)     \n",
    "        return out, out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step1(model, dataloader, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, target_attr, padding_masks = batch\n",
    "            # print(X, targets, target_feat, target_attr)\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            target_attr = target_attr.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "                class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "                feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            pred_class = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "            \n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step1(model, dataloader, loss_module, device, class_names, target_feat_met, phase='seen', l2_reg=False, print_report=True, loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, target_attr, padding_masks = batch\n",
    "            X = X.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            target_attr = target_attr.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            padding_masks = padding_masks.to(device)  # 0s: ignore\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                class_output, feat_output = model(X)\n",
    "                class_loss = loss_module['class'](class_output, targets.squeeze())\n",
    "                feat_loss = loss_module['feature'](feat_output, target_feat)\n",
    "\n",
    "            loss = loss_alpha*class_loss+(1-loss_alpha)*feat_loss\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine \n",
    "            if phase == 'seen':\n",
    "                pred_action = np.argmax(class_output.cpu().detach().numpy(), axis=1)\n",
    "            else:\n",
    "                feat_numpy = torch.sigmoid(feat_output.cpu().detach())\n",
    "                action_probs = cosine_similarity(feat_numpy, target_feat_met)\n",
    "                pred_action = np.argmax(action_probs, axis=1)\n",
    "                \n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action)\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\HAR-ZSL-XAI\\src\\datasets\\data.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "# import PAMAP2 dataset\n",
    "dt = config['dataset']\n",
    "if dt == 'PAMAP2':\n",
    "    dataReader = PAMAP2Reader('./data/PAMAP2_Dataset/Protocol/')\n",
    "elif dt == 'DaLiAc':\n",
    "    dataReader = DaLiAcReader('./data/DaLiAc_Dataset/')\n",
    "elif dt == 'UTD':\n",
    "    dataReader = UTDReader('./data/UTD-MHAD-Inertial/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PAMAP2 attributes\n",
    "if dt == 'PAMAP2':   \n",
    "    fpath = './data/PAMAP2_Dataset/PAMAP2_attributes.json'\n",
    "elif dt == 'DaLiAc':\n",
    "    fpath = './data/DaLiAc_Dataset/DaLiAc_attributes.json'\n",
    "elif dt == 'UTD':\n",
    "    fpath = './data/UTD-MHAD-Inertial/UTD_attributes.json'\n",
    "\n",
    "activity_dict, attribute_dict, attr_mat = load_attribute(fpath)\n",
    "_, attr_size = attr_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic feature space \n",
    "st = config['sem-space']\n",
    "if dt == 'PAMAP2':\n",
    "    if st == 'glove50-v1':\n",
    "        fpath = './data/PAMAP2_Dataset/glove_features.npy'\n",
    "    elif st == 'glove50-v2':\n",
    "        fpath = './data/PAMAP2_Dataset/glove_featureV2.npy'\n",
    "    elif st == 'video-V1':\n",
    "        fpath = './data/PAMAP2_Dataset/video_pca_ft.npy'\n",
    "    elif st == 'word2vec-v1':\n",
    "        fpath = './data/PAMAP2_Dataset/word2vec_features.npy'\n",
    "    elif st == 'word2vec-v2':\n",
    "        fpath = './data/PAMAP2_Dataset/word2vec_featureV2.npy'\n",
    "    elif st == 'attr':\n",
    "        fpath = './data/PAMAP2_Dataset/attribute_ft.npy'\n",
    "elif dt == 'DaLiAc':\n",
    "    if st == 'glove50-v1':\n",
    "        pass \n",
    "    elif st == 'attr':\n",
    "        fpath = './data/DaLiAc_Dataset/attribute_ft.npy'\n",
    "elif dt == 'UTD':\n",
    "    if st == 'glove50-v1':\n",
    "        pass \n",
    "    elif st == 'attr':\n",
    "        fpath = './data/UTD-MHAD-Inertial/attribute_ft.npy'\n",
    "\n",
    "feat_mat = np.load(fpath)\n",
    "n_actions, feat_size = feat_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "if dt == 'PAMAP2':\n",
    "    fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "elif dt == 'DaLiAc':\n",
    "    fold_classes = [['sitting', 'vacuuming', 'descending stairs'], ['lying', 'sweeping', 'treadmill running'], ['standing', 'walking', 'cycling'], ['washing dishes', 'ascending stairs', 'rope jumping']]\n",
    "elif dt == 'UTD':\n",
    "    fold_classes = [['swipe left', 'cross arms', 'draw triangle', 'arm curl', 'jogging in place', 'pick up then throw'], ['swipe right', 'basketball shoot', 'bowling', 'tennis serve', 'walking in place', 'squat'], ['wave', 'draw x', 'boxing', 'two hand push', 'sit to stand'], ['clap', 'draw circle clockwise', 'baseball swing', 'knock', 'stand to sit'], ['throw', 'draw circle counter clockwise', 'tennis swing', 'catch', 'lunge']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset'] == 'PAMAP2':\n",
    "    dts = PAMAP2Dataset\n",
    "elif config['dataset'] == 'DaLiAc':\n",
    "    dts = DaLiAcDataset\n",
    "elif config['dataset'] == 'UTD':\n",
    "    dts = UTDDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fold_cls_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fold_metric_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i, cs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fold_cls_ids):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m16\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFold-\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnseen Classes : \u001b[39m\u001b[39m{\u001b[39;00mfold_classes[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fold_cls_ids' is not defined"
     ]
    }
   ],
   "source": [
    "fold_metric_scores = []\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "    # separate seen/unseen and train/eval \n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=5.21, window_overlap=4.21, resample_freq=20)\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "    # print(\"seen classes > \", seen_classes)\n",
    "    # print(\"unseen classes > \", unseen_classes)\n",
    "    if dt == 'UTD':\n",
    "        train_n = len(data_dict['train']['X'])\n",
    "        seq_len, in_ft = 100, 6\n",
    "    else:\n",
    "        train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "    # build train_dt \n",
    "    # print(seen_classes)\n",
    "    train_dt = dts(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=seen_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt \n",
    "    eval_dt = dts(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=seen_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = dts(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=unseen_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    model = Model6a(in_ft=in_ft, d_model=config['d_model'], num_heads=config['num_heads'], ft_size=feat_size, max_len=seq_len, n_classes=len(seen_classes))\n",
    "    # model = Model1(feat_dim=in_ft, max_len=seq_len, d_model=config['d_model'], n_heads=config['num_heads'], num_layers=2, dim_feedforward=128, ft_size=feat_size)\n",
    "    model.to(device)\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "    loss_module = {'class': nn.CrossEntropyLoss(), 'feature': AttributeLoss()}\n",
    "    best_acc = 0.0\n",
    "    # train the model \n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "        train_metrics = train_step1(model, train_dl, optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.8)\n",
    "        eval_metrics = eval_step1(model, eval_dl, loss_module, device, class_names=[all_classes[i] for i in seen_classes],  target_feat_met=eval_dt.target_feat, phase='seen', print_report=False, loss_alpha=0.8)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['total_accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    if best_acc == 0.0:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step1(model, test_dl, loss_module, device, class_names=[all_classes[i] for i in unseen_classes],  target_feat_met=test_dt.target_feat, phase='unseen', loss_alpha=0.8)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "seen_score_df.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
