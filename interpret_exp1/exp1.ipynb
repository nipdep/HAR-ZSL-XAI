{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Model \n",
    "- ICANN model\n",
    "- with skeleton decoder\n",
    "- and two new reconstruction losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipun\\AppData\\Local\\Temp\\ipykernel_3468\\3997032179.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "# import neptune\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.utils.analysis import action_evaluator\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "from src.utils.losses import *\n",
    "from src.utils.analysis import action_evaluator\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model configurations\n",
    "config = {\n",
    "    # general information\n",
    "    \"experiment-name\": \"Experiment_section-1\", \n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"BiLSTM\",\n",
    "    \"sem-space\": 'I3D embeddings',\n",
    "    # model training configs\n",
    "    \"lr\": 0.0001,\n",
    "    \"imu_alpha\": 0.0001,\n",
    "    \"n_epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    # model configs\n",
    "    \"d_model\": 128, \n",
    "    \"num_heads\": 2,\n",
    "    \"feat_size\": 400, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 5.21, \n",
    "    \"overlap\": 4.21,\n",
    "    \"seq_len\": 20,  # skeleton seq. length\n",
    "    \"seen_split\": 0.1,\n",
    "    \"unseen_split\": 0.8\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\Pose-AE\\interpret_exp1\\..\\src\\datasets\\data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "IMU_data_path = '../data/PAMAP2_Dataset/Protocol/'\n",
    "dataReader = PAMAP2ReaderV2(IMU_data_path)\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_I3D_pkl(loc,feat_size=\"400\"):\n",
    "  if feat_size == \"400\":\n",
    "    feat_index = 1\n",
    "  elif feat_size == \"2048\":\n",
    "    feat_index = 0\n",
    "  else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  with open(loc,\"rb\") as f0:\n",
    "    __data = pickle.load(f0)\n",
    "\n",
    "  label = []\n",
    "  prototype = []\n",
    "  for k,v in __data.items():\n",
    "    label.append(k)\n",
    "    all_arr = [x[feat_index] for x in v]\n",
    "    all_arr = np.array(all_arr).mean(axis=0)\n",
    "    prototype.append(all_arr)\n",
    "\n",
    "  label = np.array(label)\n",
    "  prototype = np.array(prototype)\n",
    "  return {\"activity\":label, \"features\":prototype}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video dataset\n",
    "skeleton_data = np.load('../data/skeleton_k10_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\n",
    "\n",
    "skel_dict = defaultdict(list)\n",
    "for i,c in enumerate(skeleton_classes):\n",
    "    skel_dict[c].append(skeleton_mov[i]) \n",
    "\n",
    "skel_mean = {}\n",
    "for k,v in skel_dict.items():\n",
    "    # print( np.array(v).shape)\n",
    "    skel_mean[k] = np.array(v).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video dataset\n",
    "I3D_data_path  = '../data/PAMAP2_Dataset/video_feat_v2.pkl'\n",
    "video_data = read_I3D_pkl(I3D_data_path,feat_size=\"400\")\n",
    "video_classes, video_feat = video_data['activity'], video_data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 400)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "for i, a in enumerate(video_classes):\n",
    "    action_dict[label2Id[a]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 36)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skel_mean['Nordic walking'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAMAP2Dataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, attribute_dict, skel_dict, action_classes, all_classes, seq_len=120):\n",
    "        super(PAMAP2Dataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attribute_dict = attribute_dict\n",
    "        self.skel_dict = skel_dict\n",
    "        self.seq_len = seq_len\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_classes = action_classes\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.Id2action = dict(zip(action_classes, [all_classes[i] for i in action_classes]))\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        target_name = self.Id2action[target]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        # extraction semantic space generation skeleton sequences\n",
    "        vid_idx = random.choice(self.attribute_dict[target])\n",
    "        y_feat = self.attributes[vid_idx, ...]\n",
    "        y_skel = self.skel_dict[target_name]\n",
    "        return x, y, y_feat, y_skel\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def getClassAttrs(self):\n",
    "        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n",
    "        ft_mat = self.attributes[sampling_idx, ...]\n",
    "        return ft_mat\n",
    "\n",
    "    def getClassFeatures(self):\n",
    "        cls_feat = []\n",
    "        for c in self.action_classes:\n",
    "            idx = self.attribute_dict[c]\n",
    "            cls_feat.append(torch.mean(self.attributes[idx, ...], dim=0))\n",
    "\n",
    "        cls_feat = torch.vstack(cls_feat)\n",
    "        # print(cls_feat.size())\n",
    "        return cls_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, ft_size, num_heads=1, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_ft,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n",
    "        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_forward = out[:,-1, :self.d_model]\n",
    "        out_reverse = out[:, 0, self.d_model:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        out = self.drop(out_reduced)\n",
    "        out = self.act(out)\n",
    "        out = self.fcLayer1(out)\n",
    "        # out = self.fcLayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 400])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_imu = IMUEncoder(in_ft=54, d_model=128, num_heads=2, ft_size=400)\n",
    "\n",
    "imu_input = torch.randn((32, 60, 54))\n",
    "imu_output = sample_imu(imu_input)\n",
    "imu_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_size, hidden_size, linear_filters, embedding_size, num_layers=1, bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "              \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60, 24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dec = BiLSTMDecoder(seq_len=60, input_size=24, hidden_size=128, linear_filters=[64, 128], embedding_size=400, num_layers=1, bidirectional=True)\n",
    "\n",
    "skel_input = torch.randn((32, 400))\n",
    "skel_output = sample_dec(skel_input)\n",
    "skel_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompoundModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CompoundModel, self).__init__()\n",
    "        self.imu_encoder = IMUEncoder(**config['imu_config'])\n",
    "        self.skel_decoder = BiLSTMDecoder(**config['skel_config'])\n",
    "        self.device = config['device']\n",
    "\n",
    "    def forward(self, x):\n",
    "        imu_feat = self.imu_encoder(x)\n",
    "        skel_recon = self.skel_decoder(imu_feat)\n",
    "        return imu_feat, skel_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 400]) torch.Size([32, 60, 24])\n"
     ]
    }
   ],
   "source": [
    "imu_config = {\n",
    "    'in_ft': 54,\n",
    "    'd_model': 256,\n",
    "    'ft_size': 400,\n",
    "    'num_heads': 2,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "decoder_config = {\n",
    "    'input_size': 24,\n",
    "    'seq_len': 60,\n",
    "    'hidden_size': 128,\n",
    "    'linear_filters': [64, 128],\n",
    "    'embedding_size': 400,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': True,\n",
    "    'batch_size': 32,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'imu_config': imu_config,\n",
    "    'skel_config': decoder_config,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "sample_model = CompoundModel(model_config)\n",
    "sample_input = torch.randn((32, 78, 54))\n",
    "output_y, output_recon = sample_model(sample_input)\n",
    "print(output_y.shape, output_recon.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(y_pred, y, feat, loss_fn):\n",
    "    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n",
    "    feat_norm = torch.norm(feat, p=2, dim=1)\n",
    "    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n",
    "    softmax_vec = torch.softmax(norm_vec, dim=1)\n",
    "    output = loss_fn(softmax_vec, y)\n",
    "    pred = torch.argmax(softmax_vec, dim=-1)\n",
    "    return output, pred\n",
    "\n",
    "def loss_reconstruction_calc(y_pred, y_feat, loss_fn=nn.L1Loss(reduction=\"sum\")):\n",
    "    loss = loss_fn(y_pred,y_feat)\n",
    "    return loss\n",
    "\n",
    "def predict_class(y_pred, feat):\n",
    "    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n",
    "    feat_norm = torch.norm(feat, p=2, dim=1)\n",
    "    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n",
    "    softmax_vec = torch.softmax(norm_vec, dim=1)\n",
    "    pred = torch.argmax(softmax_vec, dim=-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, dataset, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7, loss_beta=0.5):\n",
    "    model = model.train()\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    random_selected_feat = dataset.getClassFeatures().to(device)\n",
    "    print(random_selected_feat.shape)\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, skel = batch\n",
    "            # print(X.shape, targets.shape, target_feat.shape, skel.shape)\n",
    "            X = X.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output, recon_output = model(X)\n",
    "                class_loss, class_output = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn=loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n",
    "                recon_loss = loss_module['recon_loss'](recon_output, skel)\n",
    "\n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss + loss_beta*recon_loss\n",
    "            # class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(targets)\n",
    "                epoch_loss += loss.item()  # add total loss of batch\n",
    "\n",
    "            # convert feature vector into action class\n",
    "            # using cosine\n",
    "            pred_class = class_output.cpu().detach().numpy()\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, dataloader,dataset, loss_module, device, class_names,  phase='seen', l2_reg=False, print_report=False, show_plot=False, loss_alpha=0.7, loss_beta=0.5):\n",
    "    model = model.eval()\n",
    "    random_selected_feat = dataset.getClassFeatures().to(device)\n",
    "    epoch_loss = 0  # total loss of epoch\n",
    "    total_samples = 0  # total samples in epoch\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, targets, target_feat, skel = batch\n",
    "            X = X.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "            target_feat = target_feat.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "            # with autocast():\n",
    "                feat_output, recon_output = model(X)\n",
    "                class_loss, class_output = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn =loss_module['class'] )\n",
    "                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n",
    "                recon_loss = loss_module['recon_loss'](recon_output, skel)\n",
    "\n",
    "            #loss = cross_entropy_loss\n",
    "            loss = feat_loss + loss_alpha*class_loss + loss_beta*recon_loss\n",
    "            # class_output = predict_class(feat_output,random_selected_feat)\n",
    "\n",
    "            pred_action = class_output\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metrics['samples'] += len(targets)\n",
    "                metrics['loss'] += loss.item()  # add total loss of batch\n",
    "                metrics['feat. loss'] += feat_loss.item()\n",
    "                metrics['classi. loss'] += class_loss.item()\n",
    "\n",
    "            per_batch['targets'].append(targets.cpu().numpy())\n",
    "            per_batch['predictions'].append(pred_action.cpu().numpy())\n",
    "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    metrics_dict.update(metrics)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(df):\n",
    "    df['loss'] = df['loss']/df['samples']\n",
    "    df['feat. loss'] = df['feat. loss']/df['samples']\n",
    "    df['classi. loss'] = df['classi. loss']/df['samples']\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=4)\n",
    "    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(fold, phase, metrics):\n",
    "    for m, v in metrics.items():\n",
    "        if fold == 'global':\n",
    "            run[f'global/{m}'].log(v)\n",
    "        else:\n",
    "            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n",
      "unseen classes >  [7, 15, 2, 10]\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d0b10dcb448fd9282c69b3d13083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968700b75dce45389675080aae1ad3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60dfc5c86614aaa85a079e943363b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seen:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.331\n",
      "\n",
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758fce5a924d430c987f3a53e1540a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e4abbbd10945a7a207dea5d9b6d4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seen:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.517\n",
      "\n",
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72285a7c494b4da9990b10d348c00267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2690cf73fa94ed1b13f12c1e41788dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seen:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.680\n",
      "\n",
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34849ccbc61047e68650ae6b1825b2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff135e279fc4294b21b8622386fee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seen:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.792\n",
      "\n",
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a48aae93494ec19dc3704ee5f5e8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71116b78a042b1882aac4375583a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "seen:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.822\n",
      "\n",
      "torch.Size([14, 400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260a93922db4156960c4be4c447f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/294 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m train_data \u001b[39m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m]), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epoch\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 65\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names\u001b[39m=\u001b[39;49m[all_classes[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m seen_classes], phase\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, loss_alpha\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m)\n\u001b[0;32m     66\u001b[0m     train_metrics[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m epoch\n\u001b[0;32m     67\u001b[0m     train_metrics[\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[72], line 9\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, dataset, optimizer, loss_module, device, class_names, phase, l2_reg, loss_alpha, loss_beta)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(random_selected_feat\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(dataloader, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m, desc\u001b[39m=\u001b[39mphase) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tepoch:\n\u001b[0;32m     10\u001b[0m         X, targets, target_feat, skel \u001b[39m=\u001b[39m batch\n\u001b[0;32m     11\u001b[0m         \u001b[39m# print(X.shape, targets.shape, target_feat.shape, skel.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run['parameters'] = config\n",
    "fold_metric_scores = []\n",
    "\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['seq_len'])\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = data_dict['seen_classes']\n",
    "    unseen_classes = data_dict['unseen_classes']\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=seen_classes, all_classes=all_classes, seq_len=100)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=seen_classes, all_classes=all_classes, seq_len=100)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=unseen_classes, all_classes=all_classes, seq_len=100)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    imu_config = {\n",
    "        'in_ft': in_ft,\n",
    "        'd_model': config['d_model'],\n",
    "        'ft_size': config['feat_size'],\n",
    "        'num_heads': config['num_heads'],\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "\n",
    "    decoder_config = {\n",
    "        'input_size': 36,\n",
    "        'seq_len': 60,\n",
    "        'hidden_size': 128,\n",
    "        'linear_filters': [64, 128],\n",
    "        'embedding_size': config['feat_size'],\n",
    "        'num_layers': 1,\n",
    "        'bidirectional': True,\n",
    "        'batch_size': config['batch_size'],\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "    model_config = {\n",
    "        'imu_config': imu_config,\n",
    "        'skel_config': decoder_config,\n",
    "        'device': 'cpu'\n",
    "    }\n",
    "    model = CompoundModel(model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'], weight_decay=1e-6)\n",
    "    loss_module = {'class': nn.CrossEntropyLoss(reduction=\"sum\"), 'feature': nn.L1Loss(reduction=\"sum\"), 'recon_loss': nn.MSELoss()}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # train the model \n",
    "    train_data = []\n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "    \n",
    "        train_metrics = train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.0001)\n",
    "        train_metrics['epoch'] = epoch\n",
    "        train_metrics['phase'] = 'train'\n",
    "        train_data.append(train_metrics)\n",
    "        # log(i, 'train', train_metrics)\n",
    "\n",
    "        eval_metrics = eval_step(model, eval_dl, eval_dt,loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='seen', loss_alpha=0.0001, print_report=False, show_plot=False)\n",
    "        eval_metrics['epoch'] = epoch \n",
    "        eval_metrics['phase'] = 'valid'\n",
    "        train_data.append(eval_metrics)\n",
    "        # log(i, 'eval', eval_metrics)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    train_df = pd.DataFrame().from_records(train_data)\n",
    "    plot_curves(train_df)\n",
    "\n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "    # save_model(model,notebook_iden,model_iden,i)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = eval_step(model, test_dl,test_dt, loss_module, device, class_names=[all_classes[i] for i in unseen_classes], phase='unseen', loss_alpha=0.0001, print_report=True, show_plot=True)\n",
    "    test_metrics['N'] = len(unseen_classes)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    # log('test', i, test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "weighted_score_df = seen_score_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].multiply(seen_score_df[\"N\"], axis=\"index\")\n",
    "final_results = weighted_score_df.sum()/seen_score_df['N'].sum()\n",
    "print(final_results)\n",
    "# log('global', '',final_results.to_dict())\n",
    "# run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
