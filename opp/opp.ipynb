{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPPReader(object):\n",
    "    def __init__(self, root_path):\n",
    "        self.root_path = root_path\n",
    "        self.readOPP()\n",
    "\n",
    "    def readFile(self, file_path, active_cols):\n",
    "        all_data = {\"data\": {}, \"target\": {}, 'collection': []}\n",
    "        prev_action = -1\n",
    "        starting = True\n",
    "        # action_seq = []\n",
    "        accepted_actions = [406516, 406517, 404516, 404517, 406520, 404520, 406505, 404505, 406519, 404519, 406511, 404511, 406508, 404508, 408512, 407521, 405506]\n",
    "        action_ID = 0\n",
    "        cols = list(range(37,133))\n",
    "        for l in open(file_path).readlines():\n",
    "            s = l.rstrip('\\n').split(' ')\n",
    "            act = int(s[-1])\n",
    "            if act in accepted_actions:\n",
    "                if (prev_action != act):\n",
    "                    if not(starting):\n",
    "                        df = pd.DataFrame(action_seq)\n",
    "                        intep_df = df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "                        intep_data = intep_df.values[:, cols]\n",
    "                        # print(intep_data[:, 1:])\n",
    "                        k = np.isnan(intep_data[:, :]).mean()\n",
    "                        if k == 0:\n",
    "                            # intep_data = action_seq\n",
    "                            all_data['data'][action_ID] = np.array(intep_data)\n",
    "                            # print(all_data['data'][action_ID].shape)\n",
    "                            all_data['target'][action_ID] = prev_action\n",
    "                            action_ID+=1\n",
    "                    action_seq = []\n",
    "                else:\n",
    "                    starting = False\n",
    "\n",
    "                data_seq = np.array(s[:-1]).astype(np.float16)                # data_seq[np.isnan(data_seq)] = 0\n",
    "                action_seq.append(data_seq)\n",
    "                prev_action = act\n",
    "                \n",
    "                # print(prev_action)\n",
    "                all_data['collection'].append(data_seq)\n",
    "        else: \n",
    "            if len(action_seq) > 1:\n",
    "                df = pd.DataFrame(action_seq)\n",
    "                intep_df = df.interpolate(method='linear', limit_direction='backward', axis=0)\n",
    "                intep_data = intep_df.values[:, cols]\n",
    "                # print(np.isnan(intep_data[:, :]).mean(axis=0))\n",
    "                if np.isnan(intep_data).sum() == 0:\n",
    "                    all_data['data'][action_ID] = np.array(intep_data)\n",
    "                    all_data['target'][action_ID] = prev_action\n",
    "        return all_data\n",
    "\n",
    "    def readOPPFiles(self, filelist, labelToId):\n",
    "        data = []\n",
    "        labels = []\n",
    "        collection = []\n",
    "        accepted_cols = list(range(1, 101))\n",
    "        for i, fpath in enumerate(filelist):\n",
    "            print('Reading file %d of %d' % (i+1, len(filelist)))\n",
    "            # fpath = os.path.join(self.root_path, filename)\n",
    "            file_data = self.readFile(fpath, accepted_cols)\n",
    "            # print(np.array(list(file_data['data'].values())).shape)\n",
    "            data.extend(list(file_data['data'].values()))\n",
    "            labels.extend(list(file_data['target'].values()))\n",
    "            collection.extend(file_data['collection'])\n",
    "        return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n",
    "\n",
    "    def readOPP(self):\n",
    "        files = []\n",
    "        for p in glob(f'{self.root_path}/S*-ADL*.dat'):\n",
    "            files.append(p)\n",
    "            # files = [f'dataset_{i}.txt' for i in range(1, 20)]\n",
    "            \n",
    "        label_map = [\n",
    "            (406516, 'Open Door 1'),\n",
    "            (406517, 'Open Door 2'),\n",
    "            (404516, 'Close Door 1'),\n",
    "            (404517, 'Close Door 2'),\n",
    "            (406520, 'Open Fridge'),\n",
    "            (404520, 'Close Fridge'),\n",
    "            (406505, 'Open Dishwasher'),\n",
    "            (404505, 'Close Dishwasher'),\n",
    "            (406519, 'Open Drawer 1'),\n",
    "            (404519, 'Close Drawer 1'),\n",
    "            (406511, 'Open Drawer 2'),\n",
    "            (404511, 'Close Drawer 2'),\n",
    "            (406508, 'Open Drawer 3'),\n",
    "            (404508, 'Close Drawer 3'),\n",
    "            (408512, 'Clean Table'),\n",
    "            (407521, 'Drink from Cup'),\n",
    "            (405506, 'Toggle Switch'),\n",
    "        ]\n",
    "        labelToId = {x[0]: i for i, x in enumerate(label_map)}\n",
    "        # print \"label2id=\",labelToId\n",
    "        idToLabel = [x[1] for x in label_map]\n",
    "        # print \"id2label=\",idToLabel\n",
    "        # print \"cols\",cols\n",
    "        self.data, self.targets, self.all_data = self.readOPPFiles(files, labelToId)\n",
    "        # print(self.data)\n",
    "        # nan_perc = np.isnan(self.data).astype(int).mean()\n",
    "        # print(\"null value percentage \", nan_perc)\n",
    "        # f = lambda x: labelToId[x]\n",
    "        self.targets = np.array([labelToId[i] for i in list(self.targets)])\n",
    "        self.label_map = label_map\n",
    "        self.idToLabel = idToLabel\n",
    "        # return data, idToLabel\n",
    "\n",
    "    def dataTableOptimizerUpdated(self, mat_file):\n",
    "        our_data = mat_file['d_iner']\n",
    "        data = []\n",
    "        frame_size = len(our_data[0][0])-1\n",
    "        for each in range(0,frame_size):\n",
    "            data_flatten = our_data[:,:,each].flatten()\n",
    "            data_flatten = data_flatten\n",
    "            data.append(data_flatten)\n",
    "        return data,frame_size\n",
    "\n",
    "    def resample(self, signal, freq=10):\n",
    "        step_size = int(30/freq)\n",
    "        seq_len, _ = signal.shape \n",
    "        resample_indx = np.arange(0, seq_len, step_size)\n",
    "        resampled_sig = signal[resample_indx, :]\n",
    "        return resampled_sig\n",
    "\n",
    "    def windowing(self, signal, window_len, overlap):\n",
    "        seq_len = int(window_len*30) # 30Hz compensation \n",
    "        overlap_len = int(overlap*30) # 30Hz\n",
    "        l, _ = signal.shape\n",
    "        if l > seq_len:\n",
    "            windowing_points = np.arange(start=0, stop=l-seq_len, step=seq_len-overlap_len, dtype=int)[:-1]\n",
    "\n",
    "            windows = [signal[p:p+seq_len, :] for p in windowing_points]\n",
    "            # print(np.array(windows).shape)\n",
    "        else:\n",
    "            windows = []\n",
    "        return windows\n",
    "\n",
    "    def resampling(self, data, targets, window_size, window_overlap, resample_freq):\n",
    "        assert len(data) == len(targets), \"# action data & # action labels are not matching\"\n",
    "        all_data, all_ids, all_labels = [], [], []\n",
    "        for i, d in enumerate(data):\n",
    "            # print(\">>>>>>>>>>>>>>>  \", np.isnan(d).mean())\n",
    "            label = targets[i]\n",
    "            windows = self.windowing(d, window_size, window_overlap)\n",
    "            for w in windows:\n",
    "                # print(np.isnan(w).mean(), label, i)\n",
    "                resample_sig = w#self.resample(w, resample_freq)\n",
    "                # print(np.isnan(resample_sig).mean(), label, i)\n",
    "                # print(resample_sig.shape)\n",
    "                all_data.append(resample_sig)\n",
    "                all_ids.append(i+1)\n",
    "                all_labels.append(label)\n",
    "\n",
    "        return all_data, all_ids, all_labels\n",
    "\n",
    "    def generate(self, unseen_classes, window_size=5.21, window_overlap=1, resample_freq=20, seen_ratio=0.2, unseen_ratio=0.8):\n",
    "        # assert all([i in list(self.label_map.keys()) for i in unseen_classes]), \"Unknown Class label!\"\n",
    "        seen_classes = [i for i in range(len(self.idToLabel)) if i not in unseen_classes]\n",
    "        unseen_mask = np.in1d(self.targets, unseen_classes)\n",
    "        \n",
    "        # build seen dataset \n",
    "        seen_data = self.data[np.invert(unseen_mask)]\n",
    "        seen_targets = self.targets[np.invert(unseen_mask)]\n",
    "        print(f\"data shape : {self.data.shape}, seen_data shape : {seen_data.shape}\")\n",
    "        ids, cnts = np.unique(self.targets, return_counts=True)\n",
    "        print({self.idToLabel[ids[e]]: cnts[e] for e in range(len(ids))})\n",
    "        \n",
    "        # build unseen dataset\n",
    "        unseen_data = self.data[unseen_mask]\n",
    "        unseen_targets = self.targets[unseen_mask]\n",
    "\n",
    "        # # resampling seen and unseen datasets \n",
    "        seen_data, seen_ids, seen_targets = self.resampling(seen_data, seen_targets, window_size, window_overlap, resample_freq)\n",
    "        unseen_data, unseen_ids, unseen_targets = self.resampling(unseen_data, unseen_targets, window_size, window_overlap, resample_freq)\n",
    "\n",
    "        seen_data, seen_targets = np.array(seen_data), np.array(seen_targets)\n",
    "        unseen_data, unseen_targets = np.array(unseen_data), np.array(unseen_targets)\n",
    "       # train-val split\n",
    "        seen_index = list(range(len(seen_targets)))\n",
    "        random.shuffle(seen_index)\n",
    "        split_point = int((1-seen_ratio)*len(seen_index))\n",
    "        fst_index, sec_index = seen_index[:split_point], seen_index[split_point:]\n",
    "        # print(fst_index)\n",
    "        # print(type(fst_index), type(sec_index), type(seen_data), type(seen_targets))\n",
    "        X_seen_train, X_seen_val = seen_data[fst_index, :], seen_data[sec_index, :]\n",
    "        y_seen_train, y_seen_val = seen_targets[fst_index], seen_targets[sec_index]\n",
    "        \n",
    "    \n",
    "        data = {'train': {\n",
    "                        'X': X_seen_train,\n",
    "                        'y': y_seen_train\n",
    "                        },\n",
    "                'eval-seen':{\n",
    "                        'X': X_seen_val,\n",
    "                        'y': y_seen_val\n",
    "                        },\n",
    "                'test': {\n",
    "                        'X': unseen_data,\n",
    "                        'y': unseen_targets\n",
    "                        },\n",
    "                'seen_classes': seen_classes,\n",
    "                'unseen_classes': unseen_classes\n",
    "                }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 20\n",
      "Reading file 2 of 20\n",
      "Reading file 3 of 20\n",
      "Reading file 4 of 20\n",
      "Reading file 5 of 20\n",
      "Reading file 6 of 20\n",
      "Reading file 7 of 20\n",
      "Reading file 8 of 20\n",
      "Reading file 9 of 20\n",
      "Reading file 10 of 20\n",
      "Reading file 11 of 20\n",
      "Reading file 12 of 20\n",
      "Reading file 13 of 20\n",
      "Reading file 14 of 20\n",
      "Reading file 15 of 20\n",
      "Reading file 16 of 20\n",
      "Reading file 17 of 20\n",
      "Reading file 18 of 20\n",
      "Reading file 19 of 20\n",
      "Reading file 20 of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deela\\AppData\\Local\\Temp\\ipykernel_10844\\1743490314.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = OPPReader(root_path='../data/OPP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (978,), seen_data shape : (756,)\n",
      "{'Open Door 1': 45, 'Open Door 2': 43, 'Close Door 1': 38, 'Close Door 2': 40, 'Open Fridge': 129, 'Close Fridge': 131, 'Open Dishwasher': 54, 'Close Dishwasher': 53, 'Open Drawer 1': 50, 'Close Drawer 1': 49, 'Open Drawer 2': 45, 'Close Drawer 2': 44, 'Open Drawer 3': 56, 'Close Drawer 3': 57, 'Clean Table': 17, 'Drink from Cup': 56, 'Toggle Switch': 71}\n"
     ]
    }
   ],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[2, 5, 7], window_size=1, window_overlap=0.5, resample_freq=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data_dict['train']['X']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3118, 30, 96)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Open Door 1',\n",
       " 'Open Door 2',\n",
       " 'Close Door 1',\n",
       " 'Close Door 2',\n",
       " 'Open Fridge',\n",
       " 'Close Fridge',\n",
       " 'Open Dishwasher',\n",
       " 'Close Dishwasher',\n",
       " 'Open Drawer 1',\n",
       " 'Close Drawer 1',\n",
       " 'Open Drawer 2',\n",
       " 'Close Drawer 2',\n",
       " 'Open Drawer 3',\n",
       " 'Close Drawer 3',\n",
       " 'Clean Table',\n",
       " 'Drink from Cup',\n",
       " 'Toggle Switch']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPPDataset(Dataset):\n",
    "    def __init__(self, data, actions, action_classes, seq_len=120):\n",
    "        super(OPPDataset, self).__init__()\n",
    "        cols = list(range(4, 9))+list(range(16,18))+list(range(22,34))+list(range(37,133))\n",
    "        self.data = torch.from_numpy(data)[:, :, cols] # get only subject related IMU features\n",
    "        self.actions = actions\n",
    "        # self.attributes = torch.from_numpy(attributes)\n",
    "        # self.action_feats = torch.from_numpy(action_feats)\n",
    "        # self.target_feat = torch.from_numpy(action_feats[action_classes, :])\n",
    "        self.seq_len = seq_len\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        # y_feat = self.action_feats[target, ...]\n",
    "        # attr = self.attributes[target, ...]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[2, 5, 7], seen_ratio=0.2, unseen_ratio=0.8, window_size=1, window_overlap=0.5, resample_freq=30)\n",
    "sample_dt = OPPDataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], action_classes=data_dict['seen_classes'], seq_len=100)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for d in sample_dl:\n",
    "    print(d[0].shape, torch.isnan(d[0]).numpy().mean(axis=0).mean(axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02675010550247693"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(data_dict['train']['X']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dict(d1, d2):\n",
    "    return {k: d1[k]+d2[k] for k in d1.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  3118\n",
      "per class count :  {0: 238, 1: 262, 3: 192, 4: 350, 6: 123, 8: 65, 9: 53, 10: 58, 11: 40, 12: 90, 13: 79, 14: 272, 15: 1191, 16: 105}\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "train_X, train_y = data_dict['train']['X'], data_dict['train']['y']\n",
    "print(\"number of training samples : \", len(train_y))\n",
    "s = np.unique(train_y, return_counts=True)\n",
    "std = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  780\n",
      "per class count :  {0: 59, 1: 54, 3: 47, 4: 87, 6: 40, 8: 14, 9: 7, 10: 13, 11: 10, 12: 28, 13: 26, 14: 80, 15: 299, 16: 16}\n"
     ]
    }
   ],
   "source": [
    "# Seen Evaluation dataset\n",
    "Seval_X, Seval_y = data_dict['eval-seen']['X'], data_dict['eval-seen']['y']\n",
    "print(\"number of training samples : \", len(Seval_y))\n",
    "s = np.unique(Seval_y, return_counts=True)\n",
    "sed = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'eval-unseen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Unseen Eval dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Ueval_X, Ueval_y \u001b[39m=\u001b[39m data_dict[\u001b[39m'\u001b[39;49m\u001b[39meval-unseen\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m], data_dict[\u001b[39m'\u001b[39m\u001b[39meval-unseen\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnumber of training samples : \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(Ueval_y))\n\u001b[0;32m      4\u001b[0m s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(Ueval_y, return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval-unseen'"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "Ueval_X, Ueval_y = data_dict['eval-unseen']['X'], data_dict['eval-unseen']['y']\n",
    "print(\"number of training samples : \", len(Ueval_y))\n",
    "s = np.unique(Ueval_y, return_counts=True)\n",
    "utd = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", utd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples :  742\n",
      "per class count :  {2: 235, 5: 360, 7: 147}\n"
     ]
    }
   ],
   "source": [
    "# Unseen Eval dataset\n",
    "test_X, test_y = data_dict['test']['X'], data_dict['test']['y']\n",
    "print(\"number of training samples : \", len(test_y))\n",
    "s = np.unique(test_y, return_counts=True)\n",
    "ued = dict(zip(s[0], s[1]))\n",
    "print(\"per class count : \", ued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sum_dict(utd, ued)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utd' is not defined"
     ]
    }
   ],
   "source": [
    "sum_dict(utd, ued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = np.concatenate([train_y, Seval_y, test_y])\n",
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n",
       " array([ 297,  316,  235,  239,  437,  360,  163,  147,   79,   60,   71,\n",
       "          50,  118,  105,  352, 1490,  121], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict['test']['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points :  4640\n",
      "Total number of unseen data :  742\n",
      "Total number of seen data :  3898\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of data points : \", len(test_y)+len(Seval_y)+len(train_y))\n",
    "print(\"Total number of unseen data : \", len(test_y))\n",
    "print(\"Total number of seen data : \", len(Seval_y)+len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = dataReader.idToLabel\n",
    "seen_classes = data_dict['seen_classes']\n",
    "unseen_classes = data_dict['unseen_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3118, 30, 96)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['train']['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaLiAcDataset(Dataset):\n",
    "    def __init__(self, data, actions, attributes, action_feats, action_classes, seq_len=120):\n",
    "        super(DaLiAcDataset, self).__init__()\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.actions = actions\n",
    "        self.attributes = torch.from_numpy(attributes)\n",
    "        self.action_feats = torch.from_numpy(action_feats)\n",
    "        self.target_feat = torch.from_numpy(action_feats[action_classes, :])\n",
    "        self.seq_len = seq_len\n",
    "        # build action to id mapping dict\n",
    "        self.n_action = len(self.actions)\n",
    "        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind, ...]\n",
    "        x_mask = np.array([0]) #self.padding_mask[ind, ...]\n",
    "        target = self.actions[ind]\n",
    "        y = torch.from_numpy(np.array([self.action2Id[target]]))\n",
    "        y_feat = self.action_feats[target, ...]\n",
    "        attr = self.attributes[target, ...]\n",
    "        return x, y, y_feat, attr, x_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes = dataReader.idToLabel\n",
    "seen_classes = data_dict['seen_classes']\n",
    "unseen_classes = data_dict['unseen_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_mat = np.zeros((13, 32))\n",
    "feat_mat = np.zeros((13, 42))\n",
    "train_dt = DaLiAcDataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=attr_mat, action_feats=feat_mat, action_classes=seen_classes, seq_len=120)\n",
    "train_dl = DataLoader(train_dt, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 130, 24])\n"
     ]
    }
   ],
   "source": [
    "for b in train_dl:\n",
    "    x, y, yf, attr, xm = b\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IMU feature columns in OPP \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
