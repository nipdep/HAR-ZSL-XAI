{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:19:10,237 | INFO : Loading packages ...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Loading packages ...\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from argparse import Namespace\n",
    "\n",
    "# 3rd party packages\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Project modules\n",
    "from src.options import Options\n",
    "from src.running import setup, pipeline_factory, validate, check_progress, NEG_METRICS\n",
    "from src.utils import utils\n",
    "from src.datasets.data import data_factory, Normalizer\n",
    "from src.datasets.datasplit import split_dataset\n",
    "from src.models.ts_transformer import model_factory\n",
    "from src.models.loss import get_loss_module\n",
    "from src.optimizers import get_optimizer\n",
    "\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/FYP-Group22/Transformer-SSC/e/TRAN-19\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"FYP-Group22/Transformer-SSC\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:22:05,057 | INFO : Stored configuration file in './tmp\\first_test'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_dir': './tmp\\\\first_test',\n",
       " 'seed': 123,\n",
       " 'gpu': '0',\n",
       " 'n_proc': 1,\n",
       " 'load_model': None,\n",
       " 'test_only': None,\n",
       " 'config_filepath': None,\n",
       " 'task': 'imputation',\n",
       " 'experiment_name': 'first_test',\n",
       " 'no_timestamp': True,\n",
       " 'limit_size': 300,\n",
       " 'data_class': 'kuhar',\n",
       " 'data_dir': '../../Data/KU-HAR_time_domain_subsamples_20750x300.csv',\n",
       " 'val_ratio': 0.1,\n",
       " 'test_ratio': 0.1,\n",
       " 'norm_from': False,\n",
       " 'normalization': 'standardization',\n",
       " 'record_file': './tmp/Imputation_records.xls',\n",
       " 'records_file': './tmp/records.xls',\n",
       " 'num_workers': 0,\n",
       " 'console': True,\n",
       " 'save_all': False,\n",
       " 'comment': 'mvts_transformer | KU-HAR dataset Dynamic Actions | nipdep 1DConv processing layer | first test',\n",
       " 'test_pattern': False,\n",
       " 'val_pattern': False,\n",
       " 'test_from': False,\n",
       " 'freeze': False,\n",
       " 'masking_ratio': 0.15,\n",
       " 'mean_mask_length': 5,\n",
       " 'mask_mode': 'seperate',\n",
       " 'mask_distribution': 'geometric',\n",
       " 'exclude_feats': None,\n",
       " 'mask_feats': '0, 1',\n",
       " 'start_hint': 0.0,\n",
       " 'end_hint': 0.0,\n",
       " 'harden': True,\n",
       " 'model': 'transformer',\n",
       " 'pos_encoding': 'learnable',\n",
       " 'd_model': 128,\n",
       " 'dim_feedforward': 256,\n",
       " 'num_heads': 8,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.1,\n",
       " 'activation': 'relu',\n",
       " 'normalization_layer': 'BatchNorm',\n",
       " 'data_window_len': None,\n",
       " 'max_seq_len': 300,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.0005,\n",
       " 'val_interval': 1,\n",
       " 'lr_step': '1000',\n",
       " 'lr_factor': '0.1',\n",
       " 'l2_reg': 0,\n",
       " 'global_reg': True,\n",
       " 'key_metric': 'loss',\n",
       " 'optimizer': 'Adam',\n",
       " 'batch_size': 32,\n",
       " 'print_interval': 1,\n",
       " 'initial_timestamp': '2022-09-16_10-22-05',\n",
       " 'save_dir': './tmp\\\\first_test\\\\checkpoints',\n",
       " 'pred_dir': './tmp\\\\first_test\\\\predictions',\n",
       " 'tensorboard_dir': './tmp\\\\first_test\\\\tb_summaries'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"output_dir\"] = './tmp'\n",
    "config[\"seed\"] = 123\n",
    "config[\"gpu\"] = \"0\" # activate gpu > o, on cpu > -1\n",
    "config[\"n_proc\"] = 1\n",
    "\n",
    "# loading pre-trained model\n",
    "config[\"load_model\"] = None\n",
    "config[\"test_only\"] = None\n",
    "config[\"config_filepath\"] = None\n",
    "\n",
    "# experiment config\n",
    "config[\"task\"] = \"imputation\"\n",
    "config[\"experiment_name\"] = \"first_test\"\n",
    "config[\"no_timestamp\"] = True\n",
    "\n",
    "# dataset settings\n",
    "config[\"limit_size\"] = 300 \n",
    "config[\"data_class\"] = 'kuhar'\n",
    "config[\"data_dir\"] = '../../Data/KU-HAR_time_domain_subsamples_20750x300.csv'\n",
    "config[\"val_ratio\"] = 0.1 \n",
    "config[\"test_ratio\"] = 0.1 \n",
    "config[\"norm_from\"] = False \n",
    "config['normalization'] = 'standardization'\n",
    "config[\"record_file\"] = \"./tmp/Imputation_records.xls\"\n",
    "config[\"records_file\"] = \"./tmp/records.xls\"\n",
    "config[\"num_workers\"] = 0\n",
    "config[\"console\"] = True\n",
    "config[\"save_all\"] = False\n",
    "config[\"comment\"] = \"mvts_transformer | KU-HAR dataset Dynamic Actions | nipdep 1DConv processing layer | first test\"\n",
    "\n",
    "# test, val from seperate files \n",
    "config[\"test_pattern\"] = False \n",
    "config[\"val_pattern\"] = False \n",
    "config[\"test_from\"] = False\n",
    "\n",
    "# freeze model weight for fine-tunning\n",
    "config[\"freeze\"] = False\n",
    "\n",
    "# if task is a imputation \n",
    "config[\"masking_ratio\"] = 0.15\n",
    "config[\"mean_mask_length\"] = 5\n",
    "config[\"mask_mode\"] = \"seperate\"\n",
    "config[\"mask_distribution\"] = \"geometric\"\n",
    "config[\"exclude_feats\"] = None \n",
    "config[\"mask_feats\"] = '0, 1'\n",
    "config[\"start_hint\"] = 0.0\n",
    "config[\"end_hint\"] = 0.0 \n",
    "config[\"harden\"] = True\n",
    "\n",
    "# model parameters\n",
    "config['model'] = 'transformer'\n",
    "config[\"pos_encoding\"] = \"learnable\"\n",
    "config[\"d_model\"] = 128 \n",
    "config[\"dim_feedforward\"] = 256\n",
    "config[\"num_heads\"] = 8\n",
    "config[\"num_layers\"] = 3\n",
    "config[\"dropout\"] = 0.1\n",
    "config[\"activation\"] = 'relu'\n",
    "config[\"normalization_layer\"] = \"BatchNorm\"\n",
    "config[\"data_window_len\"] = None \n",
    "config[\"max_seq_len\"] = 300\n",
    "\n",
    "# model training parameters\n",
    "config[\"epochs\"] = 10\n",
    "config[\"lr\"] = 0.0005\n",
    "config[\"val_interval\"] = 1\n",
    "config[\"lr_step\"] = '1000'\n",
    "config[\"lr_factor\"] = '0.1'\n",
    "config[\"l2_reg\"] = 0\n",
    "config[\"global_reg\"] = True\n",
    "config[\"key_metric\"] = \"loss\"\n",
    "config[\"optimizer\"] = \"Adam\"\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"print_interval\"] = 1\n",
    "\n",
    "setup(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"parameters\"] = config\n",
    "# config['class_names'] = ['Talk-sit', 'Talk-stand', 'Stand-sit', 'Lay-stand', 'Pick', 'Jump', 'Push-up', 'Sit-up', 'Walk', 'Walk-backward', 'Walk-circle', 'Run', 'Stair-up', 'Stair-down', 'Table-tennis']\n",
    "config['class_names'] = ['Stand', 'Sit', 'Talk-sit', 'Talk-stand', 'Stand-sit', 'Lay', 'Lay-stand', 'Pick', 'Jump', 'Push-up', 'Sit-up', 'Walk', 'Walk-backward', 'Walk-circle', 'Run', 'Stair-up', 'Stair-down', 'Table-tennis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:19:19,585 | INFO : Running:\n",
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\ipykernel_launcher.py --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"4334a332-653a-4c63-bb0e-b94496ffd1cf\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=c:\\Users\\deela\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-5100Er5FAaxyLBFx.json\n",
      "\n",
      "2022-09-16 10:19:19,589 | INFO : Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "total_epoch_time = 0\n",
    "total_eval_time = 0\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Add file logging besides stdout\n",
    "file_handler = logging.FileHandler(os.path.join(config['output_dir'], 'output.log'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info('Running:\\n{}\\n'.format(' '.join(sys.argv)))  # command used to run\n",
    "\n",
    "if config['seed'] is not None:\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(\"Using device: {}\".format(device))\n",
    "if device == 'cuda':\n",
    "    logger.info(\"Device index: {}\".format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:19:20,931 | INFO : Loading and preprocessing data ...\n"
     ]
    }
   ],
   "source": [
    "# Build data\n",
    "logger.info(\"Loading and preprocessing data ...\")\n",
    "data_class = data_factory[config['data_class']]\n",
    "my_data = data_class(config['data_dir'], n_proc=config['n_proc'], limit_size=config['limit_size'], config=config, filter_classes=[])\n",
    "feat_dim = my_data.feature_df.shape[1]  # dimensionality of data features\n",
    "if config['task'] == 'classification':\n",
    "    validation_method = 'StratifiedShuffleSplit'\n",
    "    labels = my_data.labels_df.label.values\n",
    "    print(labels)\n",
    "else:\n",
    "    validation_method = 'ShuffleSplit'\n",
    "    labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:21:08,505 | INFO : 16806 samples may be used for training\n",
      "2022-09-16 10:21:08,506 | INFO : 1868 samples will be used for validation\n",
      "2022-09-16 10:21:08,507 | INFO : 2075 samples will be used for testing\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "test_data = my_data\n",
    "test_indices = None  # will be converted to empty list in `split_dataset`, if also test_set_ratio == 0\n",
    "val_data = my_data\n",
    "val_indices = []\n",
    "if config['test_pattern']:  # used if test data come from different files / file patterns\n",
    "    test_data = data_class(config['data_dir'], n_proc=-1, config=config)\n",
    "    test_indices = test_data.all_IDs\n",
    "if config['test_from']:  # load test IDs directly from file, if available, otherwise use `test_set_ratio`. Can work together with `test_pattern`\n",
    "    test_indices = list(set([line.rstrip() for line in open(config['test_from']).readlines()]))\n",
    "    try:\n",
    "        test_indices = [int(ind) for ind in test_indices]  # integer indices\n",
    "    except ValueError:\n",
    "        pass  # in case indices are non-integers\n",
    "    logger.info(\"Loaded {} test IDs from file: '{}'\".format(len(test_indices), config['test_from']))\n",
    "if config['val_pattern']:  # used if val data come from different files / file patterns\n",
    "    val_data = data_class(config['data_dir'], n_proc=-1, config=config)\n",
    "    val_indices = val_data.all_IDs\n",
    "\n",
    "# Note: currently a validation set must exist, either with `val_pattern` or `val_ratio`\n",
    "# Using a `val_pattern` means that `val_ratio` == 0 and `test_ratio` == 0\n",
    "if config['val_ratio'] > 0:\n",
    "    train_indices, val_indices, test_indices = split_dataset(data_indices=my_data.all_IDs,\n",
    "                                                                validation_method=validation_method,\n",
    "                                                                n_splits=1,\n",
    "                                                                validation_ratio=config['val_ratio'],\n",
    "                                                                test_set_ratio=config['test_ratio'],  # used only if test_indices not explicitly specified\n",
    "                                                                test_indices=test_indices,\n",
    "                                                                random_seed=1337,\n",
    "                                                                labels=labels)\n",
    "    train_indices = train_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "    val_indices = val_indices[0]  # `split_dataset` returns a list of indices *per fold/split*\n",
    "else:\n",
    "    train_indices = my_data.all_IDs\n",
    "    if test_indices is None:\n",
    "        test_indices = []\n",
    "\n",
    "logger.info(\"{} samples may be used for training\".format(len(train_indices)))\n",
    "logger.info(\"{} samples will be used for validation\".format(len(val_indices)))\n",
    "logger.info(\"{} samples will be used for testing\".format(len(test_indices)))\n",
    "\n",
    "with open(os.path.join(config['output_dir'], 'data_indices.json'), 'w') as f:\n",
    "    try:\n",
    "        json.dump({'train_indices': list(map(int, train_indices)),\n",
    "                    'val_indices': list(map(int, val_indices)),\n",
    "                    'test_indices': list(map(int, test_indices))}, f, indent=4)\n",
    "    except ValueError:  # in case indices are non-integers\n",
    "        json.dump({'train_indices': list(train_indices),\n",
    "                    'val_indices': list(val_indices),\n",
    "                    'test_indices': list(test_indices)}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset normalization\n",
      "Test dataset normalization\n"
     ]
    }
   ],
   "source": [
    "# Pre-process features\n",
    "normalizer = None\n",
    "if config['norm_from']:\n",
    "    with open(config['norm_from'], 'rb') as f:\n",
    "        norm_dict = pickle.load(f)\n",
    "    normalizer = Normalizer(**norm_dict)\n",
    "elif config['normalization'] is not None:\n",
    "    normalizer = Normalizer(config['normalization'])\n",
    "    my_data.feature_df.loc[train_indices] = normalizer.normalize(my_data.feature_df.loc[train_indices])\n",
    "    if not config['normalization'].startswith('per_sample'):\n",
    "        # get normalizing values from training set and store for future use\n",
    "        norm_dict = normalizer.__dict__\n",
    "        with open(os.path.join(config['output_dir'], 'normalization.pickle'), 'wb') as f:\n",
    "            pickle.dump(norm_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "if normalizer is not None:\n",
    "    if len(val_indices):\n",
    "        print(\"Validation dataset normalization\")\n",
    "        val_data.feature_df.loc[val_indices] = normalizer.normalize(val_data.feature_df.loc[val_indices])\n",
    "    if len(test_indices):\n",
    "        print(\"Test dataset normalization\")\n",
    "        test_data.feature_df.loc[test_indices] = normalizer.normalize(test_data.feature_df.loc[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.35786451784537"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.feature_df.accelX.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:23:19,671 | INFO : Creating model ...\n",
      "2022-09-16 10:23:19,686 | INFO : Model:\n",
      "TSTransformerEncoder(\n",
      "  (project_inp): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (pos_enc): LearnablePositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBatchNormEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "2022-09-16 10:23:19,689 | INFO : Total number of parameters: 437510\n",
      "2022-09-16 10:23:19,690 | INFO : Trainable parameters: 437510\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "logger.info(\"Creating model ...\")\n",
    "model = model_factory(config, my_data)\n",
    "\n",
    "if config['freeze']:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('output_layer'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "logger.info(\"Model:\\n{}\".format(model))\n",
    "logger.info(\"Total number of parameters: {}\".format(utils.count_parameters(model)))\n",
    "logger.info(\"Trainable parameters: {}\".format(utils.count_parameters(model, trainable=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "\n",
    "if config['global_reg']:\n",
    "    weight_decay = config['l2_reg']\n",
    "    output_reg = None\n",
    "else:\n",
    "    weight_decay = 0\n",
    "    output_reg = config['l2_reg']\n",
    "\n",
    "optim_class = get_optimizer(config['optimizer'])\n",
    "optimizer = optim_class(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "lr_step = 0  # current step index of `lr_step`\n",
    "lr = config['lr']  # current learning step\n",
    "# Load model and optimizer state\n",
    "if config[\"load_model\"]:\n",
    "    model, optimizer, start_epoch = utils.load_model(model, config['load_model'], optimizer, config['resume'],\n",
    "                                                        config['change_output'],\n",
    "                                                        config['lr'],\n",
    "                                                        config['lr_step'],\n",
    "                                                        config['lr_factor'])\n",
    "model.to(device)\n",
    "\n",
    "loss_module = get_loss_module(config)\n",
    "\n",
    "if config['test_only'] == 'testset':  # Only evaluate and skip training\n",
    "    dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "    test_dataset = dataset_class(test_data, test_indices)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=config['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                num_workers=config['num_workers'],\n",
    "                                pin_memory=True,\n",
    "                                collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "    test_evaluator = runner_class(model, test_loader, device, loss_module,\n",
    "                                        print_interval=config['print_interval'], console=config['console'])\n",
    "    aggr_metrics_test, per_batch_test = test_evaluator.evaluate(keep_all=True)\n",
    "    print_str = 'Test Summary: '\n",
    "    for k, v in aggr_metrics_test.items():\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data generators\n",
    "dataset_class, collate_fn, runner_class = pipeline_factory(config)\n",
    "val_dataset = dataset_class(val_data, val_indices)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=config['batch_size'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=config['num_workers'],\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "train_dataset = dataset_class(my_data, train_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            shuffle=True,\n",
    "                            num_workers=config['num_workers'],\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=lambda x: collate_fn(x, max_len=model.max_len))\n",
    "\n",
    "trainer = runner_class(model, train_loader, device, loss_module, optimizer, l2_reg=output_reg,\n",
    "                                print_interval=config['print_interval'], console=config['console'])\n",
    "val_evaluator = runner_class(model, val_loader, device, loss_module,\n",
    "                                    print_interval=config['print_interval'], console=config['console'])\n",
    "\n",
    "tensorboard_writer = SummaryWriter(config['tensorboard_dir'])\n",
    "\n",
    "best_value = 1e16 if config['key_metric'] in NEG_METRICS else -1e16  # initialize with +inf or -inf depending on key metric\n",
    "metrics = []  # (for validation) list of lists: for each epoch, stores metrics like loss, ...\n",
    "best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300, 6]) tensor(0.0509)\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    # print(i, len(i))\n",
    "    X, Xm, m, p, I = i \n",
    "    print(X.shape, X.max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:23:28,835 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 0  98.3% | batch:        58 of        59\t|\tloss: 0.245817"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:23:36,139 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.303072690963745 seconds\n",
      "\n",
      "2022-09-16 10:23:36,141 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.673379063606262 seconds\n",
      "2022-09-16 10:23:36,143 | INFO : Avg batch val. time: 0.1300572722645129 seconds\n",
      "2022-09-16 10:23:36,144 | INFO : Avg sample val. time: 0.004107804637904851 seconds\n",
      "2022-09-16 10:23:36,148 | INFO : Epoch 0 Validation Summary: epoch: 0.000000 | loss: 1.856899 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\numpy\\lib\\npyio.py:696: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n",
      "2022-09-16 10:23:36,252 | INFO : Starting training...\n",
      "Training Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 1  99.8% | batch:       525 of       526\t|\tloss: 0.001213912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:27:17,812 | INFO : Epoch 1 Training Summary: epoch: 1.000000 | loss: 0.545071 | \n",
      "2022-09-16 10:27:17,813 | INFO : Epoch runtime: 0.0 hours, 3.0 minutes, 41.55656814575195 seconds\n",
      "\n",
      "2022-09-16 10:27:17,816 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 41.55656814575195 seconds\n",
      "2022-09-16 10:27:17,817 | INFO : Avg batch train. time: 0.4212102056002889 seconds\n",
      "2022-09-16 10:27:17,819 | INFO : Avg sample train. time: 0.013183182681527546 seconds\n",
      "2022-09-16 10:27:17,820 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 1  98.3% | batch:        58 of        59\t|\tloss: 0.000435387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:27:25,081 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.259398698806763 seconds\n",
      "\n",
      "2022-09-16 10:27:25,082 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.590582990646363 seconds\n",
      "2022-09-16 10:27:25,083 | INFO : Avg batch val. time: 0.12865394899400615 seconds\n",
      "2022-09-16 10:27:25,085 | INFO : Avg sample val. time: 0.0040634812583759975 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:27:25,090 | INFO : Epoch 1 Validation Summary: epoch: 1.000000 | loss: 0.713198 | \n",
      "Training Epoch:  10%|█         | 1/10 [03:48<34:20, 228.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 2  99.8% | batch:       525 of       526\t|\tloss: 0.000281339"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:31:09,278 | INFO : Epoch 2 Training Summary: epoch: 2.000000 | loss: 0.396065 | \n",
      "2022-09-16 10:31:09,279 | INFO : Epoch runtime: 0.0 hours, 3.0 minutes, 44.092564821243286 seconds\n",
      "\n",
      "2022-09-16 10:31:09,280 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 42.82456648349762 seconds\n",
      "2022-09-16 10:31:09,282 | INFO : Avg batch train. time: 0.42362084882794226 seconds\n",
      "2022-09-16 10:31:09,283 | INFO : Avg sample train. time: 0.013258631826936667 seconds\n",
      "2022-09-16 10:31:09,283 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 2  98.3% | batch:        58 of        59\t|\tloss: 0.000215018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:31:16,905 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.620870590209961 seconds\n",
      "\n",
      "2022-09-16 10:31:16,907 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.595630923906962 seconds\n",
      "2022-09-16 10:31:16,908 | INFO : Avg batch val. time: 0.12873950718486377 seconds\n",
      "2022-09-16 10:31:16,910 | INFO : Avg sample val. time: 0.004066183578108652 seconds\n",
      "2022-09-16 10:31:16,912 | INFO : Epoch 2 Validation Summary: epoch: 2.000000 | loss: 0.668020 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  20%|██        | 2/10 [07:40<30:45, 230.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 3  99.8% | batch:       525 of       526\t|\tloss: 0.000294342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:35:03,168 | INFO : Epoch 3 Training Summary: epoch: 3.000000 | loss: 0.401068 | \n",
      "2022-09-16 10:35:03,169 | INFO : Epoch runtime: 0.0 hours, 3.0 minutes, 46.140748262405396 seconds\n",
      "\n",
      "2022-09-16 10:35:03,170 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 43.9299604098002 seconds\n",
      "2022-09-16 10:35:03,171 | INFO : Avg batch train. time: 0.4257223581935365 seconds\n",
      "2022-09-16 10:35:03,171 | INFO : Avg sample train. time: 0.013324405593823646 seconds\n",
      "2022-09-16 10:35:03,172 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 3  98.3% | batch:        58 of        59\t|\tloss: 0.000124011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:35:10,780 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.6065993309021 seconds\n",
      "\n",
      "2022-09-16 10:35:10,781 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.597197839191982 seconds\n",
      "2022-09-16 10:35:10,783 | INFO : Avg batch val. time: 0.12876606507105054 seconds\n",
      "2022-09-16 10:35:10,784 | INFO : Avg sample val. time: 0.0040670223978543805 seconds\n",
      "2022-09-16 10:35:10,787 | INFO : Epoch 3 Validation Summary: epoch: 3.000000 | loss: 0.936572 | \n",
      "Training Epoch:  30%|███       | 3/10 [11:34<27:04, 232.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[KTraining Epoch 4  99.8% | batch:       525 of       526\t|\tloss: 0.001736742"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:38:58,830 | INFO : Epoch 4 Training Summary: epoch: 4.000000 | loss: 0.339200 | \n",
      "2022-09-16 10:38:58,831 | INFO : Epoch runtime: 0.0 hours, 3.0 minutes, 48.03800296783447 seconds\n",
      "\n",
      "2022-09-16 10:38:58,832 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 44.95697104930878 seconds\n",
      "2022-09-16 10:38:58,833 | INFO : Avg batch train. time: 0.42767484990362886 seconds\n",
      "2022-09-16 10:38:58,835 | INFO : Avg sample train. time: 0.01338551535459412 seconds\n",
      "2022-09-16 10:38:58,836 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 4  98.3% | batch:        58 of        59\t|\tloss: 0.000506249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:39:06,382 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.543957233428955 seconds\n",
      "\n",
      "2022-09-16 10:39:06,383 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.590542763471603 seconds\n",
      "2022-09-16 10:39:06,385 | INFO : Avg batch val. time: 0.1286532671774848 seconds\n",
      "2022-09-16 10:39:06,386 | INFO : Avg sample val. time: 0.0040634597234858695 seconds\n",
      "2022-09-16 10:39:06,393 | INFO : Epoch 4 Validation Summary: epoch: 4.000000 | loss: 0.424020 | \n",
      "Training Epoch:  40%|████      | 4/10 [15:30<23:21, 233.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[KTraining Epoch 5  99.8% | batch:       525 of       526\t|\tloss: 0.004644439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:42:54,888 | INFO : Epoch 5 Training Summary: epoch: 5.000000 | loss: 0.290691 | \n",
      "2022-09-16 10:42:54,890 | INFO : Epoch runtime: 0.0 hours, 3.0 minutes, 48.390196323394775 seconds\n",
      "\n",
      "2022-09-16 10:42:54,892 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 45.64361610412598 seconds\n",
      "2022-09-16 10:42:54,894 | INFO : Avg batch train. time: 0.4289802587530912 seconds\n",
      "2022-09-16 10:42:54,896 | INFO : Avg sample train. time: 0.013426372492212662 seconds\n",
      "2022-09-16 10:42:54,897 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 5  98.3% | batch:        58 of        59\t|\tloss: 0.005510826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:43:03,263 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 8.365394592285156 seconds\n",
      "\n",
      "2022-09-16 10:43:03,265 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.676637411117554 seconds\n",
      "2022-09-16 10:43:03,267 | INFO : Avg batch val. time: 0.13011249849351786 seconds\n",
      "2022-09-16 10:43:03,270 | INFO : Avg sample val. time: 0.004109548935287769 seconds\n",
      "2022-09-16 10:43:03,274 | INFO : Epoch 5 Validation Summary: epoch: 5.000000 | loss: 0.725827 | \n",
      "Training Epoch:  50%|█████     | 5/10 [19:27<19:33, 234.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[KTraining Epoch 6  99.8% | batch:       525 of       526\t|\tloss: 0.000169816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:47:11,752 | INFO : Epoch 6 Training Summary: epoch: 6.000000 | loss: 0.186672 | \n",
      "2022-09-16 10:47:11,753 | INFO : Epoch runtime: 0.0 hours, 4.0 minutes, 8.472110986709595 seconds\n",
      "\n",
      "2022-09-16 10:47:11,753 | INFO : Avg epoch train. time: 0.0 hours, 3.0 minutes, 49.44836525122324 seconds\n",
      "2022-09-16 10:47:11,755 | INFO : Avg batch train. time: 0.43621362215061454 seconds\n",
      "2022-09-16 10:47:11,756 | INFO : Avg sample train. time: 0.01365276480133424 seconds\n",
      "2022-09-16 10:47:11,758 | INFO : Evaluating on validation set ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KEvaluating Epoch 6  98.3% | batch:        58 of        59\t|\tloss: 4.22998e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:47:19,598 | INFO : Validation runtime: 0.0 hours, 0.0 minutes, 7.838864803314209 seconds\n",
      "\n",
      "2022-09-16 10:47:19,599 | INFO : Avg val. time: 0.0 hours, 0.0 minutes, 7.69286015033722 seconds\n",
      "2022-09-16 10:47:19,600 | INFO : Avg batch val. time: 0.1303874601752071 seconds\n",
      "2022-09-16 10:47:19,602 | INFO : Avg sample val. time: 0.004118233485191231 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:47:19,605 | INFO : Epoch 6 Validation Summary: epoch: 6.000000 | loss: 0.326069 | \n",
      "Training Epoch:  60%|██████    | 6/10 [23:43<16:08, 242.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KTraining Epoch 7   0.0% | batch:         0 of       526\t|\tloss: 7.45211e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [57], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m mark \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_all\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m aggr_metrics_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dictionary of aggregate epoch metrics\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"train metrics >> {aggr_metrics_train}\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(aggr_metrics_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mg:\\FYP\\Codebases\\mvts_transformer-master\\src\\running.py:281\u001b[0m, in \u001b[0;36mUnsupervisedRunner.train_epoch\u001b[1;34m(self, epoch_num)\u001b[0m\n\u001b[0;32m    279\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# total loss of epoch\u001b[39;00m\n\u001b[0;32m    280\u001b[0m total_active_elements \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# total unmasked elements in epoch\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader):\n\u001b[0;32m    283\u001b[0m     X, targets, target_masks, padding_masks, IDs \u001b[39m=\u001b[39m batch\n\u001b[0;32m    284\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mg:\\FYP\\Codebases\\mvts_transformer-master\\src\\datasets\\dataset.py:34\u001b[0m, in \u001b[0;36mImputationDataset.__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mFor a given integer index, returns the corresponding (seq_length, feat_dim) array and a noise mask of same shape\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m    ID: ID of sample\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# X = np.array([i for i in self.feature_df.loc[self.IDs[ind]].values]).T\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_df\u001b[39m.\u001b[39;49mloc[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mIDs[ind]]\u001b[39m.\u001b[39mvalues  \u001b[39m# (seq_length, feat_dim) array\u001b[39;00m\n\u001b[0;32m     35\u001b[0m mask \u001b[39m=\u001b[39m noise_mask(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasking_ratio, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_mask_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution,\n\u001b[0;32m     36\u001b[0m                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexclude_feats)  \u001b[39m# (seq_length, feat_dim) boolean array\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mfrom_numpy(X), torch\u001b[39m.\u001b[39mfrom_numpy(mask), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mIDs[ind]\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m    966\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\core\\indexing.py:1205\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[39m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1205\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_label\u001b[39m(\u001b[39mself\u001b[39m, label, axis: \u001b[39mint\u001b[39m):\n\u001b[0;32m   1152\u001b[0m     \u001b[39m# GH#5667 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49mxs(label, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\core\\generic.py:3864\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3862\u001b[0m             new_index \u001b[39m=\u001b[39m index[loc]\n\u001b[0;32m   3863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3864\u001b[0m     loc \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3866\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   3867\u001b[0m         \u001b[39mif\u001b[39;00m loc\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mbool_:\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3627\u001b[0m casted_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_indexer(key)\n\u001b[0;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\_libs\\index.pyx:160\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\_libs\\index.pyx:201\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\_libs\\index.pyx:209\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\deela\\anaconda3\\envs\\mvts_trans\\lib\\site-packages\\pandas\\_libs\\index.pyx:99\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate on validation before training\n",
    "aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config, best_metrics,\n",
    "                                                        best_value, epoch=0)\n",
    "metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "metrics.append(list(metrics_values))\n",
    "\n",
    "logger.info('Starting training...')\n",
    "for epoch in tqdm(range(start_epoch + 1, config[\"epochs\"] + 1), desc='Training Epoch', leave=False):\n",
    "    mark = epoch if config['save_all'] else 'last'\n",
    "    epoch_start_time = time.time()\n",
    "    aggr_metrics_train = trainer.train_epoch(epoch)  # dictionary of aggregate epoch metrics\n",
    "    # print(f\"train metrics >> {aggr_metrics_train}\")\n",
    "    run['train/loss'].log(aggr_metrics_train['loss'])\n",
    "    epoch_runtime = time.time() - epoch_start_time\n",
    "    print_str = 'Epoch {} Training Summary: '.format(epoch)\n",
    "    for k, v in aggr_metrics_train.items():\n",
    "        tensorboard_writer.add_scalar('{}/train'.format(k), v, epoch)\n",
    "        print_str += '{}: {:8f} | '.format(k, v)\n",
    "    logger.info(print_str)\n",
    "    logger.info(\"Epoch runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(epoch_runtime)))\n",
    "    total_epoch_time += epoch_runtime\n",
    "    avg_epoch_time = total_epoch_time / (epoch - start_epoch)\n",
    "    avg_batch_time = avg_epoch_time / len(train_loader)\n",
    "    avg_sample_time = avg_epoch_time / len(train_dataset)\n",
    "    logger.info(\"Avg epoch train. time: {} hours, {} minutes, {} seconds\".format(*utils.readable_time(avg_epoch_time)))\n",
    "    logger.info(\"Avg batch train. time: {} seconds\".format(avg_batch_time))\n",
    "    logger.info(\"Avg sample train. time: {} seconds\".format(avg_sample_time))\n",
    "\n",
    "    # evaluate if first or last epoch or at specified interval\n",
    "    if (epoch == config[\"epochs\"]) or (epoch == start_epoch + 1) or (epoch % config['val_interval'] == 0):\n",
    "        aggr_metrics_val, best_metrics, best_value = validate(val_evaluator, tensorboard_writer, config,\n",
    "                                                                best_metrics, best_value, epoch)\n",
    "        # print(f\"eval metrics >> {aggr_metrics_val}\")\n",
    "        run['eval/loss'].log(aggr_metrics_val['loss'])\n",
    "        # run['eval/accuracy'].log(aggr_metrics_val['accuracy'])\n",
    "        # run['eval/precision'].log(aggr_metrics_val['precision'])\n",
    "        metrics_names, metrics_values = zip(*aggr_metrics_val.items())\n",
    "        metrics.append(list(metrics_values))\n",
    "\n",
    "    # utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(mark)), epoch, model, optimizer)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    if epoch == config['lr_step'][lr_step]:\n",
    "        utils.save_model(os.path.join(config['save_dir'], 'model_{}.pth'.format(epoch)), epoch, model, optimizer)\n",
    "        lr = lr * config['lr_factor'][lr_step]\n",
    "        if lr_step < len(config['lr_step']) - 1:  # so that this index does not get out of bounds\n",
    "            lr_step += 1\n",
    "        logger.info('Learning rate updated to: ', lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    # Difficulty scheduling\n",
    "    if config['harden'] and check_progress(epoch):\n",
    "        train_loader.dataset.update()\n",
    "        val_loader.dataset.update()\n",
    "\n",
    "# Export evolution of metrics over epochs\n",
    "# header = metrics_names #@nipdep\n",
    "# metrics_filepath = os.path.join(config[\"output_dir\"], \"metrics_\" + config[\"experiment_name\"] + \".xls\")\n",
    "# book = utils.export_performance_metrics(metrics_filepath, metrics, header, sheet_name=\"metrics\")\n",
    "\n",
    "# # Export record metrics to a file accumulating records from all experiments\n",
    "# utils.register_record(config[\"records_file\"], config[\"initial_timestamp\"], config[\"experiment_name\"],\n",
    "#                         best_metrics, aggr_metrics_val, comment=config['comment'])\n",
    "\n",
    "logger.info('Best {} was {}. Other metrics: {}'.format(config['key_metric'], best_value, best_metrics))\n",
    "logger.info('All Done!')\n",
    "\n",
    "total_runtime = time.time() - total_start_time\n",
    "logger.info(\"Total runtime: {} hours, {} minutes, {} seconds\\n\".format(*utils.readable_time(total_runtime)))\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(os.path.join(config['save_dir'], 'prep_model_{}.pth'.format(mark)), epoch, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for i in val_loader:\n",
    "    x, y, m, ids = i\n",
    "    x = x.to(device)\n",
    "    m = m.to(device)\n",
    "    pred = model(x, m)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 18])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.softmax(pred, dim=-1)\n",
    "pred_label = torch.argmax(probs, dim=-1)\n",
    "pred_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  2, 14,  4,  4,  1,  4,  3, 15, 17,  4, 11, 17,  1,  1, 15,  1, 10,\n",
       "         1,  1,  3,  6,  1, 15,  6,  6, 15,  6,  1, 15,  1,  1],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  2, 14,  4,  4,  1,  4,  3, 15, 17,  4, 11, 17,  1,  5, 11,  5,  4,\n",
       "         0,  2,  3,  6,  5, 12,  4,  9, 15, 10,  1, 15,  2,  0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9982e-03,  1.8686e-03, -7.6950e-03,  ...,  3.2540e-05,\n",
       "          2.5046e-03, -1.7847e-03],\n",
       "        [ 1.0441e-02, -2.4760e-02, -1.4667e-02,  ..., -5.1662e-03,\n",
       "          6.1747e-04, -1.9824e-02],\n",
       "        [-4.3647e-03,  1.7393e-02,  1.9783e-03,  ...,  3.0929e-03,\n",
       "          2.6371e-03, -6.4250e-03],\n",
       "        ...,\n",
       "        [ 1.8496e-02, -1.9215e-02, -2.6746e-02,  ...,  2.0559e-02,\n",
       "         -1.6608e-02,  1.2978e-02],\n",
       "        [ 1.7822e-02,  5.6438e-02,  3.2866e-02,  ...,  1.7167e-02,\n",
       "         -6.8521e-03,  2.6117e-02],\n",
       "        [-1.5895e-02, -5.7300e-03, -1.0521e-02,  ..., -1.7554e-02,\n",
       "          2.3200e-02,  2.8165e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer_encoder.layers[0].linear1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSTransformerEncoderClassiregressor(\n",
       "  (project_inp): Linear(in_features=6, out_features=128, bias=True)\n",
       "  (pos_enc): FixedPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBatchNormEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (output_layer): Linear(in_features=38400, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1D-Conv layer\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = nn.Conv1d(300, 1, 1)\n",
    "sample_inp = torch.rand((32, 300, 128))\n",
    "sample_out = conv_layer(sample_inp)\n",
    "sample_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
