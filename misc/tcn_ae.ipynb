{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794b0821-b1cf-4224-8aa6-ea44970abe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba7eddb-7bff-4d14-9b9f-b0d86bfb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"/Users/devin/Documents/FYP/HAR-ZSL-XAI\"\n",
    "data_dir = os.path.join(main_dir,\"sequence_data\")\n",
    "class_names = os.listdir(data_dir)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 1-train_ratio - val_ratio\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5f6ab7-fd86-4435-9bec-331d014a6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90625df-343b-4bc3-897d-184be8d8422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(os.path.join(main_dir,\"data\",\"pose_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27b84c1-0510-42f6-b53e-503875da2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f164bb-3aa7-450b-b2b7-3f11a055c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = []\n",
    "val_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "file_list = [os.path.join(data_dir,x) for x in os.listdir(data_dir)]\n",
    "\n",
    "random.shuffle(file_list)\n",
    "num_list = len(file_list)\n",
    "\n",
    "train_range = [0,int(num_list*train_ratio)]\n",
    "val_range = [int(num_list*train_ratio),int(num_list*(train_ratio+val_ratio))]\n",
    "test_range = [int(num_list*(train_ratio+val_ratio)),num_list-1]\n",
    "\n",
    "train_file_list += file_list[train_range[0]:train_range[1]]\n",
    "val_file_list += file_list[val_range[0]:val_range[1]]\n",
    "test_file_list += file_list[test_range[0]:test_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9f2788-0c6f-414a-a65e-eab0956e5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12357, 2648, 2647)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca03ac0-3133-4e69-9c4c-7e3c346cae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = train_file_list[:(len(train_file_list)//batch_size)*batch_size]\n",
    "val_file_list = val_file_list[:(len(val_file_list)//batch_size)*batch_size]\n",
    "test_file_list = test_file_list[:(len(test_file_list)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f014d9f-b951-481e-af46-a677cb0e2c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12352, 2624, 2624)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea002f29-1936-46b1-a35f-7dc62c439642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, file_list,class2id, transform=None, \n",
    "                 target_transform=None,active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class2id = class2id\n",
    "        self.target_transform = target_transform\n",
    "        self.active_locations = active_locations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_file = np.load(self.file_list[idx])\n",
    "        action_type = self.file_list[idx].strip().split(\"/\")[-1].split(\"_cls_\")[0]\n",
    "        coords, vid_size = a_file[\"coords\"],a_file[\"video_size\"]\n",
    "        coords = coords[:,self.active_locations,:]\n",
    "        \n",
    "        shape = coords.shape\n",
    "        \n",
    "        coords = torch.from_numpy(coords).float()\n",
    "        \n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "        \n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "        return coords, label, self.class2id[action_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bfa6f8-44df-49ac-901c-1835612db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SkeletonDataset(train_file_list,clsname2id)\n",
    "val_data = SkeletonDataset(val_file_list,clsname2id)\n",
    "test_data = SkeletonDataset(test_file_list,clsname2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f2a581-485c-4ac9-836a-86f74ec4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b980c5a-2d57-4c0f-bcea-8bd2bbab1a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54930f30-64e9-4f45-978e-ceb2b930ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN\n",
    "\n",
    "class TCNEnc(nn.Module):\n",
    "    def __init__(self,seq_len = 50,ts_dimension = 1,batch_size = 32,dilations = (1, 2, 4, 8, 16),nb_filters = 20,kernel_size = 20,nb_stacks = 1,padding = 'same',dropout_rate = 0.00,filters_conv1d = 8,pooler = nn.AvgPool1d,sampling_factor = 2,):\n",
    "        super(TCNEnc, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_dimension : int\n",
    "            The dimension of the time series (default is 1)\n",
    "        dilations : tuple\n",
    "            The dilation rates used in the TCN-AE model (default is (1, 2, 4, 8, 16))\n",
    "        nb_filters : int\n",
    "            The number of filters used in the dilated convolutional layers. All dilated conv. layers use the same number of filters (default is 20)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.ts_dimension = ts_dimension\n",
    "        self.dilations = dilations\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.padding = padding\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.filters_conv1d = filters_conv1d\n",
    "        self.pooler = pooler\n",
    "        self.sampling_factor = sampling_factor\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # build the model\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the TCN-AE model.\n",
    "        If the argument `verbose` isn't passed in, the default verbosity level is used.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : str, optional\n",
    "            The verbosity level (default is 1)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        KerasXYZType\n",
    "        Todo\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If ...\n",
    "        \"\"\"\n",
    "\n",
    "        # Put signal through TCN. Output-shape: (batch, sequence length, nb_filters)\n",
    "        self.tcn_enc = TCN(50,nb_filters=self.nb_filters, kernel_size=self.kernel_size, nb_stacks=self.nb_stacks, dilations=self.dilations, \n",
    "                      padding=self.padding, use_skip_connections=True, dropout_rate=self.dropout_rate, return_sequences=True)\n",
    "        \n",
    "        \n",
    "        self.input_conv1d = self.nb_filters[-1] if isinstance(self.nb_filters,list) else self.nb_filters\n",
    "        # Now, adjust the number of channels...\n",
    "        self.enc_flat_conv = nn.Conv1d(self.input_conv1d, self.filters_conv1d, kernel_size=1, padding=self.padding)\n",
    "\n",
    "        ## Do some average (max) pooling to get a compressed representation of the time series (e.g. a sequence of length 8)\n",
    "        self.enc_pooled = self.pooler(self.sampling_factor)\n",
    "        \n",
    "        # If you want, maybe put the pooled values through a non-linear Activation\n",
    "        self.enc_out = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.tcn_enc(x)\n",
    "        x = self.enc_flat_conv(x)\n",
    "        x = self.enc_pooled(x)\n",
    "        \n",
    "        #flatten \n",
    "        x= torch.flatten(x, start_dim=1)\n",
    "        x = self.enc_out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class TCNDec(nn.Module):\n",
    "    def __init__(self,seq_len = 50,ts_dimension = 1,batch_size = 32,dilations = (1, 2, 4, 8, 16),nb_filters = 20,kernel_size = 20,nb_stacks = 1,padding = 'same',dropout_rate = 0.00,filters_conv1d = 8,pooler = nn.AvgPool1d,sampling_factor = 2,):\n",
    "        super(TCNDec, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_dimension : int\n",
    "            The dimension of the time series (default is 1)\n",
    "        dilations : tuple\n",
    "            The dilation rates used in the TCN-AE model (default is (1, 2, 4, 8, 16))\n",
    "        nb_filters : int\n",
    "            The number of filters used in the dilated convolutional layers. All dilated conv. layers use the same number of filters (default is 20)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.ts_dimension = ts_dimension\n",
    "        self.dilations = dilations\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.padding = padding\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.filters_conv1d = filters_conv1d\n",
    "        self.pooler = pooler\n",
    "        self.sampling_factor = sampling_factor\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # build the model\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the TCN-AE model.\n",
    "        If the argument `verbose` isn't passed in, the default verbosity level is used.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : str, optional\n",
    "            The verbosity level (default is 1)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        KerasXYZType\n",
    "        Todo\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If ...\n",
    "        \"\"\"\n",
    "        self.input_conv1d = self.nb_filters[-1] if isinstance(self.nb_filters,list) else self.nb_filters\n",
    "\n",
    "        # Put signal through TCN. Output-shape: (batch,sequence length, nb_filters)\n",
    "        self.dec_upsample = nn.Upsample(scale_factor=self.sampling_factor)\n",
    "\n",
    "        self.tcn_dec = TCN(8,nb_filters=self.nb_filters, kernel_size=self.kernel_size, nb_stacks=self.nb_stacks, dilations=self.dilations, \n",
    "                                padding=self.padding, use_skip_connections=True, dropout_rate=self.dropout_rate, return_sequences=True)\n",
    "        \n",
    "        self.dec_upscale_conv = nn.Conv1d(self.input_conv1d,self.seq_len, kernel_size=1, padding=self.padding)\n",
    "\n",
    "        # Put the filter-outputs through a dense layer finally, to get the reconstructed signal\n",
    "        self.linear_out = nn.Linear(self.ts_dimension, self.ts_dimension)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view((self.batch_size, self.filters_conv1d, self.ts_dimension//self.sampling_factor))\n",
    "        x = self.dec_upsample(x)\n",
    "        x = self.tcn_dec(x)\n",
    "        x = self.dec_upscale_conv(x)\n",
    "        x = self.linear_out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "class TCNEncDec(nn.Module):\n",
    "    \"\"\"\n",
    "    A class used to represent the Temporal Convolutional Autoencoder (TCN-AE).\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    model : xxtypexx\n",
    "        The TCN-AE model.\n",
    "    Methods\n",
    "    -------\n",
    "    build_model(verbose = 1)\n",
    "        Builds the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,seq_len = 50,ts_dimension = 1,batch_size = 32,dilations = (1, 2, 4, 8, 16),nb_filters = 20,kernel_size = 20,nb_stacks = 1,padding = 'same',dropout_rate = 0.00,filters_conv1d = 8,pooler = nn.AvgPool1d,sampling_factor = 2,):\n",
    "        super(TCNEncDec, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_dimension : int\n",
    "            The dimension of the time series (default is 1)\n",
    "        dilations : tuple\n",
    "            The dilation rates used in the TCN-AE model (default is (1, 2, 4, 8, 16))\n",
    "        nb_filters : int\n",
    "            The number of filters used in the dilated convolutional layers. All dilated conv. layers use the same number of filters (default is 20)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.ts_dimension = ts_dimension\n",
    "        self.dilations = dilations\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.padding = padding\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.filters_conv1d = filters_conv1d\n",
    "        self.pooler = pooler\n",
    "        self.sampling_factor = sampling_factor\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # build the model\n",
    "        self.build_model()\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the TCN-AE model.\n",
    "        If the argument `verbose` isn't passed in, the default verbosity level is used.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : str, optional\n",
    "            The verbosity level (default is 1)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        KerasXYZType\n",
    "        Todo\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError\n",
    "            If ...\n",
    "        \"\"\"\n",
    "        self.encoder = TCNEnc(seq_len=self.seq_len, ts_dimension=self.ts_dimension,dilations = self.dilations,nb_filters = self.nb_filters,kernel_size = self.kernel_size,nb_stacks = self.nb_stacks,padding = self.padding,dropout_rate = self.dropout_rate,filters_conv1d = self.filters_conv1d,pooler = self.pooler,sampling_factor = self.sampling_factor,batch_size = self.batch_size)\n",
    "        \n",
    "        self.decoder = TCNDec(seq_len = self.seq_len,ts_dimension = self.ts_dimension,dilations = self.dilations,nb_filters = self.nb_filters,kernel_size = self.kernel_size,nb_stacks = self.nb_stacks,padding = self.padding,dropout_rate = self.dropout_rate,filters_conv1d = self.filters_conv1d,pooler = self.pooler,sampling_factor = self.sampling_factor,batch_size = self.batch_size)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.encoder(x)\n",
    "        x = self.decoder(embedding)\n",
    "        return x, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e0d8fe8-26de-4d03-a444-3eac3055caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_model = TCNEncDec(ts_dimension=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed1b0e37-a8ca-4f8c-b616-a86accc45e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample = torch.randn((32,50,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981aec7c-e339-4858-9fb2-7abc614ae8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 36])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_sample.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb702c5e-acb5-4852-a264-f3fac27b6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fyp/lib/python3.8/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662707330697/work/aten/src/ATen/native/Convolution.cpp:894.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "out = tcn_model(rand_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b12b06e-44b2-4df6-9bc0-c27fb817db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 144])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be9b320d-2659-435c-b6fe-2c9937360f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 36])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3206b242-0537-4c4b-9ff2-e361cd4dff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcc0d9c3-9abd-4897-b67a-6f3958b69899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    std_loss = nn.L1Loss(reduction='sum').to(device)\n",
    "    #contrastive_loss = SupConLoss(contrast_mode=\"one\").to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for in_seq,tar_seq,action in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            in_seq = in_seq.to(device)\n",
    "            tar_seq = tar_seq.to(device)\n",
    "            seq_pred, _ = model(in_seq)\n",
    "            \n",
    "            loss = std_loss(seq_pred, tar_seq)\n",
    "            #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "            #print(contrastive_loss(embed,labels=sample_label.view(-1)))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for in_seq,tar_seq,action in val_dataset:\n",
    "\n",
    "                in_seq = in_seq.to(device)\n",
    "                tar_seq = tar_seq.to(device)\n",
    "                seq_pred,_  = model(in_seq)\n",
    "\n",
    "                loss = std_loss(seq_pred, tar_seq)\n",
    "                #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8151f9-65c0-40d2-a99e-6787bdf5e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [01:01<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 14424.16729578206 val loss 11860.944478849085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [01:01<00:00,  6.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  tcn_model, \n",
    "  train_dl, \n",
    "  val_dl, \n",
    "  n_epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924402fb-167c-42cf-b6bc-db7c196a27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "\n",
    "ax.plot(history['train'])\n",
    "ax.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c61a5-2d32-4151-9190-98af5bd3621e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
