{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "794b0821-b1cf-4224-8aa6-ea44970abe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5ba7eddb-7bff-4d14-9b9f-b0d86bfb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"/Users/devin/Documents/FYP/HAR-ZSL-XAI\"\n",
    "data_dir = os.path.join(main_dir,\"sequence_data\")\n",
    "class_names = os.listdir(data_dir)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 1-train_ratio - val_ratio\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ef5f6ab7-fd86-4435-9bec-331d014a6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f90625df-343b-4bc3-897d-184be8d8422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(os.path.join(main_dir,\"data\",\"pose_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f27b84c1-0510-42f6-b53e-503875da2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b3f164bb-3aa7-450b-b2b7-3f11a055c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = []\n",
    "val_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "file_list = [os.path.join(data_dir,x) for x in os.listdir(data_dir)]\n",
    "\n",
    "random.shuffle(file_list)\n",
    "num_list = len(file_list)\n",
    "\n",
    "train_range = [0,int(num_list*train_ratio)]\n",
    "val_range = [int(num_list*train_ratio),int(num_list*(train_ratio+val_ratio))]\n",
    "test_range = [int(num_list*(train_ratio+val_ratio)),num_list-1]\n",
    "\n",
    "train_file_list += file_list[train_range[0]:train_range[1]]\n",
    "val_file_list += file_list[val_range[0]:val_range[1]]\n",
    "test_file_list += file_list[test_range[0]:test_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "db9f2788-0c6f-414a-a65e-eab0956e5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12357, 2648, 2647)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cca03ac0-3133-4e69-9c4c-7e3c346cae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = train_file_list[:(len(train_file_list)//batch_size)*batch_size]\n",
    "val_file_list = val_file_list[:(len(val_file_list)//batch_size)*batch_size]\n",
    "test_file_list = test_file_list[:(len(test_file_list)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6f014d9f-b951-481e-af46-a677cb0e2c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12352, 2624, 2624)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ea002f29-1936-46b1-a35f-7dc62c439642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, file_list,class2id, transform=None, target_transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class2id = class2id\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_file = np.load(self.file_list[idx])\n",
    "        action_type = self.file_list[idx].strip().split(\"/\")[-1].split(\"_cls_\")[0]\n",
    "        coords, vid_size = a_file[\"coords\"],a_file[\"video_size\"]\n",
    "        \n",
    "        shape = coords.shape\n",
    "        \n",
    "        coords = torch.from_numpy(coords).float()\n",
    "        \n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "        \n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "        return coords, label, self.class2id[action_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d0bfa6f8-44df-49ac-901c-1835612db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SkeletonDataset(train_file_list,clsname2id)\n",
    "val_data = SkeletonDataset(val_file_list,clsname2id)\n",
    "test_data = SkeletonDataset(test_file_list,clsname2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "63f2a581-485c-4ac9-836a-86f74ec4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4b980c5a-2d57-4c0f-bcea-8bd2bbab1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.linear1 = nn.Linear(self.input_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 512)\n",
    "        self.lstm = nn.LSTM(input_size = 512, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        x = self.linear1(x_input)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.linear1 = nn.Linear(128, self.input_size)\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.linear4 = nn.Linear(1024, 512)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 512, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        \n",
    "        hidden = encoder_hidden.view((hidden_shape[0],4,512))\n",
    "        hidden = torch.transpose(hidden,1,0)\n",
    "        h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "        h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        \n",
    "        dummy_input = torch.rand((32,50,512), requires_grad=True)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.linear4(lstm_out)\n",
    "        x = self.linear3(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(input_size,hidden_size)\n",
    "        self.decoder = BiLSTMDecoder(input_size,hidden_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ad1910a4-fa78-4ee0-86bd-f7d9d6e559af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BiLSTMEncoder(99,512)\n",
    "decoder = BiLSTMDecoder(99,512)\n",
    "\n",
    "bilstm_model = BiLSTMEncDecModel(99,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "da6f836f-4250-4abe-b392-71d7340e4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, embedding = encoder(torch.randn((32,50,99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f28e99-e59d-4b68-a1f4-41836e81d2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b673418b-ab02-4b77-8fb6-ee5f87b43225",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out = decoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71696fa1-687e-431b-85fa-0f0e4db2dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(torch.randn((32,50,99)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3a443bd3-6669-4017-828c-d2df95e4cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 99])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3206b242-0537-4c4b-9ff2-e361cd4dff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bcc0d9c3-9abd-4897-b67a-6f3958b69899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    std_loss = nn.L1Loss(reduction='sum').to(device)\n",
    "    #contrastive_loss = SupConLoss(contrast_mode=\"one\").to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for in_seq,tar_seq,action in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            in_seq = in_seq.to(device)\n",
    "            tar_seq = tar_seq.to(device)\n",
    "            seq_pred = model(in_seq)\n",
    "            \n",
    "            loss = std_loss(seq_pred, tar_seq)\n",
    "            #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "            #print(contrastive_loss(embed,labels=sample_label.view(-1)))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for in_seq,tar_seq,action in val_dataset:\n",
    "\n",
    "                in_seq = in_seq.to(device)\n",
    "                tar_seq = tar_seq.to(device)\n",
    "                seq_pred = model(in_seq)\n",
    "\n",
    "                loss = std_loss(seq_pred, tar_seq)\n",
    "                #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8151f9-65c0-40d2-a99e-6787bdf5e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 286/386 [01:39<00:42,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  bilstm_model, \n",
    "  train_dl, \n",
    "  val_dl, \n",
    "  n_epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924402fb-167c-42cf-b6bc-db7c196a27f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
