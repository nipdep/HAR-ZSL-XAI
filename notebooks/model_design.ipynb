{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_head, remainder = divmod(d_model, num_heads)\n",
    "        if remainder:\n",
    "            raise ValueError(\n",
    "                \"incompatible `d_model` and `num_heads`\"\n",
    "            )\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(max_len, max_len))\n",
    "            .unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(\n",
    "                \"sequence length exceeds model capacity\"\n",
    "            )\n",
    "        \n",
    "        k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        \n",
    "        start = self.max_len - seq_len\n",
    "        Er_t = self.Er[start:, :].transpose(0, 1) # automatic positional padding\n",
    "        QEr = torch.matmul(q, Er_t)\n",
    "        Srel = self.skew(QEr)\n",
    "        \n",
    "        QK_t = torch.matmul(q, k_t)\n",
    "        attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
    "        mask = self.mask[:, :, :seq_len, :seq_len]\n",
    "        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        return self.dropout(out)\n",
    "        \n",
    "    \n",
    "    def skew(self, QEr):\n",
    "        padded = F.pad(QEr, (1, 0))\n",
    "        batch_size, num_heads, num_rows, num_cols = padded.shape\n",
    "        reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        return Srel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise ValueError(\"activation should be relu/gelu, not {}\".format(activation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPositionalEncoding(nn.Module): # deterministic positional encoding\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=1024).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n",
    "        super(FixedPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)  # this stores the variable in the state_dict (used for non-trainable variables)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(max_len, 1, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_pos_encoder(pos_encoding):\n",
    "    if pos_encoding == \"learnable\":\n",
    "        return LearnablePositionalEncoding\n",
    "    elif pos_encoding == \"fixed\":\n",
    "        return FixedPositionalEncoding\n",
    "\n",
    "    raise NotImplementedError(\"pos_encoding should be 'learnable'/'fixed', not '{}'\".format(pos_encoding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBatchNormEncoderLayer(nn.modules.Module):\n",
    "    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n",
    "    It differs from TransformerEncoderLayer in torch/nn/modules/transformer.py in that it replaces LayerNorm\n",
    "    with BatchNorm.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerBatchNormEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = BatchNorm1d(d_model, eps=1e-5)  # normalizes each feature across batch samples and time steps\n",
    "        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n",
    "        src = self.norm1(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n",
    "        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n",
    "        src = self.norm2(src)\n",
    "        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n",
    "        return src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOTA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOTAEmbedding(nn.Module):\n",
    "    def __init__(self, linear_filters=[1024,1024,1024,2048],input_feat=36, dropout=0.1):\n",
    "        super(SOTAEmbedding, self).__init__()\n",
    "        self.input_feat = input_feat\n",
    "        self.linear_filters = linear_filters\n",
    "        self.input_feat = input_feat\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.linear1 = nn.Linear(input_feat,linear_filters[0])\n",
    "        self.batch_norm1 = nn.BatchNorm1d(linear_filters[0])\n",
    "        self.linear2 = nn.Linear(linear_filters[0],linear_filters[1])\n",
    "        self.batch_norm2 = nn.BatchNorm1d(linear_filters[1])\n",
    "        self.linear3 = nn.Linear(linear_filters[1],linear_filters[2])\n",
    "        self.batch_norm3 = nn.BatchNorm1d(linear_filters[2])\n",
    "        self.linear4 = nn.Linear(linear_filters[2]+linear_filters[1]+linear_filters[0],linear_filters[3])\n",
    "        self.batch_norm4 = nn.BatchNorm1d(linear_filters[3])\n",
    "        self.act = F.relu\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #layer1\n",
    "        out1 = self.linear1(x)\n",
    "        out1 = self.batch_norm1(out1)\n",
    "        out1 = self.act(out1)\n",
    "\n",
    "        #layer2\n",
    "        out2 = self.linear2(out1)\n",
    "        out2 = self.batch_norm2(out2)\n",
    "        out2 = self.act(out2)\n",
    "\n",
    "        #layer3\n",
    "        out3 = self.linear3(out2)\n",
    "        out3 = self.batch_norm3(out3)\n",
    "        out3 = self.act(out3)\n",
    "\n",
    "        concat = torch.cat([out1,out2,out3],-1)\n",
    "\n",
    "        #layer4\n",
    "        out4 = self.linear4(concat)\n",
    "        out4 = self.batch_norm4(out4)\n",
    "        out4 = self.act(out4)\n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SOTAEmbedding(\n",
    "    linear_filters=[1024,1024,1024,400],\n",
    "    input_feat=36,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SOTAEmbedding                            [32, 400]                 --\n",
       "├─Linear: 1-1                            [32, 1024]                37,888\n",
       "├─BatchNorm1d: 1-2                       [32, 1024]                2,048\n",
       "├─Linear: 1-3                            [32, 1024]                1,049,600\n",
       "├─BatchNorm1d: 1-4                       [32, 1024]                2,048\n",
       "├─Linear: 1-5                            [32, 1024]                1,049,600\n",
       "├─BatchNorm1d: 1-6                       [32, 1024]                2,048\n",
       "├─Linear: 1-7                            [32, 400]                 1,229,200\n",
       "├─BatchNorm1d: 1-8                       [32, 400]                 800\n",
       "==========================================================================================\n",
       "Total params: 3,373,232\n",
       "Trainable params: 3,373,232\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 107.94\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.78\n",
       "Params size (MB): 13.49\n",
       "Estimated Total Size (MB): 15.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(32, 36))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, ft_size, n_classes, num_heads=1, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_ft,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fcLayer1 = nn.Linear(self.d_model, self.ft_size)\n",
    "        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.drop(out)\n",
    "        out = self.act(out)\n",
    "        out = self.fcLayer1(out)\n",
    "        # out = self.fcLayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IMUEncoder                               [32, 128]                 --\n",
       "├─LSTM: 1-1                              [32, 52, 128]             220,160\n",
       "├─Dropout: 1-2                           [32, 128]                 --\n",
       "├─ReLU: 1-3                              [32, 128]                 --\n",
       "├─Linear: 1-4                            [32, 128]                 16,512\n",
       "==========================================================================================\n",
       "Total params: 236,672\n",
       "Trainable params: 236,672\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 366.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.28\n",
       "Forward/backward pass size (MB): 1.74\n",
       "Params size (MB): 0.95\n",
       "Estimated Total Size (MB): 2.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IMUEncoder(in_ft=42, d_model=128, num_heads=2, ft_size=128, n_classes=2, max_len=52, dropout=0.1)\n",
    "summary(model, input_size=(32, 52, 42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, ft_size, n_classes, num_heads=1, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_ft,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n",
    "        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_forward = out[:, self.max_len - 1, :self.d_model]\n",
    "        out_reverse = out[:, 0, self.d_model:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        out = self.drop(out_reduced)\n",
    "        out = self.act(out)\n",
    "        out = self.fcLayer1(out)\n",
    "        # out = self.fcLayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IMUEncoder                               [32, 128]                 --\n",
       "├─LSTM: 1-1                              [32, 52, 256]             571,392\n",
       "├─Dropout: 1-2                           [32, 256]                 --\n",
       "├─ReLU: 1-3                              [32, 256]                 --\n",
       "├─Linear: 1-4                            [32, 128]                 32,896\n",
       "==========================================================================================\n",
       "Total params: 604,288\n",
       "Trainable params: 604,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 951.85\n",
       "==========================================================================================\n",
       "Input size (MB): 0.28\n",
       "Forward/backward pass size (MB): 3.44\n",
       "Params size (MB): 2.42\n",
       "Estimated Total Size (MB): 6.14\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IMUEncoder(in_ft=42, d_model=128, num_heads=2, ft_size=128, n_classes=2, max_len=52, dropout=0.1)\n",
    "summary(model, input_size=(32, 52, 42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, n_classes, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu  # _get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.logist = nn.Linear(self.ft_size, self.n_classes)\n",
    "        self.DenseL3 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out = self.Act(out)\n",
    "        out = self.AvgPoolL(out)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.DenseL2(out)   \n",
    "        out = self.DenseL3(out)\n",
    "        # out = self.logist(out1)     \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IMUEncoder                               [32, 400]                 802\n",
       "├─Linear: 1-1                            [32, 120, 128]            5,504\n",
       "├─RelativeGlobalAttention: 1-2           [32, 120, 128]            7,680\n",
       "│    └─Linear: 2-1                       [32, 120, 128]            16,512\n",
       "│    └─Linear: 2-2                       [32, 120, 128]            16,512\n",
       "│    └─Linear: 2-3                       [32, 120, 128]            16,512\n",
       "│    └─Dropout: 2-4                      [32, 120, 128]            --\n",
       "├─AvgPool2d: 1-3                         [32, 1, 128]              --\n",
       "├─Dropout1d: 1-4                         [32, 128]                 --\n",
       "├─Linear: 1-5                            [32, 400]                 51,600\n",
       "├─Linear: 1-6                            [32, 400]                 160,400\n",
       "==========================================================================================\n",
       "Total params: 275,522\n",
       "Trainable params: 275,522\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 8.55\n",
       "==========================================================================================\n",
       "Input size (MB): 0.65\n",
       "Forward/backward pass size (MB): 15.93\n",
       "Params size (MB): 1.07\n",
       "Estimated Total Size (MB): 17.65\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IMUEncoder(in_ft=42, d_model=128, num_heads=2, ft_size=400, n_classes=2, max_len=120, dropout=0.1)\n",
    "summary(model, input_size=(32, 120, 42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention + LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, num_heads, ft_size, n_classes, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # feature prep layer\n",
    "        self.DenseL = nn.Linear(in_ft, d_model)\n",
    "        # relative global attention layer\n",
    "        self.AttnL = RelativeGlobalAttention(self.d_model, self.num_heads, self.max_len)\n",
    "        self.lstm = nn.LSTM(input_size=self.d_model,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        # positional encoding concat <-> 1DConv \n",
    "        self.Act = F.relu  # _get_activation_fn(activation)\n",
    "        self.AvgPoolL = nn.AvgPool2d((self.max_len,1))\n",
    "        self.DenseL2 = nn.Linear(self.d_model, self.ft_size)\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "        self.logist = nn.Linear(self.ft_size, self.n_classes)\n",
    "        self.DenseL3 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.DenseL(x)\n",
    "        out = self.AttnL(out)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.Act(out)\n",
    "        # out = self.AvgPoolL(out)\n",
    "        # out = torch.squeeze(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.DenseL2(out)   \n",
    "        # out = self.DenseL3(out)\n",
    "        # out = self.logist(out1)     \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IMUEncoder                               [32, 400]                 161,202\n",
       "├─Linear: 1-1                            [32, 120, 128]            5,504\n",
       "├─RelativeGlobalAttention: 1-2           [32, 120, 128]            7,680\n",
       "│    └─Linear: 2-1                       [32, 120, 128]            16,512\n",
       "│    └─Linear: 2-2                       [32, 120, 128]            16,512\n",
       "│    └─Linear: 2-3                       [32, 120, 128]            16,512\n",
       "│    └─Dropout: 2-4                      [32, 120, 128]            --\n",
       "├─LSTM: 1-3                              [32, 120, 128]            264,192\n",
       "├─Dropout1d: 1-4                         [32, 128]                 --\n",
       "├─Linear: 1-5                            [32, 400]                 51,600\n",
       "==========================================================================================\n",
       "Total params: 539,714\n",
       "Trainable params: 539,714\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.65\n",
       "Forward/backward pass size (MB): 19.76\n",
       "Params size (MB): 1.48\n",
       "Estimated Total Size (MB): 21.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IMUEncoder(in_ft=42, d_model=128, num_heads=2, ft_size=400, n_classes=2, max_len=120, dropout=0.1)\n",
    "summary(model, input_size=(32, 120, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
