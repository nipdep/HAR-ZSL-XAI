{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init training pipeline with new models\n",
    "\n",
    "#### Sub-modules \n",
    "- [har-imu-transformer](https://github.com/yolish/har-with-imu-transformer/tree/main)\n",
    "- [sgn skeleton encoder](https://github.com/microsoft/SGN/tree/master)\n",
    "- custom skeleton decoder\n",
    "\n",
    "#### Training\n",
    "- train with matching and non-matching IMU and skeleton pairs\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipun\\AppData\\Local\\Temp\\ipykernel_7196\\4166601334.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "# import neptune\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.utils.analysis import action_evaluator\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "from src.utils.losses import *\n",
    "from src.utils.analysis import action_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'G:/FYP/Codebases/Pose-AE/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\DUET\\notebooks\\..\\src\\datasets\\data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2ReaderV2(data_root+'/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 60, 36)\n",
      "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
      "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
      "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
      "       'rope jumping', 'running', 'sitting', 'standing',\n",
      "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10], dtype=int64))\n",
      "[(1, 'lying'), (2, 'sitting'), (3, 'standing'), (4, 'walking'), (5, 'running'), (6, 'cycling'), (7, 'Nordic walking'), (9, 'watching TV'), (10, 'computer work'), (11, 'car driving'), (12, 'ascending stairs'), (13, 'descending stairs'), (16, 'vacuum cleaning'), (17, 'ironing'), (18, 'folding laundry'), (19, 'house cleaning'), (20, 'playing soccer'), (24, 'rope jumping')]\n",
      "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
      "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
      "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
      "       'rope jumping', 'running', 'sitting', 'standing',\n",
      "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "skeleton_data = np.load(data_root+'/skeleton_k10_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\n",
    "\n",
    "print(skeleton_mov.shape)\n",
    "print(np.unique(skeleton_classes, return_counts=True))\n",
    "print(dataReader.label_map)\n",
    "print(np.unique(skeleton_classes, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 60, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeleton_mov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'lying',\n",
       " 1: 'sitting',\n",
       " 2: 'standing',\n",
       " 3: 'walking',\n",
       " 4: 'running',\n",
       " 5: 'cycling',\n",
       " 6: 'Nordic walking',\n",
       " 7: 'watching TV',\n",
       " 8: 'computer work',\n",
       " 9: 'car driving',\n",
       " 10: 'ascending stairs',\n",
       " 11: 'descending stairs',\n",
       " 12: 'vacuum cleaning',\n",
       " 13: 'ironing',\n",
       " 14: 'folding laundry',\n",
       " 15: 'house cleaning',\n",
       " 16: 'playing soccer',\n",
       " 17: 'rope jumping'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action2label = {i:j for i,j in enumerate(dataReader.idToLabel)}\n",
    "action2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lying',\n",
       " 'sitting',\n",
       " 'standing',\n",
       " 'walking',\n",
       " 'running',\n",
       " 'cycling',\n",
       " 'Nordic walking',\n",
       " 'watching TV',\n",
       " 'computer work',\n",
       " 'car driving',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'vacuum cleaning',\n",
       " 'ironing',\n",
       " 'folding laundry',\n",
       " 'house cleaning',\n",
       " 'playing soccer',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[0,1,2,3,4,5], seen_ratio=0.8, unseen_ratio=0.1, window_size=12, window_overlap=10, resample_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, imu_data, imu_actions, skel_data, skel_label, action2label, active_cls, skel_len=120):\n",
    "        super(BaseDataset, self).__init__()\n",
    "        self.imu_data = torch.from_numpy(imu_data)\n",
    "        self.imu_actions = imu_actions\n",
    "        new_fts = [i for i in range(skel_data.shape[-1]) if i%3 != 2]\n",
    "        self.skel_data = torch.from_numpy(skel_data[:, :skel_len, new_fts])\n",
    "        self.skel_label = skel_label\n",
    "        self.action2label = action2label\n",
    "        # build action to id mapping dict\n",
    "        self.skel_len = skel_len\n",
    "        self.active_cls = active_cls\n",
    "        self.n_action = len(self.active_cls)\n",
    "        self.action2Id = dict(zip(active_cls, range(self.n_action)))\n",
    "        self.Id2action = dict(zip(range(self.n_action), active_cls))\n",
    "        self.__setup_skel_dict()\n",
    "    \n",
    "    def __setup_skel_dict(self):\n",
    "        action_dict = defaultdict(list)\n",
    "        for i, c in enumerate(self.skel_label):\n",
    "            if c in self.active_cls:\n",
    "                action_dict[c].append(self.skel_data[i])\n",
    "        \n",
    "        self.skel_dict = action_dict #{i: np.array(j) for i,j in action_dict.items()}\n",
    "\n",
    "    def __get_skel(self, anchor_id, flip):\n",
    "        if flip:\n",
    "            lbl_ind = anchor_id\n",
    "        else:\n",
    "            lbl_ind = random.randint(0, self.n_action)\n",
    "            # lbl_ind = self.n_action - anchor_id - 1  # has to update\n",
    "\n",
    "        lbl = self.Id2action[lbl_ind]\n",
    "        ind = random.randint(0, 10) # has to update\n",
    "        skel = self.skel_dict[lbl][ind]\n",
    "        return skel \n",
    "        \n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.imu_data[ind, ...]\n",
    "        anchor_action = self.imu_actions[ind]\n",
    "        anchor_label = self.action2label[anchor_action]\n",
    "        anchor_id = self.action2Id[anchor_label]\n",
    "        \n",
    "        y = random.randint(0, 2)\n",
    "        skel = self.__get_skel(anchor_id, y)\n",
    "        return x, y, skel\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imu_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_c = ['Nordic walking', 'watching TV', 'computer work', 'car driving', 'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n",
    "dataset = BaseDataset(imu_data=data_dict['train']['X'], imu_actions=data_dict['train']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=act_c, skel_len=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_dict['train']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =  DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 54]) torch.Size([32]) torch.Size([32, 60, 24])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y,skel in dl:\n",
    "    print(x.shape, y.shape, skel.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnseenDataset(Dataset):\n",
    "    def __init__(self, imu_data, imu_actions, skel_data, skel_label, action2label, active_cls, skel_len=120):\n",
    "        super(UnseenDataset, self).__init__()\n",
    "        self.imu_data = torch.from_numpy(imu_data)\n",
    "        self.imu_actions = imu_actions\n",
    "        new_fts = [i for i in range(skel_data.shape[-1]) if i%3 != 2]\n",
    "        self.skel_data = skel_data[:, :skel_len, new_fts]\n",
    "        self.skel_label = skel_label\n",
    "        self.action2label = action2label\n",
    "        # build action to id mapping dict\n",
    "        self.skel_len = skel_len\n",
    "        self.active_cls = active_cls\n",
    "        self.n_action = len(self.active_cls)\n",
    "        self.action2Id = dict(zip(active_cls, range(self.n_action)))\n",
    "        self.Id2action = dict(zip(range(self.n_action), active_cls))\n",
    "        self.__setup_skel_dict()\n",
    "\n",
    "    def __setup_skel_dict(self):\n",
    "        action_dict = defaultdict(list)\n",
    "        for i, c in enumerate(self.skel_label):\n",
    "            if c in self.active_cls:\n",
    "                action_dict[c] = self.skel_data[i]\n",
    "        \n",
    "        self.skel_dict = action_dict\n",
    "        self.skel_anchor = torch.from_numpy(np.array([action_dict[i] for i in self.active_cls]))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.imu_data[ind, ...]\n",
    "        anchor_action = self.imu_actions[ind]\n",
    "        anchor_label = self.action2label[anchor_action]\n",
    "        anchor_id = self.action2Id[anchor_label]\n",
    "        \n",
    "        return x, anchor_id, self.skel_anchor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imu_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_cls = ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling']\n",
    "unseen_dt = UnseenDataset(imu_data=data_dict['test']['X'], imu_actions=data_dict['test']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=unseen_cls, skel_len=60)\n",
    "\n",
    "unseen_dl =  DataLoader(unseen_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 54]) torch.Size([32]) torch.Size([32, 6, 60, 24])\n"
     ]
    }
   ],
   "source": [
    "for ux, uy, uskels in unseen_dl:\n",
    "    print(ux.shape, uy.shape, uskels.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.modeling_sgn import embed, local, gcn_spa, compute_g_spa\n",
    "from src.models.modeling_lxmert import LxmertConfig, LxmertXLayer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNEncoder(nn.Module):\n",
    "    def __init__(self, num_joint, seg, hidden_size=128, bs=32, is_3d=True, train=True, bias=True, device='cpu'):\n",
    "        super(SGNEncoder, self).__init__()\n",
    "\n",
    "        self.dim1 = hidden_size\n",
    "        self.dim_unit = hidden_size // 4 \n",
    "        self.seg = seg\n",
    "        self.num_joint = num_joint\n",
    "        self.bs = bs\n",
    "\n",
    "        if is_3d:\n",
    "          self.spatial_dim = 3\n",
    "        else:\n",
    "          self.spatial_dim = 2\n",
    "\n",
    "        if train:\n",
    "            self.spa = self.one_hot(bs, num_joint, self.seg)\n",
    "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
    "            self.tem = self.one_hot(bs, self.seg, num_joint)\n",
    "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
    "        else:\n",
    "            self.spa = self.one_hot(32 * 5, num_joint, self.seg)\n",
    "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
    "            self.tem = self.one_hot(32 * 5, self.seg, num_joint)\n",
    "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        self.tem_embed = embed(self.seg, joint=self.num_joint, hidden_dim=self.dim_unit*4, norm=False, bias=bias)\n",
    "        self.spa_embed = embed(num_joint, joint=self.num_joint, hidden_dim=self.dim_unit, norm=False, bias=bias)\n",
    "        self.joint_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n",
    "        self.dif_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n",
    "        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n",
    "        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
    "        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n",
    "        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
    "        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n",
    "        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n",
    "        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Dynamic Representation\n",
    "        x = x.view((self.bs, self.seg, self.num_joint, self.spatial_dim))\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        dif = x[:, :, :, 1:] - x[:, :, :, 0:-1]\n",
    "        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n",
    "        # print(x.shape)\n",
    "        pos = self.joint_embed(x)\n",
    "        tem1 = self.tem_embed(self.tem)\n",
    "        spa1 = self.spa_embed(self.spa)\n",
    "        dif = self.dif_embed(dif)\n",
    "        dy = torch.add(pos, dif)\n",
    "        # Joint-level Module\n",
    "        x= torch.cat([dy, spa1], 1)\n",
    "        g = self.compute_g1(x)\n",
    "        x = self.gcn1(x, g)\n",
    "        x = self.gcn2(x, g)\n",
    "        x = self.gcn3(x, g)\n",
    "        # Frame-level Module\n",
    "        x = torch.add(x, tem1)\n",
    "        x = self.cnn(x)\n",
    "        output_feat = torch.squeeze(x).permute(0,2,1)\n",
    "\n",
    "        return output_feat\n",
    "\n",
    "    def one_hot(self, bs, spa, tem):\n",
    "\n",
    "        y = torch.arange(spa).unsqueeze(-1)\n",
    "        y_onehot = torch.FloatTensor(spa, spa)\n",
    "\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n",
    "        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n",
    "\n",
    "        return y_onehot\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.seq_len, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.maxpool = nn.AdaptiveMaxPool2d([self.seq_len, self.seq_len])\n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        \"\"\"\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        \"\"\"\n",
    "        output = self.maxpool(encoder_hidden)\n",
    "        lstm_out, self.hidden = self.lstm(output)\n",
    "        x = self.net(lstm_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUTransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: (dict) configuration of the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_dim = config.get(\"transformer_dim\")\n",
    "\n",
    "        self.input_proj = nn.Sequential(nn.Conv1d(config.get(\"input_dim\"), self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU())\n",
    "\n",
    "        self.window_size = config.get(\"window_size\")\n",
    "        self.encode_position = config.get(\"encode_position\")\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = self.transformer_dim,\n",
    "                                       nhead = config.get(\"nhead\"),\n",
    "                                       dim_feedforward = config.get(\"dim_feedforward\"),\n",
    "                                       dropout = config.get(\"transformer_dropout\"),\n",
    "                                       activation = config.get(\"transformer_activation\"))\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer,\n",
    "                                              num_layers = config.get(\"num_encoder_layers\"),\n",
    "                                              norm = nn.LayerNorm(self.transformer_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros((1, self.transformer_dim)), requires_grad=True)\n",
    "\n",
    "        if self.encode_position:\n",
    "            self.position_embed = nn.Parameter(torch.randn(self.window_size + 1, 1, self.transformer_dim))\n",
    "\n",
    "        # init\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        src = data  # Shape N x S x C with S = sequence length, N = batch size, C = channels\n",
    "        src = self.input_proj(src.transpose(1, 2)).permute(2, 0, 1)\n",
    "\n",
    "        cls_token = self.cls_token.unsqueeze(1).repeat(1, src.shape[1], 1)\n",
    "        src = torch.cat([cls_token, src])\n",
    "\n",
    "        if self.encode_position:\n",
    "            src += self.position_embed\n",
    "\n",
    "        target = self.transformer_encoder(src)\n",
    "        target = torch.squeeze(target.permute(1,0,2))\n",
    "        return target\n",
    "\n",
    "def get_activation(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if activation == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    raise RuntimeError(\"Activation {} not supported\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "  def __init__(self, window_size, embedding_size, hidden_size):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.imu_head = nn.Sequential(\n",
    "            nn.AvgPool2d((window_size,1)),\n",
    "            nn.Linear(embedding_size,  hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "        output = self.imu_head(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        self.imu_model = IMUTransformerEncoder(config['imu_config'])\n",
    "        self.skel_encoder = SGNEncoder(**config['sgn_config'])\n",
    "        self.skel_decoder = BiLSTMDecoder(**config['dec_config'])\n",
    "        self.fc_head = ClassifierHead(**config['clf_config'])\n",
    "        self.lxmert_config = LxmertConfig(**config['xmert_config'])\n",
    "        self.lxmert_xlayer = LxmertXLayer(self.lxmert_config)\n",
    "\n",
    "        self.num_layers = config['num_layers']\n",
    "\n",
    "    def forward(self, x_imu, x_skel):\n",
    "        imu_feats = self.imu_model(x_imu)\n",
    "        skel_feats = self.skel_encoder(x_skel)\n",
    "        # print(f\"imu_feats {imu_feats.shape} | skel_feats {skel_feats.shape}\")\n",
    "        for i in range(self.num_layers):\n",
    "            x_outputs = self.lxmert_xlayer(\n",
    "                lang_feats = skel_feats,\n",
    "                lang_attention_mask = None,  \n",
    "                visual_feats = imu_feats,\n",
    "                visual_attention_mask = None,\n",
    "                input_id = None,\n",
    "                output_attentions=False,\n",
    "            )\n",
    "            skel_feats, imu_feats = x_outputs[:2]\n",
    "\n",
    "        # print(f\"imu_feats {imu_feats.shape} | skel_feats {skel_feats.shape}\")\n",
    "        skel_recon = self.skel_decoder(skel_feats)\n",
    "        # imu_feats = torch.squeeze(imu_feats)\n",
    "        bin_output = torch.squeeze(self.fc_head(imu_feats))\n",
    "        return bin_output, skel_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_config = {\n",
    "\t\"input_dim\": 54,\n",
    "    \"window_size\":24,\n",
    "\t\"encode_position\":True,\n",
    "\t\"transformer_dim\": 512,\n",
    "\t\"nhead\": 8,\n",
    "\t\"num_encoder_layers\": 6, \n",
    "\t\"dim_feedforward\": 128, \n",
    "\t\"transformer_dropout\": 0.1, \n",
    "\t\"transformer_activation\": \"gelu\",\n",
    "\t\"head_activation\": \"gelu\",\n",
    "    \"baseline_dropout\": 0.1,\n",
    "\t\"batch_size\": 32,\n",
    "\t\"output_size\": 512\n",
    "}\n",
    "\n",
    "sgn_config = {\n",
    "    'num_joint': 12,\n",
    "    'seg': 60,\n",
    "    'hidden_size': 256,\n",
    "    'train': True,\n",
    "    'bs': 32,\n",
    "    'is_3d': False\n",
    "}\n",
    "\n",
    "dec_config = {\n",
    "    'seq_len': 60,\n",
    "    'input_size': 24,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': True,\n",
    "    \"embedding_size\": 128,\n",
    "    \"linear_filters\":[128,256,512,1024],\n",
    "}\n",
    "\n",
    "clf_config = {\n",
    "    'window_size': 25,\n",
    "    'embedding_size': 512,\n",
    "    'hidden_size': 256\n",
    "}\n",
    "\n",
    "base_config = {\n",
    "    'imu_config': imu_config,\n",
    "    'sgn_config': sgn_config,\n",
    "    'dec_config': dec_config,\n",
    "    'clf_config': clf_config,\n",
    "    'num_layers': 1,\n",
    "    'xmert_config': {\n",
    "        'vocab_size': 1024,\n",
    "        'hidden_size': 512,\n",
    "        'num_attention_heads': 2,\n",
    "        'intermediate_size': 512\n",
    "    }\n",
    "}\n",
    " \n",
    "base_model = BaseModel(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imu_feats torch.Size([32, 25, 512]) | skel_feats torch.Size([32, 60, 512])\n",
      "imu_feats torch.Size([32, 25, 512]) | skel_feats torch.Size([32, 60, 512])\n"
     ]
    }
   ],
   "source": [
    "imu_output, skel_recon = base_model(x.float(), skel.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32, 60, 24])\n"
     ]
    }
   ],
   "source": [
    "print(imu_output.shape, skel_recon.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dl, optimizer, loss_modules, device, phase='train', loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    with tqdm(dl, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skel = batch \n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "            # print(f\"X : {X.shape} | skel : {skel.shape}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                cls_prob, skel_recon = model(X, skel)\n",
    "                class_loss = loss_modules['classi'](y, cls_prob)\n",
    "                recon_loss = loss_modules['recon'](skel, skel_recon)\n",
    "\n",
    "            # print(f\" Check for nan | X : {torch.isnan(X).sum()} | X-inf : {torch.isinf(X).sum()} | skel : {torch.isnan(skel).sum()} | skel-inf : {torch.isinf(skel).sum()} | cls_prob : {torch.isnan(cls_prob).sum()} | skel_recon : {torch.isnan(skel_recon).sum()}\")\n",
    "            # print(\"class loss : \", class_loss.item(), \" recon_loss : \", recon_loss.item())\n",
    "            loss = (1-loss_alpha)*class_loss + loss_alpha*recon_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     print(name, torch.isfinite(param.grad).all())\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            # pred_class = cls_prob.cpu().detach().numpy()\n",
    "            y_pred = (cls_prob.cpu().detach().numpy() >= 0.5).astype(int)\n",
    "            # print(\"y_true : \", y.cpu().detach().numpy().astype(int), \" y_pred : \", pred_class)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=y.cpu().detach().numpy().astype(int), y_pred=y_pred)\n",
    "        return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(model, dl, optimizer, loss_modules, device, class_names, phase='train', loss_alpha=0.7, print_report=False, show_plot=False):\n",
    "    model = model.eval()\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"recon. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(dl, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skel = batch \n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                cls_prob, skel_recon = model(X, skel)\n",
    "                class_loss = loss_modules['classi'](y, cls_prob)\n",
    "                recon_loss = loss_modules['recon'](skel, skel_recon)\n",
    "\n",
    "            loss = (1-loss_alpha)*class_loss + loss_alpha*recon_loss\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metrics['samples'] += len(y)\n",
    "                metrics['loss'] += loss.item()  # add total loss of batch\n",
    "                metrics['recon. loss'] += recon_loss.item()\n",
    "                metrics['classi. loss'] += class_loss.item()\n",
    "\n",
    "            y_pred = (cls_prob.cpu().detach().numpy() >= 0.5).astype(int)\n",
    "            per_batch['targets'].append(y.cpu().numpy())\n",
    "            per_batch['predictions'].append(y_pred)\n",
    "            per_batch['metrics'].append([loss.detach().cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets.astype(int), class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    metrics_dict.update(metrics)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_eval(model, unseen_dl, device, class_names, print_report=True, show_plot=True):\n",
    "    model = model.eval()\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': [], 'feat': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"recon. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(unseen_dl, unit='batch') as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skels = batch \n",
    "            X = X.float().to(device)\n",
    "            y = y.float()\n",
    "            skels = skels.float().to(device)\n",
    "\n",
    "        skels = skels.permute(1,0,2,3)\n",
    "        preds = []\n",
    "        for i in range(len(class_names)):\n",
    "            with torch.no_grad():\n",
    "                cls_prob, skel_recon = model(X, skels[i, ...])\n",
    "                preds.append(cls_prob.cpu().detach().numpy())\n",
    "            \n",
    "        preds = np.array(preds)\n",
    "        print(\"preds shape : \", preds.shape)\n",
    "        per_batch['targets'].append(y.cpu().numpy())\n",
    "        per_batch['predictions'].append(preds)\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(df):\n",
    "    df['loss'] = df['loss']/df['samples']\n",
    "    df['recon. loss'] = df['recon. loss']/df['samples']\n",
    "    df['classi. loss'] = df['classi. loss']/df['samples']\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=4)\n",
    "    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='recon. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Reconstruction Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"Transfomer-Encder\",\n",
    "    \"sem-space\": 'I3D',\n",
    "    # model training configs\n",
    "    \"lr\": 0.0001,\n",
    "    \"loss_alpha\": 0.9,\n",
    "    \"n_epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    # model configs\n",
    "    \"feat_size\": 512, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 12, \n",
    "    \"overlap\": 10,\n",
    "    \"freq\": 50,\n",
    "    \"seq_len\": 60,  # skeleton seq. length\n",
    "    \"seen_split\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1,2], seen_ratio=config['seen_split'], unseen_ratio=0.8, window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['freq'])\n",
    "dl =  DataLoader(dataset, batch_size=8, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for x,y,skel in dl:\n",
    "    _, seq_len, imu_feat = x.shape \n",
    "    _, timestamps, joints = skel.shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_config = {\n",
    "\t\"input_dim\": imu_feat, # \n",
    "    \"window_size\":seq_len, # \n",
    "\t\"encode_position\":True,\n",
    "\t\"transformer_dim\": config['feat_size'], ##\n",
    "\t\"nhead\": 8,\n",
    "\t\"num_encoder_layers\": 6, \n",
    "\t\"dim_feedforward\": 128, \n",
    "\t\"transformer_dropout\": 0.1, \n",
    "\t\"transformer_activation\": \"gelu\",\n",
    "\t\"head_activation\": \"gelu\",\n",
    "    \"baseline_dropout\": 0.1,\n",
    "\t\"batch_size\": 32,\n",
    "\t\"output_size\": config['feat_size'] ##\n",
    "}\n",
    "\n",
    "sgn_config = {\n",
    "    'num_joint': joints//2, #\n",
    "    'seg': timestamps, #\n",
    "    'hidden_size': config['feat_size']//2, \n",
    "    'train': True,\n",
    "    'bs': config['batch_size'], ##\n",
    "    'is_3d': False,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "dec_config = {\n",
    "    'seq_len': timestamps, #\n",
    "    'input_size': joints, #\n",
    "    'hidden_size': 256, \n",
    "    'num_layers': 2,\n",
    "    'bidirectional': True,\n",
    "    \"embedding_size\": 128, ##\n",
    "    \"linear_filters\":[128,256,512,1024],\n",
    "}\n",
    "\n",
    "clf_config = {\n",
    "    'window_size': seq_len+1,\n",
    "    'embedding_size': config['feat_size'],\n",
    "    'hidden_size': 256\n",
    "}\n",
    "\n",
    "base_config = {\n",
    "    'imu_config': imu_config,\n",
    "    'sgn_config': sgn_config,\n",
    "    'dec_config': dec_config,\n",
    "    'clf_config': clf_config,\n",
    "    'num_layers': 1,\n",
    "    'xmert_config': {\n",
    "        'vocab_size': 256,\n",
    "        'hidden_size': config['feat_size'], #\n",
    "        'num_attention_heads': 2,\n",
    "        'intermediate_size': config['feat_size'] #\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(fold, phase, metrics):\n",
    "    for m, v in metrics.items():\n",
    "        if fold == 'global':\n",
    "            run[f'global/{m}'].log(v)\n",
    "        else:\n",
    "            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "seen classes >  ['lying', 'sitting', 'walking', 'running', 'cycling', 'Nordic walking', 'computer work', 'car driving', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'playing soccer', 'rope jumping']\n",
      "unseen classes >  ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:15<00:00,  3.74batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.54batch/s, loss=nan]\n",
      "Training Epoch:   7%|▋         | 1/15 [01:19<18:30, 79.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:15<00:00,  3.74batch/s]\n",
      "train: 100%|██████████| 31/31 [00:02<00:00, 10.75batch/s, loss=nan]\n",
      "Training Epoch:  13%|█▎        | 2/15 [02:38<17:08, 79.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:16<00:00,  3.72batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00, 10.29batch/s, loss=nan]\n",
      "Training Epoch:  20%|██        | 3/15 [03:57<15:50, 79.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:23<00:00,  3.41batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.36batch/s, loss=nan]\n",
      "Training Epoch:  27%|██▋       | 4/15 [05:24<15:03, 82.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:27<00:00,  3.25batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.10batch/s, loss=nan]\n",
      "Training Epoch:  33%|███▎      | 5/15 [06:55<14:12, 85.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:26<00:00,  3.28batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.28batch/s, loss=nan]\n",
      "Training Epoch:  40%|████      | 6/15 [08:25<13:02, 86.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.486\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:26<00:00,  3.27batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.22batch/s, loss=nan]\n",
      "Training Epoch:  47%|████▋     | 7/15 [09:55<11:44, 88.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:26<00:00,  3.27batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.31batch/s, loss=nan]\n",
      "Training Epoch:  53%|█████▎    | 8/15 [11:25<10:21, 88.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:27<00:00,  3.23batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.27batch/s, loss=nan]\n",
      "Training Epoch:  60%|██████    | 9/15 [12:56<08:57, 89.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.525\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:27<00:00,  3.23batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.30batch/s, loss=nan]\n",
      "Training Epoch:  67%|██████▋   | 10/15 [14:28<07:30, 90.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.493\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:25<00:00,  3.33batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.53batch/s, loss=nan]\n",
      "Training Epoch:  73%|███████▎  | 11/15 [15:56<05:58, 89.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:26<00:00,  3.29batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.51batch/s, loss=nan]\n",
      "Training Epoch:  80%|████████  | 12/15 [17:26<04:28, 89.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:27<00:00,  3.26batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  8.89batch/s, loss=nan]\n",
      "Training Epoch:  87%|████████▋ | 13/15 [18:57<02:59, 89.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:25<00:00,  3.33batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.45batch/s, loss=nan]\n",
      "Training Epoch:  93%|█████████▎| 14/15 [20:25<01:29, 89.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 284/284 [01:25<00:00,  3.32batch/s]\n",
      "train: 100%|██████████| 31/31 [00:03<00:00,  9.39batch/s, loss=nan]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.511\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m         best_model \u001b[39m=\u001b[39m deepcopy(model\u001b[39m.\u001b[39mstate_dict())\n\u001b[0;32m     56\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\u001b[39m.\u001b[39mfrom_records(train_data)\n\u001b[1;32m---> 57\u001b[0m plot_curves(train_df)\n\u001b[0;32m     59\u001b[0m \u001b[39m# replace by best model \u001b[39;00m\n\u001b[0;32m     60\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(best_model)\n",
      "Cell \u001b[1;32mIn[32], line 7\u001b[0m, in \u001b[0;36mplot_curves\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mclassi. loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mclassi. loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(nrows\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m sns\u001b[39m.\u001b[39;49mlineplot(data\u001b[39m=\u001b[39;49mdf, x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, y\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m, hue\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mphase\u001b[39;49m\u001b[39m'\u001b[39;49m, marker\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m'\u001b[39;49m, ax\u001b[39m=\u001b[39;49maxs[\u001b[39m2\u001b[39;49m])\u001b[39m.\u001b[39mset(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m sns\u001b[39m.\u001b[39mlineplot(data\u001b[39m=\u001b[39mdf, x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecon. loss\u001b[39m\u001b[39m'\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m, ax\u001b[39m=\u001b[39maxs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mset(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReconstruction Loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m sns\u001b[39m.\u001b[39mlineplot(data\u001b[39m=\u001b[39mdf, x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mclassi. loss\u001b[39m\u001b[39m'\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m, ax\u001b[39m=\u001b[39maxs[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mset(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClassification Loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\seaborn\\relational.py:645\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m color \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m\"\u001b[39m, kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    643\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _default_color(ax\u001b[39m.\u001b[39mplot, hue, color, kwargs)\n\u001b[1;32m--> 645\u001b[0m p\u001b[39m.\u001b[39;49mplot(ax, kwargs)\n\u001b[0;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\seaborn\\relational.py:459\u001b[0m, in \u001b[0;36m_LinePlotter.plot\u001b[1;34m(self, ax, kws)\u001b[0m\n\u001b[0;32m    457\u001b[0m         lines\u001b[39m.\u001b[39mextend(ax\u001b[39m.\u001b[39mplot(unit_data[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m], unit_data[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws))\n\u001b[0;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     lines \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mplot(sub_data[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m], sub_data[\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws)\n\u001b[0;32m    461\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhue\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sub_vars:\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run['parameters'] = config\n",
    "fold_metric_scores = []\n",
    "\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=0.8, window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['freq'])\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = [all_classes[i] for i in data_dict['seen_classes']]\n",
    "    unseen_classes = [all_classes[i] for i in data_dict['unseen_classes']]\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    # train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = BaseDataset(imu_data=data_dict['train']['X'], imu_actions=data_dict['train']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=seen_classes, skel_len=timestamps)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = BaseDataset(imu_data=data_dict['eval-seen']['X'], imu_actions=data_dict['eval-seen']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=seen_classes, skel_len=timestamps)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = UnseenDataset(imu_data=data_dict['test']['X'], imu_actions=data_dict['test']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=unseen_classes, skel_len=timestamps)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    model = BaseModel(base_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "    loss_modules = {'classi': nn.BCELoss(), 'recon': nn.L1Loss()}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # train the model \n",
    "    train_data = []\n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "    \n",
    "        train_metrics = train_step(model, train_dl, optimizer, loss_modules, device, phase='train', loss_alpha=config['loss_alpha'])\n",
    "        train_metrics['epoch'] = epoch\n",
    "        train_metrics['phase'] = 'train'\n",
    "        train_data.append(train_metrics)\n",
    "        # log(i, 'train', train_metrics)\n",
    "\n",
    "        eval_metrics = validate_step(model, eval_dl, optimizer, loss_modules, device, seen_classes, phase='train', loss_alpha=config['loss_alpha'], print_report=False, show_plot=False)\n",
    "        eval_metrics['epoch'] = epoch \n",
    "        eval_metrics['phase'] = 'valid'\n",
    "        train_data.append(eval_metrics)\n",
    "        # log(i, 'valid', train_metrics)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    train_df = pd.DataFrame().from_records(train_data)\n",
    "    plot_curves(train_df)\n",
    "\n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "    # save_model(model,notebook_iden,model_iden,i)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = unseen_eval(model, test_dl, device, unseen_classes, print_report=True, show_plot=True)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    # log('test', i, test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "print(seen_score_df.mean())\n",
    "# log('global', '',seen_score_df.mean().to_dict())\n",
    "# run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
