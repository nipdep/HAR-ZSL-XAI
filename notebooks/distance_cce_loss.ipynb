{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch import nn, Tensor \n",
    "\n",
    "from random import sample, choice\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32, device=device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceCCELoss(nn.Module):\n",
    "    # def __init__(self, action_feats, label_map, pos_thr=0.05, neg_thr=0.95, agg=\"mean\"):\n",
    "    def __init__(self, reference_mat, distance='cosine', theta=1e-4, agg=\"mean\", device=\"cpu\"):\n",
    "        super(DistanceCCELoss, self).__init__()\n",
    "        self.ref_mat = reference_mat\n",
    "        self.n, self.c, f = reference_mat.shape \n",
    "        self.ref_mat = reference_mat.view(self.n*self.c, 1, f) \n",
    "        \n",
    "        self.distance = distance\n",
    "        self.get_disMet(distance)\n",
    "\n",
    "        self.agg = agg \n",
    "        self.device = device \n",
    "        self.theta = theta\n",
    "        self.cceLoss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_disMet(self, distance):\n",
    "        if distance == 'cosine':\n",
    "            self.disMet = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        else:\n",
    "            self.disMet = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def forward(self, pred_fts, y):\n",
    "        dist = self.disMet(pred_fts, self.ref_mat)\n",
    "        if self.distance == 'cosine':\n",
    "            dist = 1- torch.abs(dist)\n",
    "\n",
    "        class_dist = dist.view(self.n, self.c, -1)\n",
    "        mean_dist = torch.transpose(class_dist.mean(dim=0), 0, 1)\n",
    "        cce_loss = self.cceLoss(mean_dist, y)\n",
    "        return cce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = DistanceCCELoss(reference_mat=torch.randn((10, 18,128)),distance='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn((32, 128))\n",
    "sample_classes = torch.abs(torch.randn(32)*8).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 18, 32])\n"
     ]
    }
   ],
   "source": [
    "loss = loss_func(sample_input, sample_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8897)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 18, 32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.randn(32, 128)\n",
    "tensor2 = torch.randn(10, 18, 128)\n",
    "\n",
    "# Reshape the second tensor to (10*18, 128)\n",
    "tensor2 = tensor2.reshape(10*18, 1, 128)\n",
    "\n",
    "# Calculate the cosine distance between the two tensors\n",
    "dist = nn.PairwiseDistance()\n",
    "cosine_distance = 1 - dist(tensor1, tensor2)\n",
    "\n",
    "class_distances = cosine_distance.view(10, 18, 32)\n",
    "class_distances.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distances.sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 18])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.transpose(class_distances.mean(dim=0), 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceCCELoss(nn.Module):\n",
    "    # def __init__(self, action_feats, label_map, pos_thr=0.05, neg_thr=0.95, agg=\"mean\"):\n",
    "    def __init__(self, action_feats, label_map, distance='cosine', k=10, pos_thr=0.05, neg_thr=0.95, agg=\"mean\", device=\"cpu\"):\n",
    "        super(DistanceCCELoss, self).__init__()\n",
    "        self.action_feats = deepcopy(action_feats)\n",
    "        self.label_map = label_map\n",
    "        self.distance = distance\n",
    "        self.get_disMet(distance)\n",
    "        self.avg_vector(action_feats)\n",
    "        self.k = k\n",
    "        self.pos_thr = pos_thr\n",
    "        self.neg_thr = neg_thr\n",
    "        self.agg = agg \n",
    "        self.device = device \n",
    "        self.cceLoss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_disMet(self, distance):\n",
    "        if distance == 'cosine':\n",
    "            self.disMet = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        else:\n",
    "            self.disMet = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def avg_vector(self, action_feats):\n",
    "        avg_feats = {i: v.mean(axis=0) for i,v in action_feats.items()}\n",
    "        keys = list(avg_feats.keys())\n",
    "        keys.sort()\n",
    "        self.ref_mat = torch.Tensor([avg_feats[i] for i in keys]).to(device)\n",
    "\n",
    "    def forward(self, pred_fts, y):\n",
    "        # print(pred_fts.shape, self.ref_mat.shape)\n",
    "        pred_fts = torch.unsqueeze(pred_fts, 1)\n",
    "        dist = self.disMet(pred_fts, self.ref_mat)\n",
    "        if self.distance == 'cosine':\n",
    "            dist = 1- torch.abs(dist)\n",
    "\n",
    "        # print(dist.shape)\n",
    "        cce_loss = self.cceLoss(dist, y)\n",
    "        pred_y = torch.argmin(dist, dim=-1)\n",
    "        return cce_loss, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 5, 6, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [9, 3, 5, 6, 10, 2, 0, 1, 12, 11]\n",
    "l.sort()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5105e-01,  9.9553e-02,  1.4555e-02, -4.6371e-02,  3.1975e-02,\n",
       "         1.3174e-01, -8.3854e-03,  1.3381e-01, -5.5751e-02, -3.1016e-02,\n",
       "        -2.5598e-02, -2.9823e-02,  5.5216e-02,  8.6839e-03, -1.1508e-01,\n",
       "        -1.0554e-03, -8.1863e-02,  7.1212e-02,  1.1283e-01, -2.0324e-02,\n",
       "         2.8713e-02, -7.7041e-02,  5.6254e-02,  1.9283e-01,  1.0349e-01,\n",
       "        -3.2376e-03, -2.0324e-01,  2.8374e-02,  4.5663e-02,  7.7501e-03,\n",
       "         2.5818e-02,  3.5124e-02,  1.0218e-01, -2.4832e-02,  1.0733e-01,\n",
       "         6.1876e-02,  5.0084e-02,  1.8377e-01,  5.5532e-02,  3.1180e-03,\n",
       "        -1.2405e-01, -2.0716e-01, -6.0127e-03,  3.5816e-03, -1.3235e-01,\n",
       "         1.5695e-02,  5.4732e-02,  1.4135e-02,  8.7572e-02, -8.4399e-02,\n",
       "         5.5877e-02,  5.8034e-02,  2.4401e-02, -1.1981e-01,  3.0698e-02,\n",
       "        -2.9763e-02, -1.0775e-01, -2.3126e-02,  1.1734e-01, -1.6157e-02,\n",
       "         1.0567e-01,  4.4563e-02, -4.5023e-02,  4.9310e-02, -3.2808e-02,\n",
       "        -6.1295e-02, -9.3148e-03,  4.6464e-02,  4.7387e-03, -4.5208e-02,\n",
       "         3.7407e-02, -8.8090e-02, -1.0580e-01, -1.0589e-01,  1.3357e-01,\n",
       "         4.0688e-02,  1.2819e-01,  2.8393e-02,  1.5385e-01,  9.8569e-02,\n",
       "         1.9903e-01, -9.9475e-02,  8.5229e-02,  2.2682e-03, -1.0292e-01,\n",
       "         4.4437e-02,  3.6676e-02,  2.4726e-01, -1.0914e-01, -3.7369e-02,\n",
       "        -5.8549e-02,  3.6071e-02, -1.6689e-01, -2.4018e-02, -2.1566e-01,\n",
       "         2.3948e-02,  1.1452e-01, -1.7604e-04, -7.7982e-02,  1.6480e-02])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [(1, 'lying'),\n",
    " (2, 'sitting'),\n",
    " (3, 'standing'),\n",
    " (4, 'walking'),\n",
    " (5, 'running'),\n",
    " (6, 'cycling'),\n",
    " (7, 'Nordic walking'),\n",
    " (9, 'watching TV'),\n",
    " (10, 'computer work'),\n",
    " (11, 'car driving'),\n",
    " (12, 'ascending stairs'),\n",
    " (13, 'descending stairs'),\n",
    " (16, 'vacuum cleaning'),\n",
    " (17, 'ironing'),\n",
    " (18, 'folding laundry'),\n",
    " (19, 'house cleaning'),\n",
    " (20, 'playing soccer'),\n",
    " (24, 'rope jumping')]\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ft(data, model, device, bs=32):\n",
    "    ns, _, _ = data.shape \n",
    "    padded_mat = F.pad(input=data, pad=(0,0,0,0,0,bs-ns), mode='constant', value=0)\n",
    "    _, vector_out = model(padded_mat.float().to(device)) # batch second mode\n",
    "    action_feat_mat = vector_out[:ns, :].cpu().detach().numpy()\n",
    "    return action_feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32, device=device)\n",
    "prep_dir = '../tmp/random_input_100_epochs.pt'\n",
    "\n",
    "ae_model.load_state_dict(torch.load(prep_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = ae_model.to(device)\n",
    "action_ft_dict = {a: get_class_ft(torch.from_numpy(skeleton_mov[i, ...]), ae_model, device) for a,i in action_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpLoss =ActionTripletLoss(action_ft_dict, distance='cosine', k=10, pos_thr=0.90, neg_thr=0.05, agg=\"mean\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = torch.randn(8, 256)\n",
    "true_actions = [1, 4, 3, 6, 10, 5, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = trpLoss(pred_batch, true_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
