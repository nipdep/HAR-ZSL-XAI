{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init training pipeline with new models\n",
    "\n",
    "#### Sub-modules \n",
    "- [har-imu-transformer](https://github.com/yolish/har-with-imu-transformer/tree/main)\n",
    "- [sgn skeleton encoder](https://github.com/microsoft/SGN/tree/master)\n",
    "- custom skeleton decoder\n",
    "\n",
    "#### Training\n",
    "- train with matching and non-matching IMU and skeleton pairs\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nipun\\AppData\\Local\\Temp\\ipykernel_12232\\4166601334.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "from datetime import date, datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "# import neptune\n",
    "\n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n",
    "# from src.datasets.dataset import PAMAP2Dataset\n",
    "from src.utils.analysis import action_evaluator\n",
    "from src.datasets.utils import load_attribute\n",
    "\n",
    "from src.models.loss import FeatureLoss, AttributeLoss\n",
    "from src.utils.losses import *\n",
    "from src.utils.analysis import action_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'G:/FYP/Codebases/Pose-AE/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1 of 14\n",
      "Reading file 2 of 14\n",
      "Reading file 3 of 14\n",
      "Reading file 4 of 14\n",
      "Reading file 5 of 14\n",
      "Reading file 6 of 14\n",
      "Reading file 7 of 14\n",
      "Reading file 8 of 14\n",
      "Reading file 9 of 14\n",
      "Reading file 10 of 14\n",
      "Reading file 11 of 14\n",
      "Reading file 12 of 14\n",
      "Reading file 13 of 14\n",
      "Reading file 14 of 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\FYP\\Codebases\\DUET\\notebooks\\..\\src\\datasets\\data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"
     ]
    }
   ],
   "source": [
    "dataReader = PAMAP2ReaderV2(data_root+'/PAMAP2_Dataset/Protocol/')\n",
    "actionList = dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 60, 36)\n",
      "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
      "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
      "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
      "       'rope jumping', 'running', 'sitting', 'standing',\n",
      "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10], dtype=int64))\n",
      "[(1, 'lying'), (2, 'sitting'), (3, 'standing'), (4, 'walking'), (5, 'running'), (6, 'cycling'), (7, 'Nordic walking'), (9, 'watching TV'), (10, 'computer work'), (11, 'car driving'), (12, 'ascending stairs'), (13, 'descending stairs'), (16, 'vacuum cleaning'), (17, 'ironing'), (18, 'folding laundry'), (19, 'house cleaning'), (20, 'playing soccer'), (24, 'rope jumping')]\n",
      "(array(['Nordic walking', 'ascending stairs', 'car driving',\n",
      "       'computer work', 'cycling', 'descending stairs', 'folding laundry',\n",
      "       'house cleaning', 'ironing', 'lying', 'playing soccer',\n",
      "       'rope jumping', 'running', 'sitting', 'standing',\n",
      "       'vacuum cleaning', 'walking', 'watching TV'], dtype='<U17'), array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "skeleton_data = np.load(data_root+'/skeleton_k10_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\n",
    "\n",
    "print(skeleton_mov.shape)\n",
    "print(np.unique(skeleton_classes, return_counts=True))\n",
    "print(dataReader.label_map)\n",
    "print(np.unique(skeleton_classes, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 60, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeleton_mov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'lying',\n",
       " 1: 'sitting',\n",
       " 2: 'standing',\n",
       " 3: 'walking',\n",
       " 4: 'running',\n",
       " 5: 'cycling',\n",
       " 6: 'Nordic walking',\n",
       " 7: 'watching TV',\n",
       " 8: 'computer work',\n",
       " 9: 'car driving',\n",
       " 10: 'ascending stairs',\n",
       " 11: 'descending stairs',\n",
       " 12: 'vacuum cleaning',\n",
       " 13: 'ironing',\n",
       " 14: 'folding laundry',\n",
       " 15: 'house cleaning',\n",
       " 16: 'playing soccer',\n",
       " 17: 'rope jumping'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action2label = {i:j for i,j in enumerate(dataReader.idToLabel)}\n",
    "action2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lying',\n",
       " 'sitting',\n",
       " 'standing',\n",
       " 'walking',\n",
       " 'running',\n",
       " 'cycling',\n",
       " 'Nordic walking',\n",
       " 'watching TV',\n",
       " 'computer work',\n",
       " 'car driving',\n",
       " 'ascending stairs',\n",
       " 'descending stairs',\n",
       " 'vacuum cleaning',\n",
       " 'ironing',\n",
       " 'folding laundry',\n",
       " 'house cleaning',\n",
       " 'playing soccer',\n",
       " 'rope jumping']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataReader.idToLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[0,1,2,3,4,5], seen_ratio=0.8, unseen_ratio=0.1, window_size=12, window_overlap=10, resample_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, imu_data, imu_actions, skel_data, skel_label, action2label, active_cls, skel_len=120):\n",
    "        super(BaseDataset, self).__init__()\n",
    "        self.imu_data = torch.from_numpy(imu_data)\n",
    "        self.imu_actions = imu_actions\n",
    "        new_fts = [i for i in range(skel_data.shape[-1]) if i%3 != 2]\n",
    "        self.skel_data = torch.from_numpy(skel_data[:, :skel_len, new_fts])\n",
    "        self.skel_label = skel_label\n",
    "        self.action2label = action2label\n",
    "        # build action to id mapping dict\n",
    "        self.skel_len = skel_len\n",
    "        self.active_cls = active_cls\n",
    "        self.n_action = len(self.active_cls)\n",
    "        self.action2Id = dict(zip(active_cls, range(self.n_action)))\n",
    "        self.Id2action = dict(zip(range(self.n_action), active_cls))\n",
    "        self.__setup_skel_dict()\n",
    "    \n",
    "    def __setup_skel_dict(self):\n",
    "        action_dict = defaultdict(list)\n",
    "        for i, c in enumerate(self.skel_label):\n",
    "            if c in self.active_cls:\n",
    "                action_dict[c].append(self.skel_data[i])\n",
    "        \n",
    "        self.skel_dict = action_dict #{i: np.array(j) for i,j in action_dict.items()}\n",
    "\n",
    "    def __get_skel(self, anchor_id, flip):\n",
    "        if flip:\n",
    "            lbl_ind = anchor_id\n",
    "        else:\n",
    "            lbl_ind = random.randint(0, self.n_action)\n",
    "            # lbl_ind = self.n_action - anchor_id - 1  # has to update\n",
    "\n",
    "        lbl = self.Id2action[lbl_ind]\n",
    "        ind = random.randint(0, 10) # has to update\n",
    "        skel = self.skel_dict[lbl][ind]\n",
    "        return skel \n",
    "        \n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.imu_data[ind, ...]\n",
    "        anchor_action = self.imu_actions[ind]\n",
    "        anchor_label = self.action2label[anchor_action]\n",
    "        anchor_id = self.action2Id[anchor_label]\n",
    "        \n",
    "        y = random.randint(0, 2)\n",
    "        skel = self.__get_skel(anchor_id, y)\n",
    "        return x, y, skel\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imu_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_c = ['Nordic walking', 'watching TV', 'computer work', 'car driving', 'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n",
    "dataset = BaseDataset(imu_data=data_dict['train']['X'], imu_actions=data_dict['train']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=act_c, skel_len=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_dict['train']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =  DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 54]) torch.Size([32]) torch.Size([32, 60, 24])\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for xi,yi,skeli in dl:\n",
    "    print(xi.shape, yi.shape, skeli.shape)\n",
    "    print(yi)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnseenDataset(Dataset):\n",
    "    def __init__(self, imu_data, imu_actions, skel_data, skel_label, action2label, active_cls, skel_len=120):\n",
    "        super(UnseenDataset, self).__init__()\n",
    "        self.imu_data = torch.from_numpy(imu_data)\n",
    "        self.imu_actions = imu_actions\n",
    "        new_fts = [i for i in range(skel_data.shape[-1]) if i%3 != 2]\n",
    "        self.skel_data = skel_data[:, :skel_len, new_fts]\n",
    "        self.skel_label = skel_label\n",
    "        self.action2label = action2label\n",
    "        # build action to id mapping dict\n",
    "        self.skel_len = skel_len\n",
    "        self.active_cls = active_cls\n",
    "        self.n_action = len(self.active_cls)\n",
    "        self.action2Id = dict(zip(active_cls, range(self.n_action)))\n",
    "        self.Id2action = dict(zip(range(self.n_action), active_cls))\n",
    "        self.__setup_skel_dict()\n",
    "\n",
    "    def __setup_skel_dict(self):\n",
    "        action_dict = defaultdict(list)\n",
    "        for i, c in enumerate(self.skel_label):\n",
    "            if c in self.active_cls:\n",
    "                action_dict[c] = self.skel_data[i]\n",
    "        \n",
    "        self.skel_dict = action_dict\n",
    "        self.skel_anchor = torch.from_numpy(np.array([action_dict[i] for i in self.active_cls]))\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.imu_data[ind, ...]\n",
    "        anchor_action = self.imu_actions[ind]\n",
    "        anchor_label = self.action2label[anchor_action]\n",
    "        anchor_id = self.action2Id[anchor_label]\n",
    "        \n",
    "        return x, anchor_id, self.skel_anchor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imu_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_cls = ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling']\n",
    "unseen_dt = UnseenDataset(imu_data=data_dict['test']['X'], imu_actions=data_dict['test']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=unseen_cls, skel_len=60)\n",
    "\n",
    "unseen_dl =  DataLoader(unseen_dt, batch_size=32, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 54]) torch.Size([32]) torch.Size([32, 6, 60, 24])\n"
     ]
    }
   ],
   "source": [
    "for ux, uy, uskels in unseen_dl:\n",
    "    print(ux.shape, uy.shape, uskels.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.modeling_sgn import embed, local, gcn_spa, compute_g_spa\n",
    "from src.models.modeling_lxmert import LxmertConfig, LxmertXLayer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "from src.models.bidirectional_cross_attention import BidirectionalCrossAttention\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNEncoder(nn.Module):\n",
    "    def __init__(self, num_joint, seg, hidden_size=128, bs=32, is_3d=True, train=True, bias=True, device='cpu'):\n",
    "        super(SGNEncoder, self).__init__()\n",
    "\n",
    "        self.dim1 = hidden_size\n",
    "        self.dim_unit = hidden_size // 4 \n",
    "        self.seg = seg\n",
    "        self.num_joint = num_joint\n",
    "        self.bs = bs\n",
    "\n",
    "        if is_3d:\n",
    "          self.spatial_dim = 3\n",
    "        else:\n",
    "          self.spatial_dim = 2\n",
    "\n",
    "        if train:\n",
    "            self.spa = self.one_hot(bs, num_joint, self.seg)\n",
    "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
    "            self.tem = self.one_hot(bs, self.seg, num_joint)\n",
    "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
    "        else:\n",
    "            self.spa = self.one_hot(32 * 5, num_joint, self.seg)\n",
    "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
    "            self.tem = self.one_hot(32 * 5, self.seg, num_joint)\n",
    "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        self.tem_embed = embed(self.seg, joint=self.num_joint, hidden_dim=self.dim_unit*4, norm=False, bias=bias)\n",
    "        self.spa_embed = embed(num_joint, joint=self.num_joint, hidden_dim=self.dim_unit, norm=False, bias=bias)\n",
    "        self.joint_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n",
    "        self.dif_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n",
    "        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n",
    "        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
    "        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n",
    "        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
    "        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n",
    "        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n",
    "        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Dynamic Representation\n",
    "        x = x.view((self.bs, self.seg, self.num_joint, self.spatial_dim))\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        dif = x[:, :, :, 1:] - x[:, :, :, 0:-1]\n",
    "        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n",
    "        # print(x.shape)\n",
    "        pos = self.joint_embed(x)\n",
    "        tem1 = self.tem_embed(self.tem)\n",
    "        spa1 = self.spa_embed(self.spa)\n",
    "        dif = self.dif_embed(dif)\n",
    "        dy = torch.add(pos, dif)\n",
    "        # Joint-level Module\n",
    "        x= torch.cat([dy, spa1], 1)\n",
    "        g = self.compute_g1(x)\n",
    "        x = self.gcn1(x, g)\n",
    "        x = self.gcn2(x, g)\n",
    "        x = self.gcn3(x, g)\n",
    "        # Frame-level Module\n",
    "        x = torch.add(x, tem1)\n",
    "        x = self.cnn(x)\n",
    "        output_feat = torch.squeeze(x).permute(0,2,1).contiguous()\n",
    "\n",
    "        return output_feat\n",
    "\n",
    "    def one_hot(self, bs, spa, tem):\n",
    "\n",
    "        y = torch.arange(spa).unsqueeze(-1)\n",
    "        y_onehot = torch.FloatTensor(spa, spa)\n",
    "\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n",
    "        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n",
    "\n",
    "        return y_onehot\n",
    "\n",
    "    \n",
    "# class BiLSTMDecoder(nn.Module):\n",
    "#     def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True, device='cpu'):\n",
    "#         super(BiLSTMDecoder, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.device = device\n",
    "#         self.num_layers = num_layers\n",
    "#         self.linear_filters = linear_filters[::-1]\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.seq_len = seq_len\n",
    "\n",
    "#         # define LSTM layer\n",
    "#         self.layers = []\n",
    "#         # add lstm\n",
    "#         self.lstm = nn.LSTM(input_size = self.seq_len, hidden_size = self.hidden_size,\n",
    "#                             num_layers = self.num_layers, bidirectional=True,\n",
    "#                             batch_first=True)\n",
    "        \n",
    "#         self.maxpool = nn.AdaptiveMaxPool2d([self.seq_len, self.seq_len])\n",
    "#         # add linear layers \n",
    "#         if bidirectional:\n",
    "#             self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "#         else:\n",
    "#             self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "#         for __id,layer_in in enumerate(self.linear_filters):\n",
    "#             if __id == len(linear_filters)-1:\n",
    "#                 self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "#             else:\n",
    "#                 self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "#         self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "\n",
    "#     def forward(self,encoder_hidden):\n",
    "#         \"\"\"\n",
    "#         : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "#         : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "#         \"\"\"\n",
    "#         output = self.maxpool(encoder_hidden)\n",
    "#         lstm_out, _ = self.lstm(output)\n",
    "#         x = self.net(lstm_out)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1, bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True, proj_size = output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        # x = self.net(x_input)\n",
    "        out, _ = self.lstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUTransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: (dict) configuration of the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_dim = config.get(\"transformer_dim\")\n",
    "\n",
    "        self.input_proj = nn.Sequential(nn.Conv1d(config.get(\"input_dim\"), self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU(),\n",
    "                                        nn.Conv1d(self.transformer_dim, self.transformer_dim, 1), nn.GELU())\n",
    "\n",
    "        self.window_size = config.get(\"window_size\")\n",
    "        self.encode_position = config.get(\"encode_position\")\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = self.transformer_dim,\n",
    "                                       nhead = config.get(\"nhead\"),\n",
    "                                       dim_feedforward = config.get(\"dim_feedforward\"),\n",
    "                                       dropout = config.get(\"transformer_dropout\"),\n",
    "                                       activation = config.get(\"transformer_activation\"))\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer,\n",
    "                                              num_layers = config.get(\"num_encoder_layers\"),\n",
    "                                              norm = nn.LayerNorm(self.transformer_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros((1, self.transformer_dim)), requires_grad=True)\n",
    "\n",
    "        if self.encode_position:\n",
    "            self.position_embed = nn.Parameter(torch.randn(self.window_size + 1, 1, self.transformer_dim))\n",
    "\n",
    "        # num_classes =  config.get(\"num_classes\")\n",
    "        output_size = config.get(\"output_size\")\n",
    "        self.imu_head = nn.Sequential(\n",
    "            nn.AvgPool2d((self.window_size,1)),\n",
    "            nn.Linear(self.transformer_dim,  output_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(output_size, output_size)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        src = data  # Shape N x S x C with S = sequence length, N = batch size, C = channels\n",
    "        src = self.input_proj(src.transpose(1, 2)).permute(2, 0, 1)\n",
    "\n",
    "        cls_token = self.cls_token.unsqueeze(1).repeat(1, src.shape[1], 1)\n",
    "        src = torch.cat([cls_token, src])\n",
    "\n",
    "        if self.encode_position:\n",
    "            src += self.position_embed\n",
    "\n",
    "        target = self.transformer_encoder(src)\n",
    "        target = torch.squeeze(target.permute(1,0,2))\n",
    "        target = self.imu_head(target)\n",
    "        return target\n",
    "\n",
    "def get_activation(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    if activation == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    raise RuntimeError(\"Activation {} not supported\".format(activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEncoder(nn.Module):\n",
    "    def __init__(self, in_ft, d_model, ft_size, num_heads=1, max_len=1024, dropout=0.1):\n",
    "        super(IMUEncoder, self).__init__()\n",
    "        self.in_ft = in_ft\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ft_size = ft_size \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_ft,\n",
    "                            hidden_size=self.d_model,\n",
    "                            num_layers=self.num_heads,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n",
    "        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_forward = out[:, self.max_len - 1, :self.d_model]\n",
    "        out_reverse = out[:, 0, self.d_model:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        out = self.drop(out_reduced)\n",
    "        out = self.act(out)\n",
    "        out = self.fcLayer1(out)\n",
    "        out = torch.unsqueeze(out, 1)\n",
    "        # out = self.fcLayer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "  def __init__(self, embedding_size):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        #   self.maxpool = nn.AdaptiveMaxPool2d([embedding_size//2, 2])\n",
    "      #   self.hidden = nn.Linear(embedding_size, embedding_size)\n",
    "      #   self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(embedding_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # x = torch.squeeze(x)\n",
    "        x = self.sigmoid(x)\n",
    "        out = torch.squeeze(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.device = config['device']\n",
    "        self.imu_model = IMUEncoder(**config['imu_config'])\n",
    "        self.skel_encoder = SGNEncoder(**config['sgn_config'])\n",
    "        self.skel_decoder = BiLSTMDecoder(**config['dec_config'])\n",
    "        self.fc_head = ClassifierHead(config[\"ft_size\"])\n",
    "        self.lxmert_xlayer = BidirectionalCrossAttention(**config['xmert_config'])\n",
    "\n",
    "        self.imu_mask = torch.ones((config['bs'], config['imu_len']), requires_grad=True).bool().to(self.device)\n",
    "        self.skel_mask = torch.ones((config['bs'], config['skel_len']), requires_grad=True).bool().to(self.device)\n",
    "        \n",
    "\n",
    "    def forward(self, x_imu, x_skel):\n",
    "        imu_feats = self.imu_model(x_imu)\n",
    "        skel_feats = self.skel_encoder(x_skel)\n",
    "        print(f\"imu_feats {imu_feats.shape} | skel_feats {skel_feats.shape}\")\n",
    "\n",
    "        imu_feats, skel_feats = self.lxmert_xlayer(imu_feats, skel_feats, mask=self.imu_mask, context_mask=self.skel_mask)\n",
    "        # print(f\"imu_feats {imu_feats.shape} | skel_feats {skel_feats.shape}\")\n",
    "        skel_recon = self.skel_decoder(skel_feats)\n",
    "        bin_output = self.fc_head(imu_feats[:, 0, :])\n",
    "        return bin_output, skel_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_config = {\n",
    "\t\"input_dim\": 54,\n",
    "    \"window_size\":24,\n",
    "\t\"encode_position\":True,\n",
    "\t\"transformer_dim\": 256,\n",
    "\t\"nhead\": 8,\n",
    "\t\"num_encoder_layers\": 6, \n",
    "\t\"dim_feedforward\": 128, \n",
    "\t\"transformer_dropout\": 0.1, \n",
    "\t\"transformer_activation\": \"gelu\",\n",
    "\t\"head_activation\": \"gelu\",\n",
    "    \"baseline_dropout\": 0.1,\n",
    "\t\"batch_size\": 32,\n",
    "\t\"output_size\": 512\n",
    "}\n",
    "\n",
    "imuenc_config = {\n",
    "    'in_ft': 54,\n",
    "    'd_model': 256,\n",
    "    'ft_size': 512,\n",
    "    'num_heads': 2,\n",
    "    'max_len': 24,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "sgn_config = {\n",
    "    'num_joint': 12,\n",
    "    'seg': 60,\n",
    "    'hidden_size': 256,\n",
    "    'train': True,\n",
    "    'bs': 32,\n",
    "    'is_3d': False\n",
    "}\n",
    "\n",
    "# dec_config = {\n",
    "#     'seq_len': 60,\n",
    "#     'input_size': 24,\n",
    "#     'hidden_size': 256,\n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': True,\n",
    "#     \"embedding_size\": 128,\n",
    "#     \"linear_filters\":[128,256,512,1024],\n",
    "# }\n",
    "\n",
    "dec_config = {\n",
    "    'input_size': 512,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'output_size': 24,\n",
    "    'bidirectional': False\n",
    "}\n",
    "\n",
    "base_config = {\n",
    "    'imu_config': imuenc_config,\n",
    "    'sgn_config': sgn_config,\n",
    "    'dec_config': dec_config,\n",
    "    'device': 'cpu',\n",
    "    'bs': 32,\n",
    "    'imu_len': 1,\n",
    "    'skel_len': 60,\n",
    "    'ft_size': 512,\n",
    "    'xmert_config': {\n",
    "            'dim': 512,\n",
    "            'heads': 8,\n",
    "            'dim_head': 64,\n",
    "            'context_dim': 512,   \n",
    "            \n",
    "    }\n",
    "}\n",
    " \n",
    "base_model = BaseModel(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imu_feats torch.Size([32, 1, 512]) | skel_feats torch.Size([32, 60, 512])\n"
     ]
    }
   ],
   "source": [
    "imu_output, skel_recon = base_model(xi.float().requires_grad_(), skeli.float().requires_grad_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32, 60, 24])\n"
     ]
    }
   ],
   "source": [
    "print(imu_output.shape, skel_recon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(low=0, high=2, size=(32,)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clossfunc = nn.BCELoss()\n",
    "rlossfunc = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "closs = clossfunc(imu_output, yi.float())\n",
    "rloss = rlossfunc(skeli, skel_recon)\n",
    "loss = closs + rloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6963, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3737, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            try:\n",
    "                ave_grads.append(p.grad.abs().mean())\n",
    "            except:\n",
    "                print(n, p.grad)\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_grad_flow(named_parameters):\n",
    "#     ave_grads = []\n",
    "#     layers = []\n",
    "#     for n, p in named_parameters:\n",
    "#         if(p.requires_grad) and (\"bias\" not in n):\n",
    "#             layers.append(n)\n",
    "#             ave_grads.append(p.grad.abs().mean())\n",
    "\n",
    "#     plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "#     plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "#     plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "#     plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "#     plt.xlabel(\"Layers\")\n",
    "#     plt.ylabel(\"average gradient\")\n",
    "#     plt.title(\"Gradient flow\")\n",
    "#     plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grad_flow(base_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dl, optimizer, loss_modules, device, phase='train', loss_alpha=0.7):\n",
    "    model = model.train()\n",
    "\n",
    "    with tqdm(dl, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skel = batch \n",
    "            X = X.float().to(device)\n",
    "            print(\"y >>> \", y)\n",
    "            y = y.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "            # print(f\"X : {X.shape} | skel : {skel.shape}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                cls_prob, skel_recon = model(X, skel)\n",
    "                # print(\"cls_prob : \", cls_prob.shape, \" skel_recon :\", skel_recon.shape)\n",
    "                class_loss = loss_modules['classi'](y, cls_prob)\n",
    "                recon_loss = loss_modules['recon'](skel, skel_recon)\n",
    "\n",
    "            # print(f\" Check for nan | X : {torch.isnan(X).sum()} | X-inf : {torch.isinf(X).sum()} | skel : {torch.isnan(skel).sum()} | skel-inf : {torch.isinf(skel).sum()} | cls_prob : {torch.isnan(cls_prob).sum()} | skel_recon : {torch.isnan(skel_recon).sum()}\")\n",
    "            print(\"class loss : \", class_loss.item(), \" recon_loss : \", recon_loss.item())\n",
    "            loss = (1-loss_alpha)*class_loss + loss_alpha*recon_loss\n",
    "            # loss = class_loss\n",
    "\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                print(name, param.grad.max())\n",
    "\n",
    "            metrics = {\"loss\": loss.item()}\n",
    "            # pred_class = cls_prob.cpu().detach().numpy()\n",
    "            y_pred = (cls_prob.cpu().detach().numpy() >= 0.5).astype(int)\n",
    "            # print(\"y_true : \", y.cpu().detach().numpy().astype(int), \" y_pred : \", pred_class)\n",
    "            metrics[\"accuracy\"] = accuracy_score(y_true=y.cpu().detach().numpy().astype(int), y_pred=y_pred)\n",
    "        return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(model, dl, optimizer, loss_modules, device, class_names, phase='train', loss_alpha=0.7, print_report=False, show_plot=False):\n",
    "    model = model.eval()\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"recon. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(dl, unit='batch', desc=phase) as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skel = batch \n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            skel = skel.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                cls_prob, skel_recon = model(X, skel)\n",
    "                class_loss = loss_modules['classi'](y, cls_prob)\n",
    "                recon_loss = loss_modules['recon'](skel, skel_recon)\n",
    "\n",
    "            loss = loss_alpha*class_loss + recon_loss\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metrics['samples'] += len(y)\n",
    "                metrics['loss'] += loss.item()  # add total loss of batch\n",
    "                metrics['recon. loss'] += recon_loss.item()\n",
    "                metrics['classi. loss'] += class_loss.item()\n",
    "\n",
    "            y_pred = (cls_prob.cpu().detach().numpy() >= 0.5).astype(int)\n",
    "            per_batch['targets'].append(y.cpu().numpy())\n",
    "            per_batch['predictions'].append(y_pred)\n",
    "            per_batch['metrics'].append([loss.detach().cpu().numpy()])\n",
    "\n",
    "            tepoch.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets.astype(int), class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "    metrics_dict.update(metrics)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unseen_eval(model, unseen_dl, device, class_names, print_report=True, show_plot=True):\n",
    "    model = model.eval()\n",
    "    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': [], 'feat': []}\n",
    "    metrics = {\"samples\": 0, \"loss\": 0, \"recon. loss\": 0, \"classi. loss\": 0}\n",
    "\n",
    "    with tqdm(unseen_dl, unit='batch') as tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y, skels = batch \n",
    "            X = X.float().to(device)\n",
    "            y = y.float()\n",
    "            skels = skels.float().to(device)\n",
    "\n",
    "        skels = skels.permute(1,0,2,3)\n",
    "        preds = []\n",
    "        for i in range(len(class_names)):\n",
    "            with torch.no_grad():\n",
    "                cls_prob, skel_recon = model(X, skels[i, ...])\n",
    "                preds.append(cls_prob.cpu().detach().numpy())\n",
    "            \n",
    "        preds = np.array(preds)\n",
    "        print(\"preds shape : \", preds.shape)\n",
    "        per_batch['targets'].append(y.cpu().numpy())\n",
    "        per_batch['predictions'].append(preds)\n",
    "\n",
    "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
    "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
    "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(df):\n",
    "    df['loss'] = df['loss']/df['samples']\n",
    "    df['recon. loss'] = df['recon. loss']/df['samples']\n",
    "    df['classi. loss'] = df['classi. loss']/df['samples']\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=4)\n",
    "    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='recon. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Reconstruction Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n",
    "    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"datetime\": date.today(),\n",
    "    \"device\": \"gpu\",\n",
    "    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n",
    "    \"Model\": \"Transfomer-Encder\",\n",
    "    \"sem-space\": 'I3D',\n",
    "    # model training configs\n",
    "    \"lr\": 0.0001,\n",
    "    \"loss_alpha\": 0.9,\n",
    "    \"n_epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    # model configs\n",
    "    \"feat_size\": 512, # skel-AE hidden size and IMU-Anc output size\n",
    "    # dataset configs\n",
    "    \"window_size\": 12, \n",
    "    \"overlap\": 10,\n",
    "    \"freq\": 50,\n",
    "    \"seq_len\": 60,  # skeleton seq. length\n",
    "    \"seen_split\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataReader.generate(unseen_classes=[1,2], seen_ratio=config['seen_split'], unseen_ratio=0.8, window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['freq'])\n",
    "dl =  DataLoader(dataset, batch_size=8, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for x,y,skel in dl:\n",
    "    _, seq_len, imu_feat = x.shape \n",
    "    _, timestamps, joints = skel.shape\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_config = {\n",
    "\t\"input_dim\": imu_feat, # \n",
    "    \"window_size\": seq_len, # \n",
    "\t\"encode_position\":True,\n",
    "\t\"transformer_dim\": 256, ##\n",
    "\t\"nhead\": 8,\n",
    "\t\"num_encoder_layers\": 6, \n",
    "\t\"dim_feedforward\": 128, \n",
    "\t\"transformer_dropout\": 0.1, \n",
    "\t\"transformer_activation\": \"gelu\",\n",
    "\t\"head_activation\": \"gelu\",\n",
    "    \"baseline_dropout\": 0.1,\n",
    "\t\"batch_size\": 32,\n",
    "\t\"output_size\": config['feat_size'] ##\n",
    "}\n",
    "\n",
    "imuenc_config = {\n",
    "    'in_ft': imu_feat,\n",
    "    'd_model': 256,\n",
    "    'ft_size': config['feat_size'],\n",
    "    'num_heads': 2,\n",
    "    'max_len': seq_len,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "sgn_config = {\n",
    "    'num_joint': joints//2, #\n",
    "    'seg': timestamps, #\n",
    "    'hidden_size': config['feat_size']//2, \n",
    "    'train': True,\n",
    "    'bs': config['batch_size'], ##\n",
    "    'is_3d': False,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# dec_config = {\n",
    "#     'seq_len': timestamps, #\n",
    "#     'input_size': joints, #\n",
    "#     'hidden_size': 256, \n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': True,\n",
    "#     \"embedding_size\": 128, ##\n",
    "#     \"linear_filters\":[128,256,512,1024],\n",
    "# }\n",
    "\n",
    "dec_config = {\n",
    "    'input_size': config['feat_size'],\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'output_size': joints,\n",
    "    'bidirectional': False\n",
    "}\n",
    "\n",
    "\n",
    "base_config = {\n",
    "    'imu_config': imuenc_config,\n",
    "    'sgn_config': sgn_config,\n",
    "    'dec_config': dec_config,\n",
    "    'num_layers': 1,\n",
    "    'device': device,\n",
    "    'bs': config['batch_size'],\n",
    "    'imu_len': 1,\n",
    "    'skel_len': timestamps,\n",
    "    'ft_size': config['feat_size'],\n",
    "    'xmert_config': {\n",
    "        'dim': config['feat_size'],\n",
    "        'heads': 8,\n",
    "        'dim_head': 64,\n",
    "        'context_dim': config['feat_size']#\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(fold, phase, metrics):\n",
    "    for m, v in metrics.items():\n",
    "        if fold == 'global':\n",
    "            run[f'global/{m}'].log(v)\n",
    "        else:\n",
    "            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5-fold running\n",
    "fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n",
    "\n",
    "fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Fold-0 ================\n",
      "Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "seen classes >  ['lying', 'sitting', 'walking', 'running', 'cycling', 'Nordic walking', 'computer work', 'car driving', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'playing soccer', 'rope jumping']\n",
      "unseen classes >  ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n",
      "Initiate IMU datasets ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y >>>  tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0])\n",
      "imu_feats torch.Size([32, 1, 512]) | skel_feats torch.Size([32, 60, 512])\n",
      "class loss :  0.4963518977165222  recon_loss :  0.5330567955970764\n",
      "imu_model.lstm.weight_ih_l0 tensor(0.0003, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0 tensor(1.4014e-05, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l0 tensor(4.3542e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l0 tensor(4.3542e-06, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l0_reverse tensor(0.0002, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0_reverse tensor(7.4555e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l0_reverse tensor(4.6303e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l0_reverse tensor(4.6303e-06, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l1 tensor(1.9250e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l1 tensor(7.0168e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l1 tensor(1.8026e-05, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l1 tensor(1.8026e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l1_reverse tensor(2.4362e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l1_reverse tensor(9.8078e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l1_reverse tensor(1.5226e-05, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l1_reverse tensor(1.5226e-05, device='cuda:0')\n",
      "imu_model.fcLayer1.weight tensor(3.5698e-05, device='cuda:0')\n",
      "imu_model.fcLayer1.bias tensor(9.2654e-05, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skel_encoder.tem_embed.cnn.0.cnn.weight tensor(8.8040e-06, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.0.cnn.bias tensor(2.6170e-05, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.2.cnn.weight tensor(7.9845e-06, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.2.cnn.bias tensor(2.0065e-05, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.0.cnn.weight tensor(0.0008, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.0.cnn.bias tensor(0.0012, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.2.cnn.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.2.cnn.bias tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.0.bn.weight tensor(0.0007, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.0.bn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.1.cnn.weight tensor(0.0023, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.1.cnn.bias tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.3.cnn.weight tensor(0.0015, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.3.cnn.bias tensor(0.0014, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.0.bn.weight tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.0.bn.bias tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.1.cnn.weight tensor(0.0013, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.1.cnn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.3.cnn.weight tensor(0.0006, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.3.cnn.bias tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.cnn.cnn1.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.cnn.cnn1.bias tensor(3.7971e-11, device='cuda:0')\n",
      "skel_encoder.cnn.bn1.weight tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.cnn.bn1.bias tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.cnn.cnn2.weight tensor(0.0006, device='cuda:0')\n",
      "skel_encoder.cnn.cnn2.bias tensor(1.1687e-10, device='cuda:0')\n",
      "skel_encoder.cnn.bn2.weight tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.cnn.bn2.bias tensor(0.0007, device='cuda:0')\n",
      "skel_encoder.compute_g1.g1.cnn.weight tensor(0., device='cuda:0')\n",
      "skel_encoder.compute_g1.g1.cnn.bias tensor(0., device='cuda:0')\n",
      "skel_encoder.compute_g1.g2.cnn.weight tensor(0., device='cuda:0')\n",
      "skel_encoder.compute_g1.g2.cnn.bias tensor(0., device='cuda:0')\n",
      "skel_encoder.gcn1.bn.weight tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.gcn1.bn.bias tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.gcn1.w.cnn.weight tensor(0.0011, device='cuda:0')\n",
      "skel_encoder.gcn1.w1.cnn.weight tensor(0.0013, device='cuda:0')\n",
      "skel_encoder.gcn1.w1.cnn.bias tensor(4.9840e-10, device='cuda:0')\n",
      "skel_encoder.gcn2.bn.weight tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.gcn2.bn.bias tensor(0.0001, device='cuda:0')\n",
      "skel_encoder.gcn2.w.cnn.weight tensor(0.0010, device='cuda:0')\n",
      "skel_encoder.gcn2.w1.cnn.weight tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.gcn2.w1.cnn.bias tensor(8.4128e-11, device='cuda:0')\n",
      "skel_encoder.gcn3.bn.weight tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.gcn3.bn.bias tensor(6.7438e-05, device='cuda:0')\n",
      "skel_encoder.gcn3.w.cnn.weight tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.gcn3.w1.cnn.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.gcn3.w1.cnn.bias tensor(3.1832e-11, device='cuda:0')\n",
      "skel_decoder.lstm.weight_ih_l0 tensor(2.8041e-05, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hh_l0 tensor(1.3396e-05, device='cuda:0')\n",
      "skel_decoder.lstm.bias_ih_l0 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.bias_hh_l0 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hr_l0 tensor(0.0003, device='cuda:0')\n",
      "skel_decoder.lstm.weight_ih_l1 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hh_l1 tensor(0.0003, device='cuda:0')\n",
      "skel_decoder.lstm.bias_ih_l1 tensor(0.0096, device='cuda:0')\n",
      "skel_decoder.lstm.bias_hh_l1 tensor(0.0096, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hr_l1 tensor(0.0022, device='cuda:0')\n",
      "fc_head.linear.weight tensor(0.0145, device='cuda:0')\n",
      "fc_head.linear.bias tensor(-0.0500, device='cuda:0')\n",
      "lxmert_xlayer.to_qk.weight tensor(1.7638e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_qk.weight tensor(1.3430e-05, device='cuda:0')\n",
      "lxmert_xlayer.to_v.weight tensor(2.6525e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_v.weight tensor(0.0012, device='cuda:0')\n",
      "lxmert_xlayer.to_out.weight tensor(0.0012, device='cuda:0')\n",
      "lxmert_xlayer.to_out.bias tensor(0.0022, device='cuda:0')\n",
      "lxmert_xlayer.context_to_out.weight tensor(2.3001e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_out.bias tensor(0.0002, device='cuda:0')\n",
      "y >>>  tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0])\n",
      "imu_feats torch.Size([32, 1, 512]) | skel_feats torch.Size([32, 60, 512])\n",
      "class loss :  0.42333561182022095  recon_loss :  0.5247222781181335\n",
      "imu_model.lstm.weight_ih_l0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0 tensor(9.3437e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l0 tensor(2.7893e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l0 tensor(2.7893e-06, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l0_reverse tensor(0.0002, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0_reverse tensor(5.0853e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l0_reverse tensor(5.4549e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l0_reverse tensor(5.4549e-06, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l1 tensor(2.0006e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l1 tensor(8.3831e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l1 tensor(1.6392e-05, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l1 tensor(1.6392e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l1_reverse tensor(1.9276e-05, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l1_reverse tensor(6.9342e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l1_reverse tensor(1.6751e-05, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l1_reverse tensor(1.6751e-05, device='cuda:0')\n",
      "imu_model.fcLayer1.weight tensor(3.2789e-05, device='cuda:0')\n",
      "imu_model.fcLayer1.bias tensor(0.0001, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.0.cnn.weight tensor(8.7198e-06, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.0.cnn.bias tensor(2.0313e-05, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.2.cnn.weight tensor(9.1055e-06, device='cuda:0')\n",
      "skel_encoder.tem_embed.cnn.2.cnn.bias tensor(1.8197e-05, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.0.cnn.weight tensor(0.0011, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.0.cnn.bias tensor(0.0012, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.2.cnn.weight tensor(0.0006, device='cuda:0')\n",
      "skel_encoder.spa_embed.cnn.2.cnn.bias tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.0.bn.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.0.bn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.1.cnn.weight tensor(0.0030, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.1.cnn.bias tensor(0.0014, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.3.cnn.weight tensor(0.0016, device='cuda:0')\n",
      "skel_encoder.joint_embed.cnn.3.cnn.bias tensor(0.0017, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.0.bn.weight tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.0.bn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.1.cnn.weight tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.1.cnn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.3.cnn.weight tensor(0.0006, device='cuda:0')\n",
      "skel_encoder.dif_embed.cnn.3.cnn.bias tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.cnn.cnn1.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.cnn.cnn1.bias tensor(3.8199e-11, device='cuda:0')\n",
      "skel_encoder.cnn.bn1.weight tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.cnn.bn1.bias tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.cnn.cnn2.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.cnn.cnn2.bias tensor(6.5484e-11, device='cuda:0')\n",
      "skel_encoder.cnn.bn2.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.cnn.bn2.bias tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.compute_g1.g1.cnn.weight tensor(1.5765e-05, device='cuda:0')\n",
      "skel_encoder.compute_g1.g1.cnn.bias tensor(1.3206e-05, device='cuda:0')\n",
      "skel_encoder.compute_g1.g2.cnn.weight tensor(1.7104e-05, device='cuda:0')\n",
      "skel_encoder.compute_g1.g2.cnn.bias tensor(6.5370e-13, device='cuda:0')\n",
      "skel_encoder.gcn1.bn.weight tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.gcn1.bn.bias tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.gcn1.w.cnn.weight tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.gcn1.w1.cnn.weight tensor(0.0013, device='cuda:0')\n",
      "skel_encoder.gcn1.w1.cnn.bias tensor(8.2946e-10, device='cuda:0')\n",
      "skel_encoder.gcn2.bn.weight tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.gcn2.bn.bias tensor(0.0002, device='cuda:0')\n",
      "skel_encoder.gcn2.w.cnn.weight tensor(0.0007, device='cuda:0')\n",
      "skel_encoder.gcn2.w1.cnn.weight tensor(0.0009, device='cuda:0')\n",
      "skel_encoder.gcn2.w1.cnn.bias tensor(1.4552e-10, device='cuda:0')\n",
      "skel_encoder.gcn3.bn.weight tensor(0.0003, device='cuda:0')\n",
      "skel_encoder.gcn3.bn.bias tensor(0.0001, device='cuda:0')\n",
      "skel_encoder.gcn3.w.cnn.weight tensor(0.0004, device='cuda:0')\n",
      "skel_encoder.gcn3.w1.cnn.weight tensor(0.0005, device='cuda:0')\n",
      "skel_encoder.gcn3.w1.cnn.bias tensor(5.2751e-11, device='cuda:0')\n",
      "skel_decoder.lstm.weight_ih_l0 tensor(2.6808e-05, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hh_l0 tensor(1.2647e-05, device='cuda:0')\n",
      "skel_decoder.lstm.bias_ih_l0 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.bias_hh_l0 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hr_l0 tensor(0.0003, device='cuda:0')\n",
      "skel_decoder.lstm.weight_ih_l1 tensor(0.0004, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hh_l1 tensor(0.0003, device='cuda:0')\n",
      "skel_decoder.lstm.bias_ih_l1 tensor(0.0094, device='cuda:0')\n",
      "skel_decoder.lstm.bias_hh_l1 tensor(0.0094, device='cuda:0')\n",
      "skel_decoder.lstm.weight_hr_l1 tensor(0.0022, device='cuda:0')\n",
      "fc_head.linear.weight tensor(0.0083, device='cuda:0')\n",
      "fc_head.linear.bias tensor(0.0188, device='cuda:0')\n",
      "lxmert_xlayer.to_qk.weight tensor(1.7987e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_qk.weight tensor(1.6984e-05, device='cuda:0')\n",
      "lxmert_xlayer.to_v.weight tensor(2.4454e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_v.weight tensor(0.0008, device='cuda:0')\n",
      "lxmert_xlayer.to_out.weight tensor(0.0007, device='cuda:0')\n",
      "lxmert_xlayer.to_out.bias tensor(0.0008, device='cuda:0')\n",
      "lxmert_xlayer.context_to_out.weight tensor(2.0229e-05, device='cuda:0')\n",
      "lxmert_xlayer.context_to_out.bias tensor(0.0002, device='cuda:0')\n",
      "y >>>  tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0])\n",
      "imu_feats torch.Size([32, 1, 512]) | skel_feats torch.Size([32, 60, 512])\n",
      "class loss :  0.4215037226676941  recon_loss :  0.5364835262298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   1%|          | 2/284 [00:00<01:45,  2.68batch/s]\n",
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imu_model.lstm.weight_ih_l0 tensor(0.0001, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0 tensor(3.2440e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_ih_l0 tensor(1.9484e-06, device='cuda:0')\n",
      "imu_model.lstm.bias_hh_l0 tensor(1.9484e-06, device='cuda:0')\n",
      "imu_model.lstm.weight_ih_l0_reverse tensor(0.0005, device='cuda:0')\n",
      "imu_model.lstm.weight_hh_l0_reverse "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m train_data \u001b[39m=\u001b[39m []\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m]), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epoch\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 40\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_step(model, train_dl, optimizer, loss_modules, device, phase\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, loss_alpha\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mloss_alpha\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     41\u001b[0m     train_metrics[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m epoch\n\u001b[0;32m     42\u001b[0m     train_metrics[\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[124], line 30\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dl, optimizer, loss_modules, device, phase, loss_alpha)\u001b[0m\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mnamed_parameters():\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mprint\u001b[39;49m(name, param\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mmax())\n\u001b[0;32m     32\u001b[0m metrics \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: loss\u001b[39m.\u001b[39mitem()}\n\u001b[0;32m     33\u001b[0m \u001b[39m# pred_class = cls_prob.cpu().detach().numpy()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    423\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    635\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    565\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    566\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[0;32m    570\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    324\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    325\u001b[0m     )\n\u001b[0;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\nipun\\anaconda3\\envs\\fyp_env\\lib\\site-packages\\torch\\_tensor_str.py:116\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[0;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(\n\u001b[1;32m--> 116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39m tensor_view\u001b[39m.\u001b[39mne(\u001b[39m0\u001b[39m)\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run['parameters'] = config\n",
    "fold_metric_scores = []\n",
    "\n",
    "for i, cs in enumerate(fold_cls_ids):\n",
    "    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n",
    "    print(f'Unseen Classes : {fold_classes[i]}')\n",
    "\n",
    "    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=0.8, window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['freq'])\n",
    "    all_classes = dataReader.idToLabel\n",
    "    seen_classes = [all_classes[i] for i in data_dict['seen_classes']]\n",
    "    unseen_classes = [all_classes[i] for i in data_dict['unseen_classes']]\n",
    "    print(\"seen classes > \", seen_classes)\n",
    "    print(\"unseen classes > \", unseen_classes)\n",
    "    # train_n, seq_len, in_ft = data_dict['train']['X'].shape\n",
    "\n",
    "    print(\"Initiate IMU datasets ...\")\n",
    "    # build IMU datasets\n",
    "    train_dt = BaseDataset(imu_data=data_dict['train']['X'], imu_actions=data_dict['train']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=seen_classes, skel_len=timestamps)\n",
    "    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build seen eval_dt\n",
    "    eval_dt = BaseDataset(imu_data=data_dict['eval-seen']['X'], imu_actions=data_dict['eval-seen']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=seen_classes, skel_len=timestamps)\n",
    "    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    # build unseen test_dt\n",
    "    test_dt = UnseenDataset(imu_data=data_dict['test']['X'], imu_actions=data_dict['test']['y'], skel_data=skeleton_mov, skel_label=skeleton_classes, action2label=action2label, active_cls=unseen_classes, skel_len=timestamps)\n",
    "    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    # build model\n",
    "    model = BaseModel(base_config)\n",
    "    model.to(device)\n",
    "\n",
    "    # define run parameters \n",
    "    optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "    loss_modules = {'classi': nn.L1Loss(), 'recon': nn.L1Loss()}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # train the model \n",
    "    train_data = []\n",
    "    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n",
    "    \n",
    "        train_metrics = train_step(model, train_dl, optimizer, loss_modules, device, phase='train', loss_alpha=config['loss_alpha'])\n",
    "        train_metrics['epoch'] = epoch\n",
    "        train_metrics['phase'] = 'train'\n",
    "        train_data.append(train_metrics)\n",
    "        # log(i, 'train', train_metrics)\n",
    "\n",
    "        eval_metrics = validate_step(model, eval_dl, optimizer, loss_modules, device, seen_classes, phase='train', loss_alpha=config['loss_alpha'], print_report=False, show_plot=False)\n",
    "        eval_metrics['epoch'] = epoch \n",
    "        eval_metrics['phase'] = 'valid'\n",
    "        train_data.append(eval_metrics)\n",
    "        # log(i, 'valid', train_metrics)\n",
    "        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n",
    "        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n",
    "        if eval_metrics['accuracy'] > best_acc:\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    train_df = pd.DataFrame().from_records(train_data)\n",
    "    plot_curves(train_df)\n",
    "\n",
    "    # replace by best model \n",
    "    model.load_state_dict(best_model)\n",
    "    # save_model(model,notebook_iden,model_iden,i)\n",
    "\n",
    "    # run evaluation on unseen classes\n",
    "    test_metrics = unseen_eval(model, test_dl, device, unseen_classes, print_report=True, show_plot=True)\n",
    "    fold_metric_scores.append(test_metrics)\n",
    "    # log('test', i, test_metrics)\n",
    "    print(test_metrics)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n",
    "seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n",
    "print(seen_score_df.mean())\n",
    "# log('global', '',seen_score_df.mean().to_dict())\n",
    "# run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
