{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out.squeeze(0))     \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, seq_len, hidden_size, batch_size, ae_type='recursive', teacher_forcing_ratio=0.5):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size \n",
    "        self.bs = batch_size\n",
    "        self.ae_type = ae_type # ['recursive', 'teacher_forcing', 'mixed_teacher_forcing']\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.encoder = LSTMEncoder(input_size = input_size, hidden_size = hidden_size)\n",
    "        self.decoder = LSTMDecoder(input_size = input_size, hidden_size = hidden_size)\n",
    "\n",
    "        self.encoder_hidden = self.encoder.init_hidden(self.bs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding \n",
    "        encoder_output, self.encoder_hidden = self.encoder(x)\n",
    "        # decoding \n",
    "        decoder_input = torch.rand((self.bs, self.input_size), requires_grad=True)#self.encoder_hidden[0].squeeze()\n",
    "        print(decoder_input.shape)\n",
    "        decoder_hidden = self.encoder_hidden\n",
    "        # outputs tensor\n",
    "        outputs = torch.zeros(self.seq_len, self.bs, self.input_size)\n",
    "\n",
    "        if self.ae_type == 'recursive':\n",
    "            for t in range(self.seq_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "        \n",
    "        elif self.ae_type == 'teacher_forcing':\n",
    "            if random.random() < self.teacher_forcing_ratio:\n",
    "                for t in range(self.seq_len):\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = x[t, :, :]\n",
    "\n",
    "            else:\n",
    "                for t in range(self.seq_len):\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "        elif self.ae_type == 'mixed_teacher_forcing':\n",
    "            for t in range(self.seq_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "\n",
    "                if random.random() < self.teacher_forcing_ratio:\n",
    "                    decoder_input = x[t, :, :]\n",
    "                else:\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 42])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMAE(input_size=42, seq_len=120, hidden_size=128, batch_size=32, ae_type='recursive')\n",
    "sample_input = torch.rand((120, 32, 42))\n",
    "sample_output = model(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, optimizer, loss_module, device, phase='train'):\n",
    "    model = model.train()\n",
    "\n",
    "    epoch_loss = 0 \n",
    "    total_samples = 0 \n",
    "\n",
    "    with tqdm(dataloader, unit='batch') as  tepoch:\n",
    "        for batch in tepoch:\n",
    "            X, y = batch \n",
    "\n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                seq_output = model(X)\n",
    "\n",
    "            rec_loss = loss_module['recons'](seq_output, y)\n",
    "\n",
    "            if phase == 'train':\n",
    "                rec_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics = {'loss': rec_loss.item()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_samples += len(y)\n",
    "                epoch_loss += rec_loss.item() \n",
    "            tepoch.set_postfix(metrics)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
    "    return metrics   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
