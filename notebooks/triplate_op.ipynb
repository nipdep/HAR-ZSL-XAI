{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.concat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,batch_size=32, device='cpu'):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((self.batch_size,4,self.hidden_size))\n",
    "            # print(hidden.shape)\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((self.batch_size,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "        \n",
    "        dummy_input = torch.rand((self.batch_size,self.seq_len,self.hidden_size), requires_grad=True)\n",
    "        dummy_input = dummy_input.to(self.device)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True, batch_size=32, device='cpu'):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True,batch_size=32, device=device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model = BiLSTMEncDecModel(seq_len=50, input_size=36, hidden_size=512, linear_filters=[128,256,512], embedding_size=256, num_layers=1,bidirectional=True,batch_size=32, device=device)\n",
    "prep_dir = '../tmp/random_input_100_epochs.pt'\n",
    "\n",
    "ae_model.load_state_dict(torch.load(prep_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_data = np.load('../data/skeleton_k10_v7_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_map = [(1, 'lying'),\n",
    " (2, 'sitting'),\n",
    " (3, 'standing'),\n",
    " (4, 'walking'),\n",
    " (5, 'running'),\n",
    " (6, 'cycling'),\n",
    " (7, 'Nordic walking'),\n",
    " (9, 'watching TV'),\n",
    " (10, 'computer work'),\n",
    " (11, 'car driving'),\n",
    " (12, 'ascending stairs'),\n",
    " (13, 'descending stairs'),\n",
    " (16, 'vacuum cleaning'),\n",
    " (17, 'ironing'),\n",
    " (18, 'folding laundry'),\n",
    " (19, 'house cleaning'),\n",
    " (20, 'playing soccer'),\n",
    " (24, 'rope jumping')]\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "             9: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             8: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "             5: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "             11: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "             14: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "             15: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "             13: [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "             0: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "             6: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "             16: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "             17: [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "             4: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "             1: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "             2: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "             12: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "             3: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "             7: [170, 171, 172, 173, 174, 175, 176, 177, 178, 179]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ft(data, model, device, bs=32):\n",
    "    ns, _, _ = data.shape \n",
    "    padded_mat = F.pad(input=data, pad=(0,0,0,0,0,bs-ns), mode='constant', value=0)\n",
    "    _, vector_out = model(padded_mat.float().to(device)) # batch second mode\n",
    "    action_feat_mat = vector_out[:ns, :].cpu().detach().numpy()\n",
    "    return action_feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = ae_model.to(device)\n",
    "\n",
    "action_ft_dict = {a: get_class_ft(torch.from_numpy(skeleton_mov[i, ...]), ae_model, device) for a,i in action_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_ft_dict[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_sampling(label, k=10):\n",
    "    total_samples = action_ft_dict[label]\n",
    "    n, _ = total_samples.shape \n",
    "    sample_idx = sample(range(n), k)\n",
    "    samples = total_samples[sample_idx, :]\n",
    "    return torch.from_numpy(samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sampling(10, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(label, k=10):\n",
    "    neg_action = set(action_ft_dict.keys())-{label}\n",
    "    neg_sample_action = sample(neg_action, k)\n",
    "    sample_point = choice(range(10)) # has to change\n",
    "    neg_samples = np.array([action_ft_dict[a][sample_point, :] for a in neg_sample_action])\n",
    "    neg_samples = torch.from_numpy(neg_samples)\n",
    "    return neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sampling(10, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMet = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "pos_thr = 0.05\n",
    "neg_thr = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = torch.randn(1, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postive_distance(pred_vector, pos_vectors, agg='mean'):\n",
    "    pos_distances = disMet(pred_vector, pos_vectors)\n",
    "    soft_distance = F.relu(torch.abs(pos_distances)-Tensor([pos_thr]))\n",
    "    if agg == \"mean\":\n",
    "        return soft_distance.mean()\n",
    "    else:\n",
    "        return soft_distance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vector = torch.randn((1, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sampling(10, 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0175)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postive_distance(pred_vector, positive_sampling(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_distance(pred_vector, neg_vectors, agg=\"mean\"):\n",
    "    neg_distances = disMet(pred_vector, neg_vectors)\n",
    "    soft_distance = F.relu(Tensor([neg_thr])-torch.abs(neg_distances))\n",
    "    if agg == \"mean\":\n",
    "        return soft_distance.mean()\n",
    "    else:\n",
    "        return soft_distance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8037)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_distance(pred_vector, negative_sampling(10, 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch import nn, Tensor \n",
    "\n",
    "from random import sample, choice\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTripletLoss(nn.Module):\n",
    "    # def __init__(self, action_feats, label_map, pos_thr=0.05, neg_thr=0.95, agg=\"mean\"):\n",
    "    def __init__(self, action_feats, distance='cosine', k=10, pos_thr=0.05, neg_thr=0.95, agg=\"mean\", device=\"cpu\"):\n",
    "        super(ActionTripletLoss, self).__init__()\n",
    "        self.action_feats = deepcopy(action_feats)\n",
    "        self.get_disMet(distance)\n",
    "        self.k = k\n",
    "        self.pos_thr = pos_thr\n",
    "        self.neg_thr = neg_thr\n",
    "        self.agg = agg \n",
    "        self.device = device \n",
    "\n",
    "    def get_disMet(self, distance):\n",
    "        if distance == 'cosine':\n",
    "            self.disMet = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        else:\n",
    "            self.disMet = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def positive_sampling(self, label, k=10):\n",
    "        total_samples = self.action_feats[label]\n",
    "        self.n, _ = total_samples.shape \n",
    "        sample_idx = sample(range(self.n), k)\n",
    "        samples = total_samples[sample_idx, :]\n",
    "        return torch.from_numpy(samples) \n",
    "\n",
    "    def negative_sampling(self, label, k=10):\n",
    "        neg_action = set(self.action_feats.keys())-{label}\n",
    "        neg_sample_action = sample(neg_action, k)\n",
    "        sample_point = choice(range(self.n)) # has to change\n",
    "        neg_samples = np.array([self.action_feats[a][sample_point, :] for a in neg_sample_action])\n",
    "        neg_samples = torch.from_numpy(neg_samples)\n",
    "        return neg_samples\n",
    "\n",
    "    def postive_distance(self, pred_vector, pos_vectors, agg='mean'):\n",
    "        pos_distances = self.disMet(pred_vector, pos_vectors)\n",
    "        soft_distance = F.relu(torch.abs(pos_distances)-Tensor([self.pos_thr]))\n",
    "        if agg == \"mean\":\n",
    "            return soft_distance.mean()\n",
    "        else:\n",
    "            return soft_distance.sum()\n",
    "\n",
    "    \n",
    "    def negative_distance(self, pred_vector, neg_vectors, agg=\"mean\"):\n",
    "        neg_distances = self.disMet(pred_vector, neg_vectors)\n",
    "        soft_distance = F.relu(Tensor([self.neg_thr])-torch.abs(neg_distances))\n",
    "        if agg == \"mean\":\n",
    "            return soft_distance.mean()\n",
    "        else:\n",
    "            return soft_distance.sum()\n",
    "\n",
    "    def forward(self, pred_fts, labels):\n",
    "        pos_loss, neg_loss = 0, 0\n",
    "        for i,l in enumerate(labels):\n",
    "            ft = pred_fts[i, ...]\n",
    "            pos_vectors = self.positive_sampling(l, self.k)\n",
    "            neg_vectors = self.negative_sampling(l, self.k)\n",
    "            pos_loss += self.postive_distance(ft, pos_vectors, agg=self.agg)\n",
    "            neg_loss += self.negative_distance(ft, neg_vectors, agg=self.agg)\n",
    "\n",
    "        pos_loss = pos_loss/len(labels)\n",
    "        neg_loss = neg_loss/len(labels)\n",
    "        return pos_loss+neg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_data = np.load('../data/skeleton_k10_v7_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [(1, 'lying'),\n",
    " (2, 'sitting'),\n",
    " (3, 'standing'),\n",
    " (4, 'walking'),\n",
    " (5, 'running'),\n",
    " (6, 'cycling'),\n",
    " (7, 'Nordic walking'),\n",
    " (9, 'watching TV'),\n",
    " (10, 'computer work'),\n",
    " (11, 'car driving'),\n",
    " (12, 'ascending stairs'),\n",
    " (13, 'descending stairs'),\n",
    " (16, 'vacuum cleaning'),\n",
    " (17, 'ironing'),\n",
    " (18, 'folding laundry'),\n",
    " (19, 'house cleaning'),\n",
    " (20, 'playing soccer'),\n",
    " (24, 'rope jumping')]\n",
    "\n",
    "label2Id = {c[1]:i for i,c in enumerate(label_map)}\n",
    "action_dict = defaultdict(list)\n",
    "skeleton_Ids = []\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[label2Id[a]].append(i)\n",
    "    skeleton_Ids.append(label2Id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ft(data, model, device, bs=32):\n",
    "    ns, _, _ = data.shape \n",
    "    padded_mat = F.pad(input=data, pad=(0,0,0,0,0,bs-ns), mode='constant', value=0)\n",
    "    _, vector_out = model(padded_mat.float().to(device)) # batch second mode\n",
    "    action_feat_mat = vector_out[:ns, :].cpu().detach().numpy()\n",
    "    return action_feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = ae_model.to(device)\n",
    "action_ft_dict = {a: get_class_ft(torch.from_numpy(skeleton_mov[i, ...]), ae_model, device) for a,i in action_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpLoss =ActionTripletLoss(action_ft_dict, distance='cosine', k=10, pos_thr=0.90, neg_thr=0.05, agg=\"mean\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = torch.randn(8, 256)\n",
    "true_actions = [1, 4, 3, 6, 10, 5, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0157)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = trpLoss(pred_batch, true_actions)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch import nn, Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTripletLoss(nn.Module):\n",
    "    # def __init__(self, action_feats, label_map, pos_thr=0.05, neg_thr=0.95, agg=\"mean\"):\n",
    "    def __init__(self, distance='cosine', k=10, pos_thr=0.05, neg_thr=0.95, theta=1e-4, agg=\"mean\", device=\"cpu\"):\n",
    "        super(ActionTripletLoss, self).__init__()\n",
    "        self.distance = distance\n",
    "        self.get_disMet(distance)\n",
    "        self.k = k\n",
    "        self.pos_thr = pos_thr\n",
    "        self.neg_thr = neg_thr\n",
    "        self.agg = agg \n",
    "        self.device = device \n",
    "        self.theta = theta\n",
    "\n",
    "    def get_disMet(self, distance):\n",
    "        if distance == 'cosine':\n",
    "            self.disMet = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        else:\n",
    "            self.disMet = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def forward(self, pred_fts, pos_fts, neg_fts):\n",
    "        neg_distances = self.disMet(pred_fts, neg_fts).to(self.device)\n",
    "        pos_distances = self.disMet(pred_fts, pos_fts).to(self.device)\n",
    "\n",
    "        if self.distance == 'cosine':\n",
    "            neg_distances = torch.abs(neg_distances)\n",
    "            pos_distances = torch.abs(pos_distances)\n",
    "\n",
    "        triplet_loss = F.relu(pos_distances-neg_distances+self.theta)\n",
    "        return triplet_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = ActionTripletLoss(distance='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0353)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fts = torch.randn((32, 128))\n",
    "pos_fts = torch.randn((32, 128))\n",
    "neg_fts = torch.randn((32, 128))\n",
    "\n",
    "loss_func(pred_fts, pos_fts, neg_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_fts\u001b[39m+\u001b[39;49mpos_fts\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "pred_fts+pos_fts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fts = torch.randn((32, 128))\n",
    "pos_fts = torch.randn((10, 32, 128))\n",
    "\n",
    "mat = nn.PairwiseDistance(p=2)\n",
    "mat(pred_fts, pos_fts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch import nn, Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTripletLoss(nn.Module):\n",
    "    # def __init__(self, action_feats, label_map, pos_thr=0.05, neg_thr=0.95, agg=\"mean\"):\n",
    "    def __init__(self, distance='cosine', k=10, pos_thr=0.05, neg_thr=0.95, theta=1e-4, agg=\"mean\", device=\"cpu\"):\n",
    "        super(ActionTripletLoss, self).__init__()\n",
    "        self.distance = distance\n",
    "        self.get_disMet(distance)\n",
    "        self.k = k\n",
    "        self.pos_thr = pos_thr\n",
    "        self.neg_thr = neg_thr\n",
    "        self.agg = agg \n",
    "        self.device = device \n",
    "        self.theta = theta\n",
    "\n",
    "    def get_disMet(self, distance):\n",
    "        if distance == 'cosine':\n",
    "            self.disMet = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        else:\n",
    "            self.disMet = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    def forward(self, pred_fts, pos_fts, neg_fts):\n",
    "        neg_distances = self.disMet(pred_fts, neg_fts).to(self.device)\n",
    "        pos_distances = self.disMet(pred_fts, pos_fts).to(self.device)\n",
    "\n",
    "        if self.distance == 'cosine':\n",
    "            neg_distances = torch.abs(neg_distances)\n",
    "            pos_distances = torch.abs(pos_distances)\n",
    "\n",
    "        triplet_loss = F.relu(pos_distances-neg_distances+self.theta)\n",
    "        return triplet_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = ActionTripletLoss(distance='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0323)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fts = torch.randn((32, 128))\n",
    "pos_fts = torch.randn((10, 32, 128))\n",
    "neg_fts = torch.randn((10, 32, 128))\n",
    "\n",
    "loss_func(pred_fts, pos_fts, neg_fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
