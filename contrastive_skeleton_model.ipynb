{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc \n",
    "import random \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Dataset and Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_data = np.load('./data/skeleton_movements.npz')\n",
    "skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "action_dict = defaultdict(list)\n",
    "for i, a in enumerate(skeleton_classes):\n",
    "    action_dict[a].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, movements, actions, action_dict):\n",
    "        super(SkeletonDataset, self).__init__()\n",
    "        self.movements = movements \n",
    "        self.actions = actions\n",
    "        self.action_dict = action_dict\n",
    "        self.actionsIDs = list(self.action_dict.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.movements[idx, ...]\n",
    "        action = self.actions[idx]\n",
    "\n",
    "        partial_idx = random.sample(self.action_dict[action], k=1)[0]\n",
    "        x2 = self.movements[partial_idx, ...]\n",
    "\n",
    "        label = self.actionsIDs.index(action)\n",
    "        return x1, x2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.movements.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60, 36])\n"
     ]
    }
   ],
   "source": [
    "sample_dt = SkeletonDataset(skeleton_mov, skeleton_classes, action_dict)\n",
    "sample_dl = DataLoader(sample_dt, batch_size=BS, shuffle=True, pin_memory=True, drop_last=True)\n",
    "for b in sample_dl:\n",
    "    bx1, bx2, by = b \n",
    "    # bx = torch.transpose(bx1, 1, 0)\n",
    "    bs, seq_len, ft_in = bx1.shape\n",
    "    print(bx1.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 60, 36])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx12 = torch.vstack([bx1, bx2])\n",
    "bx = torch.transpose(bx12, 1, 0)\n",
    "bx12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bys = torch.cat([by, by], dim=0)\n",
    "bys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from torch import nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence                         \n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out.squeeze(0))     \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, seq_len, hidden_size, batch_size, ae_type='recursive', teacher_forcing_ratio=0.5, device='cpu'):\n",
    "        super(LSTMAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size \n",
    "        self.bs = batch_size\n",
    "        self.ae_type = ae_type # ['recursive', 'teacher_forcing', 'mixed_teacher_forcing']\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.device = device \n",
    "\n",
    "        self.encoder = LSTMEncoder(input_size = input_size, hidden_size = hidden_size)\n",
    "        self.decoder = LSTMDecoder(input_size = input_size, hidden_size = hidden_size)\n",
    "\n",
    "        self.encoder_hidden = self.encoder.init_hidden(self.bs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding \n",
    "        encoder_output, self.encoder_hidden = self.encoder(x)\n",
    "        # decoding \n",
    "        decoder_input = torch.rand((self.bs, self.input_size), requires_grad=True).to(self.device)#self.encoder_hidden[0].squeeze()\n",
    "        print(decoder_input.shape)\n",
    "        decoder_hidden = self.encoder_hidden\n",
    "        # outputs tensor\n",
    "        outputs = torch.zeros(self.seq_len, self.bs, self.input_size).to(self.device)\n",
    "\n",
    "        if self.ae_type == 'recursive':\n",
    "            for t in range(self.seq_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "                decoder_input = decoder_output\n",
    "        \n",
    "        elif self.ae_type == 'teacher_forcing':\n",
    "            if random.random() < self.teacher_forcing_ratio:\n",
    "                for t in range(self.seq_len):\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = x[t, :, :]\n",
    "\n",
    "            else:\n",
    "                for t in range(self.seq_len):\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "        elif self.ae_type == 'mixed_teacher_forcing':\n",
    "            for t in range(self.seq_len):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                outputs[t] = decoder_output\n",
    "\n",
    "                if random.random() < self.teacher_forcing_ratio:\n",
    "                    decoder_input = x[t, :, :]\n",
    "                else:\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "        return outputs, encoder_output[-1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 64, 36]), torch.Size([64, 128]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INTER_Ft = 128\n",
    "model = LSTMAE(input_size=ft_in, seq_len=seq_len, hidden_size=INTER_Ft, batch_size=2*bs, ae_type='recursive')\n",
    "skeleton_output, vector_output = model(bx.float())\n",
    "skeleton_output.shape, vector_output.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.losses import SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 128])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1, f2 = torch.split(vector_output, [bs, bs], dim=0)\n",
    "cons_output = torch.stack([f1.squeeze(1), f2.squeeze(1)], dim=1)\n",
    "cons_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on contrastive loss \n",
    "con_loss = SupConLoss()\n",
    "sample_loss = con_loss(cons_output, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4546, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 64, 36])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 64, 36])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeleton_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 60, 36]), torch.Size([64, 60, 36]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx_tp = torch.transpose(bx, 1, 0)\n",
    "skel_tp = torch.transpose(skeleton_output, 1, 0)\n",
    "bx_tp.shape, skel_tp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2630, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mseLoss = nn.MSELoss()\n",
    "l2_loss = mseLoss(bx_tp, skel_tp)\n",
    "l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/3 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  67%|██████▋   | 2/3 [00:00<00:00,  4.93batch/s, contrastive loss =tensor(8.5735, device='cuda:0', grad_fn=<MeanBackward0>), reconstruction loss=tensor(0.2647, device='cuda:0', grad_fn=<MseLossBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36])\n",
      "torch.Size([64, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 3/3 [00:00<00:00,  5.22batch/s, contrastive loss =tensor(5.2948, device='cuda:0', grad_fn=<MeanBackward0>), reconstruction loss=tensor(0.2636, device='cuda:0', grad_fn=<MseLossBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# build AE-training step \n",
    "# ----------------------------parameter mapping -------------------------\n",
    "\n",
    "dataloader = sample_dl \n",
    "phase = 'train'\n",
    "batch_size = 32\n",
    "alpha = 0.5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model = LSTMAE(input_size=ft_in, seq_len=seq_len, hidden_size=INTER_Ft, batch_size=2*bs, ae_type='recursive', device=device)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "conLoss = SupConLoss()\n",
    "l2Loss = nn.MSELoss()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "model = model.train()\n",
    "model.to(device)\n",
    "\n",
    "epoch_loss = 0 \n",
    "total_samples = 0 \n",
    "\n",
    "with tqdm(dataloader, unit='batch', desc=phase) as tepoch:\n",
    "    for batch in tepoch:\n",
    "        x1, x2, labels = batch \n",
    "        # post-process \n",
    "        xa = torch.transpose(torch.vstack([x1, x2]), 1, 0)\n",
    "        # device offload \n",
    "        xa = xa.float().to(device)\n",
    "        labels = labels.float()\n",
    "\n",
    "        # set optimizer grad to zero \n",
    "        optimizer.zero_grad()\n",
    "        # get model prediction \n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "            skel_output, ft_output = model(xa)\n",
    "\n",
    "        # reconstruct the output \n",
    "        f1, f2 = torch.split(ft_output, [batch_size, batch_size], dim=0)\n",
    "        cons_output = torch.stack([f1.squeeze(1), f2.squeeze(1)], dim=1)\n",
    "        # calc. contrastive loss \n",
    "        con_loss = conLoss(cons_output, labels)\n",
    "        # calc. reconstruction loss \n",
    "        l2_loss = l2Loss(xa, skel_output)\n",
    "        # calc. total loss\n",
    "        total_loss = alpha*con_loss + (1-alpha)*l2_loss\n",
    "\n",
    "        if phase == 'train':\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        metrics = {'contrastive loss ': con_loss, 'reconstruction loss': l2_loss}\n",
    "        with torch.no_grad():\n",
    "                total_samples += len(labels)\n",
    "                epoch_loss += total_loss.item()\n",
    "        \n",
    "        tepoch.set_postfix(metrics)\n",
    "\n",
    "epoch_loss = epoch_loss/total_samples\n",
    "# return epoch_loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
