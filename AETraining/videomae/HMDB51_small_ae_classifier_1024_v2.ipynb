{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7649,"status":"ok","timestamp":1684579878156,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"BkIfhXqxWhOw","outputId":"989a2212-b1b6-4863-b2bb-5c666873e77f"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15872\\1173221807.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n","C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15872\\1173221807.py:85: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n"]}],"source":["\n","import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import neptune\n","\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import itertools\n","import pandas as pd\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","\n","\"\"\"\n","Collection of functions which enable the evaluation of a classifier's performance,\n","by showing confusion matrix, accuracy, recall, precision etc.\n","\"\"\"\n","\n","import numpy as np\n","import sys\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","\n","def save_history(history, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","\n","    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n","        json.dump(history, f0)\n","\n","def get_config(file_loc):\n","    file = torch.load(file_loc)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","    \n","def save_model(model, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","    torch.save({\n","        \"n_epochs\": config[\"n_epochs\"],\n","        \"model_state_dict\": model.state_dict(),\n","        \"model_config\": config[\"model\"],\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}.pt\")\n","\n","def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n","    \"\"\"Plot confusion matrix in a separate window\"\"\"\n","    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    if label_strings:\n","        tick_marks = np.arange(len(label_strings))\n","        plt.xticks(tick_marks, label_strings, rotation=90)\n","        plt.yticks(tick_marks, label_strings)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n","    \"\"\"\n","    Returns a string of a report for given metric arrays (array length equals the number of classes).\n","    Called internally by `analyze_classification`.\n","        digits: number of digits after . for displaying results\n","        number_of_thieves: number of biggest thieves to report\n","        maxcharlength: max. number of characters to use when displaying thief names\n","    \"\"\"\n","\n","    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n","    sorted_class_indices = np.argsort(relative_freq)[\n","                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n","\n","    last_line_heading = 'avg / total'\n","\n","    width = max(len(cn) for cn in existing_class_names)\n","    width = max(width, len(last_line_heading), digits)\n","\n","    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n","    fmt = '%% %ds' % width  # first column: class name\n","    fmt += '  '\n","    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n","    fmt += '|\\t % 5s'\n","    fmt += '\\n'\n","\n","    headers = [\"\"] + headers\n","    report = fmt % tuple(headers)\n","    report += '\\n'\n","\n","    for i in sorted_class_indices:\n","        values = [existing_class_names[i]]\n","        for v in (precision[i], recall[i], f1[i],\n","                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n","            values += [\"{0:0.{1}f}\".format(v, digits)]\n","        values += [\"{}\".format(support[i])]\n","        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n","                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n","        thieves = thieves[thieves != i]  # exclude self at this point\n","        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n","        thieves_names = [\n","            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n","            in thieves]  # a little inefficient but inconsequential\n","        string_about_stealing = \"\"\n","        for j in range(len(thieves)):\n","            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n","        values += [string_about_stealing]\n","\n","        report += fmt % tuple(values)\n","\n","    report += '\\n' + 100 * '-' + '\\n'\n","\n","    # compute averages/sums\n","    values = [last_line_heading]\n","    for v in (np.average(precision, weights=relative_freq),\n","                np.average(recall, weights=relative_freq),\n","                np.average(f1, weights=relative_freq)):\n","        values += [\"{0:0.{1}f}\".format(v, digits)]\n","    values += ['{0}'.format(np.sum(relative_freq))]\n","    values += ['{0}'.format(np.sum(support))]\n","    values += ['']\n","\n","    # make last (\"Total\") line for report\n","    report += fmt % tuple(values)\n","\n","    return report\n","\n","\n","def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n","    \"\"\"\n","    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n","    Input:\n","        y_pred: 1D array of predicted labels (class indices)\n","        y_true: 1D array of true labels (class indices)\n","        class_names: 1D array or list of class names in the order of class indices.\n","            Could also be integers [0, 1, ..., num_classes-1].\n","        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n","    \"\"\"\n","\n","    # Trim class_names to include only classes existing in y_pred OR y_true\n","    in_pred_labels = set(list(y_pred))\n","    in_true_labels = set(list(y_true))\n","    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n","\n","    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n","    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n","    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n","    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n","\n","    # Confusion matrix\n","    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n","\n","    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n","    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n","\n","    if show_plot:\n","        plt.figure()\n","        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n","                                title='Confusion matrix normalized by row')\n","        plt.show(block=False)\n","\n","    # Analyze results\n","    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n","    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n","\n","    # returns metrics for each class, in the same order as existing_class_names\n","    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n","    # Print report\n","    if print_report:\n","        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n","\n","    # Calculate average precision and recall\n","    # prec_avg, rec_avg = get_avg_prec_recall(ConfMatrix, existing_class_names, excluded_classes)\n","    # if excluded_classes:\n","    #     print(\n","    #         \"\\nAverage PRECISION: {:.2f}\\n(using class frequencies as weights, excluding classes with no predictions and predictions in '{}')\".format(\n","    #             prec_avg, ', '.join(excluded_classes)))\n","    #     print(\n","    #         \"\\nAverage RECALL (= ACCURACY): {:.2f}\\n(using class frequencies as weights, excluding classes in '{}')\".format(\n","    #             rec_avg, ', '.join(excluded_classes)))\n","\n","    # Make a histogram with the distribution of classes with respect to precision and recall\n","    # prec_rec_histogram(precision, recall)\n","\n","    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15872\\1173221807.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n","C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15872\\1173221807.py:85: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n"]}],"source":["\n","import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import neptune\n","\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import itertools\n","import pandas as pd\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","\n","\"\"\"\n","Collection of functions which enable the evaluation of a classifier's performance,\n","by showing confusion matrix, accuracy, recall, precision etc.\n","\"\"\"\n","\n","import numpy as np\n","import sys\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","\n","def save_history(history, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","\n","    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n","        json.dump(history, f0)\n","\n","def get_config(file_loc):\n","    file = torch.load(file_loc)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","    \n","def save_model(model, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","    torch.save({\n","        \"n_epochs\": config[\"n_epochs\"],\n","        \"model_state_dict\": model.state_dict(),\n","        \"model_config\": config[\"model\"],\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}.pt\")\n","\n","def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n","    \"\"\"Plot confusion matrix in a separate window\"\"\"\n","    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    if label_strings:\n","        tick_marks = np.arange(len(label_strings))\n","        plt.xticks(tick_marks, label_strings, rotation=90)\n","        plt.yticks(tick_marks, label_strings)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n","    \"\"\"\n","    Returns a string of a report for given metric arrays (array length equals the number of classes).\n","    Called internally by `analyze_classification`.\n","        digits: number of digits after . for displaying results\n","        number_of_thieves: number of biggest thieves to report\n","        maxcharlength: max. number of characters to use when displaying thief names\n","    \"\"\"\n","\n","    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n","    sorted_class_indices = np.argsort(relative_freq)[\n","                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n","\n","    last_line_heading = 'avg / total'\n","\n","    width = max(len(cn) for cn in existing_class_names)\n","    width = max(width, len(last_line_heading), digits)\n","\n","    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n","    fmt = '%% %ds' % width  # first column: class name\n","    fmt += '  '\n","    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n","    fmt += '|\\t % 5s'\n","    fmt += '\\n'\n","\n","    headers = [\"\"] + headers\n","    report = fmt % tuple(headers)\n","    report += '\\n'\n","\n","    for i in sorted_class_indices:\n","        values = [existing_class_names[i]]\n","        for v in (precision[i], recall[i], f1[i],\n","                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n","            values += [\"{0:0.{1}f}\".format(v, digits)]\n","        values += [\"{}\".format(support[i])]\n","        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n","                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n","        thieves = thieves[thieves != i]  # exclude self at this point\n","        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n","        thieves_names = [\n","            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n","            in thieves]  # a little inefficient but inconsequential\n","        string_about_stealing = \"\"\n","        for j in range(len(thieves)):\n","            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n","        values += [string_about_stealing]\n","\n","        report += fmt % tuple(values)\n","\n","    report += '\\n' + 100 * '-' + '\\n'\n","\n","    # compute averages/sums\n","    values = [last_line_heading]\n","    for v in (np.average(precision, weights=relative_freq),\n","                np.average(recall, weights=relative_freq),\n","                np.average(f1, weights=relative_freq)):\n","        values += [\"{0:0.{1}f}\".format(v, digits)]\n","    values += ['{0}'.format(np.sum(relative_freq))]\n","    values += ['{0}'.format(np.sum(support))]\n","    values += ['']\n","\n","    # make last (\"Total\") line for report\n","    report += fmt % tuple(values)\n","\n","    return report\n","\n","\n","def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n","    \"\"\"\n","    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n","    Input:\n","        y_pred: 1D array of predicted labels (class indices)\n","        y_true: 1D array of true labels (class indices)\n","        class_names: 1D array or list of class names in the order of class indices.\n","            Could also be integers [0, 1, ..., num_classes-1].\n","        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n","    \"\"\"\n","\n","    # Trim class_names to include only classes existing in y_pred OR y_true\n","    in_pred_labels = set(list(y_pred))\n","    in_true_labels = set(list(y_true))\n","    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n","\n","    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n","    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n","    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n","    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n","\n","    # Confusion matrix\n","    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n","\n","    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n","    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n","\n","    if show_plot:\n","        plt.figure()\n","        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n","                                title='Confusion matrix normalized by row')\n","        plt.show(block=False)\n","\n","    # Analyze results\n","    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n","    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n","\n","    # returns metrics for each class, in the same order as existing_class_names\n","    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n","    # Print report\n","    if print_report:\n","        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n","\n","    # Calculate average precision and recall\n","    # prec_avg, rec_avg = get_avg_prec_recall(ConfMatrix, existing_class_names, excluded_classes)\n","    # if excluded_classes:\n","    #     print(\n","    #         \"\\nAverage PRECISION: {:.2f}\\n(using class frequencies as weights, excluding classes with no predictions and predictions in '{}')\".format(\n","    #             prec_avg, ', '.join(excluded_classes)))\n","    #     print(\n","    #         \"\\nAverage RECALL (= ACCURACY): {:.2f}\\n(using class frequencies as weights, excluding classes in '{}')\".format(\n","    #             rec_avg, ', '.join(excluded_classes)))\n","\n","    # Make a histogram with the distribution of classes with respect to precision and recall\n","    # prec_rec_histogram(precision, recall)\n","\n","    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1661,"status":"ok","timestamp":1684579879798,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"UrstYvBiNbvA"},"outputs":[],"source":["from functools import partial\n","import numpy as np\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n","from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n","from timm.models.registry import register_model\n","import torch.utils.checkpoint as checkpoint\n","import math\n","\n","def pretrain_trunc_normal_(tensor, mean=0., std=1.):\n","    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n","        **kwargs\n","    }\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","    \n","    def extra_repr(self) -> str:\n","        return 'p={}'.format(self.drop_prob)\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        # x = self.drop(x)\n","        # commit this for the orignal BERT implement \n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","                 attn_head_dim=None):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if init_values > 0:\n","            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        else:\n","            self.gamma_1, self.gamma_2 = None, None\n","\n","    def forward(self, x):\n","        if self.gamma_1 is None:\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        else:\n","            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        self.tubelet_size = int(tubelet_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim, \n","                            kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]), \n","                            stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n","\n","    def forward(self, x, **kwargs):\n","        B, C, T, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","    \n","# sin-cos position encoding\n","# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n","def get_sinusoid_encoding_table(n_position, d_hid): \n","    ''' Sinusoid position encoding table ''' \n","    # TODO: make it with torch instead of numpy \n","    def get_position_angle_vec(position): \n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n","\n","    return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0) \n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, \n","                 img_size=224, \n","                 patch_size=16, \n","                 in_chans=3, \n","                 num_classes=1000, \n","                 embed_dim=768, \n","                 depth=12,\n","                 num_heads=12, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 fc_drop_rate=0., \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False, \n","                 init_scale=0.,\n","                 all_frames=16,\n","                 tubelet_size=2,\n","                 use_checkpoint=False,\n","                 use_mean_pooling=True):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.tubelet_size = tubelet_size\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=all_frames, tubelet_size=self.tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings is on the way\n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n","        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n","        self.fc_dropout = nn.Dropout(p=fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        pretrain_trunc_normal_(self.head.weight, std=.02)\n","        self.apply(self._init_weights)\n","\n","        self.head.weight.data.mul_(init_scale)\n","        self.head.bias.data.mul_(init_scale)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            pretrain_trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        x = self.patch_embed(x)\n","        B, _, _ = x.size()\n","\n","        if self.pos_embed is not None:\n","            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        x = self.pos_drop(x)\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        x = self.norm(x)\n","        if self.fc_norm is not None:\n","            return self.fc_norm(x.mean(1))\n","        else:\n","            return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(self.fc_dropout(x))\n","        return x\n","\n","\n","@register_model\n","def vit_small_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_512(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_huge_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","class PretrainVisionTransformerEncoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n","                 use_learnable_pos_emb=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,tubelet_size=tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","        self.num_patches = num_patches\n","\n","\n","        # TODO: Add the cls token\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings \n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        _, _, T, _, _ = x.shape\n","        x = self.patch_embed(x)\n","        \n","        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n","\n","        B, _, C = x.shape\n","        x_vis = x.reshape(B, -1, C) # ~mask means visible\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x_vis = checkpoint.checkpoint(blk, x_vis)\n","        else:   \n","            for blk in self.blocks:\n","                x_vis = blk(x_vis)\n","\n","        x_vis = self.norm(x_vis)\n","        return x_vis\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","class PretrainVisionTransformerDecoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n","                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n","                 norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n","                 ):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        assert num_classes == 3 * tubelet_size * patch_size ** 2 \n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_size = patch_size\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x, return_token_num):\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        if return_token_num > 0:\n","            x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n","        else:\n","            x = self.head(self.norm(x))\n","\n","        return x\n","\n","class PretrainVisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self,\n","                 img_size=224, \n","                 patch_size=16, \n","                 encoder_in_chans=3, \n","                 encoder_num_classes=0, \n","                 encoder_embed_dim=768, \n","                 encoder_depth=12,\n","                 encoder_num_heads=12, \n","                 decoder_num_classes=1536, #  decoder_num_classes=768, \n","                 decoder_embed_dim=512, \n","                 decoder_depth=8,\n","                 decoder_num_heads=8, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False,\n","                 use_checkpoint=False,\n","                 tubelet_size=2,\n","                 num_classes=0, # avoid the error from create_fn in timm\n","                 in_chans=0, # avoid the error from create_fn in timm\n","                 ):\n","        super().__init__()\n","        self.encoder = PretrainVisionTransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=encoder_in_chans, \n","            num_classes=encoder_num_classes, \n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint,\n","            use_learnable_pos_emb=use_learnable_pos_emb)\n","\n","        self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=patch_size, \n","            num_patches=self.encoder.patch_embed.num_patches,\n","            num_classes=decoder_num_classes, \n","            embed_dim=decoder_embed_dim, \n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint)\n","\n","        self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n","\n","        trunc_normal_(self.mask_token, std=.02)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'mask_token'}\n","\n","    def forward(self, x):\n","        _, _, T, _, _ = x.shape\n","        x_vis = self.encoder(x) # [B, N_vis, C_e]\n","        x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","        B, N, C = x_vis.shape\n","        # we don't unshuffle the correct visible token order, \n","        # but shuffle the pos embedding accorddingly.\n","        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","        pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","        x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","        x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","\n","        return x\n","\n","@register_model\n","def pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=768, \n","        encoder_depth=12, \n","        encoder_num_heads=12,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536,\n","        decoder_embed_dim=384,\n","        decoder_num_heads=6,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n"," \n","@register_model\n","def pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1024, \n","        encoder_depth=24, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=512,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1280, \n","        encoder_depth=32, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=640,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684579879798,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"p0XIAvcQX3l0"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9206,"status":"ok","timestamp":1684579888999,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"y7gAeWUDX7X6"},"outputs":[],"source":["model_ident = \"HMDB51_small_ae_v2_classifier_1024_emb1d\"\n","unique_iden = \"epoch50_emb1024\"\n","dataset_ident = \"HMDB51\"\n","\n","main_dir = \"../..\"\n","data_dir = os.path.join(\"D:\\\\FYP\\\\VideoMAE\\\\preprocessed\\\\HMDB51_1seg\")\n","original_data_dir = \"D:\\\\FYP\\\\hmdb51_org\"\n","train_dir = os.path.join(data_dir,\"train\")\n","val_dir = os.path.join(data_dir,\"val\")\n","\n","\"\"\"\n","main_dir = \"../..\"\n","data_dir = os.path.join(\"H:\\\\Academics\\\\MHEALTH - New\\\\Videos-new\")\n","\"\"\"\n","\n","epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n","models_saves = os.path.join(main_dir,\"model_saves\")\n","embeddings_save = os.path.join(main_dir,\"embedding_save\")\n","prototypes_save = os.path.join(main_dir,\"prototypes\")\n","test_vids = os.path.join(main_dir,\"test_vids\")\n","setting_fol = os.path.join(main_dir,\"settings\")\n","os.makedirs(f\"{setting_fol}/{model_ident}\",exist_ok=True)\n","train_ratio = 0.90\n","val_ratio = 0.1\n","batch_size = 2\n","\n","os.makedirs(epoch_vids,exist_ok=True)\n","os.makedirs(models_saves,exist_ok=True)\n","os.makedirs(embeddings_save,exist_ok=True)\n","\n","class_names = os.listdir(original_data_dir)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684579888999,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"IrsB7xhhaKF5"},"outputs":[],"source":["config = {\n","    \"n_epochs\":50,\n","    \"model_name\":\"BidirectionalLSTM\",\n","    \"model\":{\n","        \"num_joint\":12,\n","        \"seq_len\":60,\n","        \"decoder_hidden_size\":1024,\n","        \"linear_filters\":[128,256,512,1024],\n","        \"embedding_size\":1024,\n","        \"num_classes\":len(class_names),\n","        \"num_layers\":1,\n","        \"is_3d\":False,\n","        \"bidirectional\":True,\n","        \"batch_size\":batch_size,\n","        \"dev\":device,\n","        \"device\":\"gpu\" if torch.cuda.is_available() else \"cpu\"\n","        },\n","    \"lr\":1e-3,\n","    'alpha_target': 0.1\n","}\n","\n","id2clsname, clsname2id = classname_id(class_names)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1782,"status":"ok","timestamp":1684579890772,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"bziGM7xoEgPz","outputId":"259ab87e-372c-4da5-e850-33b13c29422b"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["https://app.neptune.ai/FYP-Group22/ICANN-Logs/e/IC-255\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n","        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n","        for dictionaries or collections that contain unsupported values.\n","        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n","  warnings.warn(\n","c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'torch.device'>).\n","        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n","        for dictionaries or collections that contain unsupported values.\n","        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n","  warnings.warn(\n"]}],"source":["run = neptune.init_run(\n","    project=\"FYP-Group22/ICANN-Logs\",\n","    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",")  # your credentials\n","\n","run['parameters'] = config"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1684579890773,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"PrpFnUcsHcDo","outputId":"b7626b73-1ad6-4bd8-dfc8-f0153090c1cf"},"outputs":[{"data":{"text/plain":["{'n_epochs': 50,\n"," 'model_name': 'BidirectionalLSTM',\n"," 'model': {'num_joint': 12,\n","  'seq_len': 60,\n","  'decoder_hidden_size': 1024,\n","  'linear_filters': [128, 256, 512, 1024],\n","  'embedding_size': 1024,\n","  'num_classes': 51,\n","  'num_layers': 1,\n","  'is_3d': False,\n","  'bidirectional': True,\n","  'batch_size': 2,\n","  'dev': device(type='cuda'),\n","  'device': 'gpu'},\n"," 'lr': 0.001,\n"," 'alpha_target': 0.1}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":695,"status":"ok","timestamp":1684579891453,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"Res0UAsOjjki"},"outputs":[],"source":["import os\n","import numpy as np\n","from numpy.lib.function_base import disp\n","import torch\n","import decord\n","from PIL import Image\n","from torchvision import transforms\n","from random_erasing import RandomErasing\n","import warnings\n","from decord import VideoReader, cpu\n","from torch.utils.data import Dataset\n","import video_transforms as video_transforms \n","import volume_transforms as volume_transforms\n","from kinetics import *\n","\n","class VideoNumpyDataset(Dataset):\n","  def __init__(self,data_folder) -> None:\n","      super(VideoNumpyDataset,self).__init__()\n","      self.datafolder = data_folder\n","      self.files = [os.path.join(self.datafolder,x) for x in os.listdir(self.datafolder)]\n","\n","  def __getitem__(self, index):\n","    arrays = np.load(self.files[index])\n","    video = arrays[\"video\"]\n","    label = arrays[\"label\"]\n","    id = arrays[\"id\"]\n","    chunk_nb = arrays[\"chunk_nb\"]\n","    split_nb = arrays[\"split_nb\"]\n","    return video,clsname2id[str(label)],str(id),chunk_nb, split_nb\n","\n","\n","  def __len__(self):\n","    return len(self.files)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1684579891454,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"o0Mlr88WEjWt","outputId":"42d6f2c0-37f0-4cc9-d2c7-812b7f9df29e"},"outputs":[],"source":["test_mode = True\n","mean=[0.485, 0.456, 0.406],\n","std=[0.229, 0.224, 0.225]\n","\n","train_dataset = VideoNumpyDataset(train_dir)\n","\n","val_dataset = VideoNumpyDataset(val_dir)\n","\n","\n","train_dl = torch.utils.data.DataLoader(\n","            train_dataset, \n","            batch_size=batch_size,\n","            drop_last=False\n","            )\n","\n","val_dl = torch.utils.data.DataLoader(\n","            val_dataset, \n","            batch_size=batch_size,\n","            drop_last=False\n","            )"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1684579891455,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"gv4MRtlZbLYt"},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","\n","def unnormalize_img(img):\n","    \"\"\"Un-normalizes the image pixels.\"\"\"\n","    img = (img * std) + mean\n","    img = (img * 255).astype(\"uint8\")\n","    return img.clip(0, 255)\n","\n","def create_gif(video_tensor, filename=\"sample.gif\"):\n","    \"\"\"Prepares a GIF from a video tensor.\n","    \n","    The video tensor is expected to have the following shape:\n","    (num_frames, num_channels, height, width).\n","    \"\"\"\n","    frames = []\n","    for video_frame in video_tensor:\n","        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n","        frames.append(frame_unnormalized)\n","    kargs = {\"duration\": 0.25}\n","    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n","    return filename\n","\n","\n","def display_gif(video_tensor, gif_name=\"sample.gif\"):\n","    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n","    video_tensor = video_tensor.permute(1, 0, 2, 3)\n","    gif_filename = create_gif(video_tensor, gif_name)\n","    return Image(filename=gif_filename)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["ae = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        decoder_depth=4,\n","        drop_path_rate=0.0,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        use_checkpoint=False\n","        )\n","ae.default_cfg = _cfg()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["ae_params = torch.load(\n","    \"videomae_pretrained_weights\\\\ae_small_pretrained_kinetic400_1600e_.pth\",\n","    map_location=device\n","    )\n","\n","ae.load_state_dict(ae_params[\"model\"]);"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1684579891455,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"QNhRUvwyFUQb"},"outputs":[],"source":["class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","\n","class EncoderMapping(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(EncoderMapping,self).__init__(*args, **kwargs)\n","      input_dim = encoder_embed_dim\n","\n","      lout1 = EncoderMapping.calc_lout(num_patches,512,3,input_dim,stride=2)\n","      lout2 = EncoderMapping.calc_lout(512,128,3,lout1,stride=1)\n","\n","      self.attn1 = Attention(\n","            lout2, num_heads=12)\n","      self.linear1 = nn.Linear(lout2,128)\n","      \n","      self.attn2 = Attention(\n","            128, num_heads=12)\n","      self.linear2 = nn.Linear(128,32)\n","      \n","      self.conv1 = nn.Conv1d(num_patches,512,3, stride=2)\n","      self.conv2 = nn.Conv1d(512,128,3, stride=1)\n","\n","      self.linear3 = nn.Linear(32*128,1024)\n","      self.embed = nn.Linear(1024,embedding_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1):\n","    lout = (lin+2*padding-dilation*(kernal_size-1)-1)//stride+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    \n","    x = self.attn1(x)\n","    x = self.linear1(x)\n","    x = self.attn2(x)\n","    x = self.linear2(x)\n","\n","    x = x.flatten(1)\n","    \n","    x = self.linear3(x)\n","    x = self.embed(x)\n","    return x\n","\n","\n","class DecoderMapping(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMapping,self).__init__(*args, **kwargs)\n","      input_dim = encoder_embed_dim\n","\n","      lout2 = DecoderMapping.calc_lout(128,512,3,input_dim,stride=1)\n","      lout1 = DecoderMapping.calc_lout(512,num_patches,3,lout2,stride=2)\n","\n","      self.attn1 = Attention(\n","            input_dim, num_heads=12)\n","      self.linear1 = nn.Linear(32,input_dim)\n","      \n","      self.attn2 = Attention(\n","            128, num_heads=12)\n","      self.linear2 = nn.Linear(32,128)\n","      \n","      self.conv1 = nn.ConvTranspose1d(512,num_patches,3, stride=2)\n","      self.conv2 = nn.ConvTranspose1d(128,512,3, stride=1)\n","\n","      self.linear3 = nn.Linear(1024,32*128)\n","      self.embed = nn.Linear(embedding_dim,1024)\n","      self.linear4 = nn.Linear(lout1,input_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = self.linear3(x)\n","    x = x.reshape(-1,128,32)\n","\n","    x = self.linear2(x)\n","    x = self.attn2(x)\n","    x = self.linear1(x)\n","    x = self.attn1(x)\n","\n","\n","    x = self.conv2(x)\n","    x = self.conv1(x)\n","    x = self.linear4(x)\n","    \n","    return x\n","\n","class VideoEncoder(nn.Module):\n","  def __init__(self,backbone_encoder,embedding_dim, *args, **kwargs) -> None:\n","      super(VideoEncoder,self).__init__(*args, **kwargs)\n","      self.encoder = backbone_encoder\n","      self.map_unit = EncoderMapping(self.encoder.embed_dim,self.encoder.num_patches, embedding_dim)\n","\n","      \n","\n","  def forward(self,x):\n","    x = self.encoder(x)\n","    x = self.map_unit(x)\n","    return x\n","\n","class VideoDecoder(nn.Module):\n","  def __init__(self,backbone_decoder,backbone_encoder,encoder_to_decoder,embedding_dim, *args, **kwargs) -> None:\n","      super(VideoDecoder,self).__init__(*args, **kwargs)\n","      self.decoder = backbone_decoder\n","      self.encoder_to_decoder = encoder_to_decoder\n","      self.map_unit = DecoderMapping(backbone_encoder.embed_dim,backbone_encoder.num_patches, embedding_dim)\n","\n","      self.mask_token = nn.Parameter(torch.zeros(1, 1, self.decoder.embed_dim))\n","\n","      self.pos_embed = get_sinusoid_encoding_table(backbone_encoder.patch_embed.num_patches, self.decoder.embed_dim)\n","\n","      trunc_normal_(self.mask_token, std=.02)\n","\n","  def forward(self,x):\n","    x_vis = self.map_unit(x)\n","    x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","    B, N, C = x_vis.shape\n","    # we don't unshuffle the correct visible token order, \n","    # but shuffle the pos embedding accorddingly.\n","    expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","    pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","    pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","    x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","    x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","    \n","    return x\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1684579891458,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"uLUVOczxnFax"},"outputs":[],"source":["config[\"model\"][\"hidden_split\"] = (128,64)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":15327,"status":"ok","timestamp":1684579906766,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"SxOl8YuW6f71"},"outputs":[],"source":["encoder = VideoEncoder(ae.encoder, config[\"model\"][\"embedding_size\"]).to(device)\n","\n","encoder.default_cfg = _cfg()\n","\n","classifier = SGNClassifier(\n","    num_classes=len(class_names),\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n",").to(device)\n","\n","decoder = VideoDecoder(ae.decoder,ae.encoder,ae.encoder_to_decoder,config[\"model\"][\"embedding_size\"]).to(device)\n","\n","model = EncDecModel(\n","    encoder = encoder,\n","    decoder = decoder,\n","    classifier = classifier\n",").to(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684579909951,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"RETBAwj46i3Z"},"outputs":[],"source":["model.to(device);"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":671,"status":"ok","timestamp":1684579910618,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"8t79v6xM6wx7"},"outputs":[],"source":["label_map = [(k,v) for k,v in id2clsname.items()]\n","labelToId = {x[0]: i for i, x in enumerate(label_map)}"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684579910618,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"Mfa8NjY57En3"},"outputs":[],"source":["def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n","    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n","    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n","    loss =  recon_loss + tar_loss\n","\n","    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n","    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n","\n","    return loss, {\n","        \"reconstruction_loss\":recon_loss.item(),\n","        \"target_loss\":tar_loss.item()\n","    }\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684579910619,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"brTkTW9BEa3i"},"outputs":[],"source":["optimizer = torch.optim.Adam([\n","                {'params':model.encoder.map_unit.parameters(),'lr': 1e-3},\n","                {'params': model.classifier.parameters(), 'lr': 1e-3},\n","                {'params': model.decoder.map_unit.parameters(),'lr': 1e-3}\n","            ], \n","    lr=config[\"lr\"], \n","    weight_decay=0.01)\n","std_loss = {\n","    \"reconstruction_loss\" :nn.MSELoss(),\n","    \"target_loss\" :nn.CrossEntropyLoss()\n","}"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684579910619,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"RUigg8dNcWJO"},"outputs":[],"source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=4)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1684579910620,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"DksWHRbgDgPQ"},"outputs":[],"source":["from einops import rearrange\n","\n","def train_step(model, dataloader, optimizer, loss_module, device, class_names):\n","    model = model.train()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    targets = []\n","    predicts = []\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=\"train\") as tepoch:\n","          for input_sequence, target_action, id, chunk_nb, split_nb in tepoch:\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = rearrange(\n","              input_sequence, \n","              'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)', \n","              p0=2, \n","              p1=16, \n","              p2=16).to(device)\n","            target_action = target_action.to(device)\n","            \n","\n","            # Zero gradients, perform a backward pass, and update the weights.\n","            optimizer.zero_grad()\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(True):\n","            # with autocast():\n","              predicted_sequence, _, predicted_label  = model(input_sequence)\n","            \n","            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n","            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n","            loss =  (1-config['alpha_target'])*recon_loss + config['alpha_target']*tar_loss\n","            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n","\n","            class_output = torch.argmax(predicted_label,dim=1)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            metrics = {\"loss\": loss.item()}\n","            with torch.no_grad():\n","                total_samples += len(target_action)\n","                epoch_loss += loss.item()  # add total loss of batch\n","\n","            # convert feature vector into action class using cosine\n","            pred_class = class_output.cpu().detach().numpy()\n","            metrics[\"accuracy\"] = accuracy_score(y_true=target_action.cpu().detach().numpy(), y_pred=pred_class)\n","            tepoch.set_postfix(metrics)\n","\n","            targets.append(target_action.cpu().detach().numpy())\n","            predicts.append(pred_class)\n","\n","    \n","    predicts = np.concatenate(predicts)\n","    targets = np.concatenate(targets)\n","    #train_metrics = action_evaluator(predicts,targets,class_names=list(clsname2id.keys()),print_report=False)\n","\n","    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n","    return metrics"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1684579910620,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"TmXFlJJdEN9N"},"outputs":[],"source":["def eval_step(model, dataloader,loss_module, device, class_names,  print_report=False, show_plot=False):\n","    model = model.eval()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    per_batch = {'targets': [], 'predictions': [], 'metrics': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n","\n","    with torch.no_grad():\n","      with tqdm(dataloader, unit=\"batch\", desc=\"eval\") as tepoch:\n","        for input_sequence, target_action, id, chunk_nb, split_nb in tepoch:\n","\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = rearrange(\n","              input_sequence, \n","              'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)', \n","              p0=2, \n","              p1=16, \n","              p2=16).to(device)\n","            target_action = target_action.to(device)\n","\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(False):\n","            # with autocast():\n","                predicted_sequence,_,predicted_label  = model(input_sequence)\n","            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n","            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n","            loss =  (1-config['alpha_target'])*recon_loss + config['alpha_target']*tar_loss\n","            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n","            \n","            pred_action = torch.argmax(predicted_label,dim=1)\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(target_action)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += loss_detail[\"reconstruction_loss\"]\n","                metrics['classi. loss'] += loss_detail[\"target_loss\"]\n","\n","            per_batch['targets'].append(target_action.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n","    metrics_dict.update(metrics)\n","    return metrics_dict"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684579910620,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"6QMFOxCZE0T2"},"outputs":[],"source":["def log(fold, phase, metrics):\n","    for m, v in metrics.items():\n","        if fold == 'global':\n","            run[f'global/{m}'].log(v)\n","        else:\n","            run[f\"Fold-{fold}/{phase}/{m}\"].log(v)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1684579910621,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"gEfzDnIP81qB"},"outputs":[],"source":["start_epoch = 0\n","#model_params, model_config, config = get_config(f\"{models_saves}/temp_{model_ident}/{start_epoch}__{unique_iden}.pt\")\n","#model.load_state_dict(model_params)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["452aab9890df425a84b92304a02cda34","1976fbecc35341dbb6a785e87dc731a8","dd3a8a4a7b8342b897f0a2f2e381766a","7f37bd0252394f4d8c00356e301fd7bc","5a87ca092cc5485a8386e6df84296235","e3200bb6f43a429cb20aa99f41cfd4e9","34e579b4bebc406cb5e6ff93f609a9e3","1037b028381f46b497b2909f843abf42","e65811ac7aab4693b5a342490c864dce","80f23940a6324676bee9ef60cf1c362a","74b31c80476648308956b8089ef60473","baf9756108fb4071b4b581a62170a33b","e6260ee12b584554b381b60b1a834161","970d64ab740747558a04fb29b8b31af4","ac068a00a96f480db20cbaa769587186","97d2e7a2cf7d4a079ee4e45431da1150","9930888261f24609a6d91c8297ac46a4","5e41cad23d1840d7ab59b4b98d53f1c6","b277538f9ee44c73a910d8f2ebac1f06","0aa6175cc64c4438a65c6c595247035a","11f1419a443c468fb70eef44e54fc62a","90093b67fbca43b59ec87ee83b6788a2"]},"id":"wyUEFUlp7HOt","outputId":"ed0b85cc-f47a-41b1-b65f-b9a49fc51e4d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87f4656d555f43109d5095fafc21f47e","version_major":2,"version_minor":0},"text/plain":["Training Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58e31fb8407445a5abd8da4a6d643998","version_major":2,"version_minor":0},"text/plain":["train:   0%|          | 0/3033 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [3, 17, 224, 224] at entry 0 and [3, 16, 224, 224] at entry 1","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_data \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(start_epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, start_epoch\u001b[39m+\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epoch\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m   train_metrics \u001b[39m=\u001b[39m train_step(model, train_dl, optimizer, std_loss, device, class_names)\n\u001b[0;32m     11\u001b[0m   train_metrics[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m epoch\n\u001b[0;32m     12\u001b[0m   train_metrics[\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\n","Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, optimizer, loss_module, device, class_names)\u001b[0m\n\u001b[0;32m      8\u001b[0m predicts \u001b[39m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(dataloader, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m---> 11\u001b[0m       \u001b[39mfor\u001b[39;00m input_sequence, target_action, \u001b[39mid\u001b[39m, chunk_nb, split_nb \u001b[39min\u001b[39;00m tepoch:\n\u001b[0;32m     12\u001b[0m         input_sequence \u001b[39m=\u001b[39m input_sequence\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m         target_sequence \u001b[39m=\u001b[39m rearrange(\n\u001b[0;32m     14\u001b[0m           input_sequence, \n\u001b[0;32m     15\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mb c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m           p0\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[0;32m     17\u001b[0m           p1\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \n\u001b[0;32m     18\u001b[0m           p2\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\u001b[39m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n","File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n","\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 17, 224, 224] at entry 0 and [3, 16, 224, 224] at entry 1"]},{"name":"stdout","output_type":"stream","text":["Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: HTTPBadGateway\n","Communication with Neptune restored!\n"]},{"name":"stderr","output_type":"stream","text":["Exception in thread NeptuneWebhooks:\n","Traceback (most recent call last):\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_socket.py\", line 108, in recv\n","    bytes_ = _recv()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_socket.py\", line 87, in _recv\n","    return sock.recv(bufsize)\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\ssl.py\", line 1226, in recv\n","    return self.read(buflen)\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\ssl.py\", line 1101, in read\n","    return self._sslobj.read(len)\n","TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\threading.py\", line 932, in _bootstrap_inner\n","    self.run()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\internal\\threading\\daemon.py\", line 53, in run\n","    self.work()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\internal\\websockets\\websocket_signals_background_job.py\", line 79, in work\n","    raw_message = self._ws_client.recv()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\common\\websockets\\reconnecting_websocket.py\", line 51, in recv\n","    data = self.client.recv()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\neptune\\common\\websockets\\websocket_client_adapter.py\", line 63, in recv\n","    opcode, data = self._ws_client.recv_data()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_core.py\", line 385, in recv_data\n","    opcode, frame = self.recv_data_frame(control_frame)\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_core.py\", line 406, in recv_data_frame\n","    frame = self.recv_frame()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_core.py\", line 445, in recv_frame\n","    return self.frame_buffer.recv_frame()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_abnf.py\", line 338, in recv_frame\n","    self.recv_header()\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_abnf.py\", line 294, in recv_header\n","    header = self.recv_strict(2)\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_abnf.py\", line 373, in recv_strict\n","    bytes_ = self.recv(min(16384, shortage))\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_core.py\", line 529, in _recv\n","    return recv(self.sock, bufsize)\n","  File \"c:\\Users\\User\\miniconda3\\envs\\fyp\\lib\\site-packages\\websocket\\_socket.py\", line 110, in recv\n","    raise WebSocketTimeoutException(\"Connection timed out\")\n","websocket._exceptions.WebSocketTimeoutException: Connection timed out\n"]}],"source":["from tqdm.autonotebook import tqdm\n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_acc = 0.0\n","show_interval = 10\n","\n","train_data = []\n","for epoch in tqdm(range(start_epoch+1, start_epoch+config[\"n_epochs\"] + 1), desc='Training Epoch', leave=False):\n","  \n","  train_metrics = train_step(model, train_dl, optimizer, std_loss, device, class_names)\n","  train_metrics['epoch'] = epoch\n","  train_metrics['phase'] = 'train'\n","  train_data.append(train_metrics)\n","  log(epoch, 'train', train_metrics)\n","  \n","  if epoch % 10 == 0:\n","    eval_metrics = eval_step(model, val_dl,std_loss, device, class_names,  print_report=True, show_plot=True)\n","  else:\n","    eval_metrics = eval_step(model, val_dl,std_loss, device, class_names,  print_report=False, show_plot=False)\n","  eval_metrics['epoch'] = epoch \n","  eval_metrics['phase'] = 'valid'\n","  train_data.append(eval_metrics)\n","  log(epoch, 'valid', eval_metrics)\n","  \n","  save_model(\n","      model, \n","      f\"temp_{model_ident}\", \n","      f\"{epoch}__{unique_iden}\",\n","      models_saves, \n","      config)\n","    \n","  if eval_metrics['accuracy'] > best_acc:\n","    best_model = copy.deepcopy(model.state_dict())\n","  \n","train_df = pd.DataFrame().from_records(train_data)\n","plot_curves(train_df)\n","\n","# replace by best model \n","model.load_state_dict(best_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0aa6175cc64c4438a65c6c595247035a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1037b028381f46b497b2909f843abf42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f1419a443c468fb70eef44e54fc62a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1976fbecc35341dbb6a785e87dc731a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3200bb6f43a429cb20aa99f41cfd4e9","placeholder":"​","style":"IPY_MODEL_34e579b4bebc406cb5e6ff93f609a9e3","value":"Training Epoch:   0%"}},"34e579b4bebc406cb5e6ff93f609a9e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"452aab9890df425a84b92304a02cda34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1976fbecc35341dbb6a785e87dc731a8","IPY_MODEL_dd3a8a4a7b8342b897f0a2f2e381766a","IPY_MODEL_7f37bd0252394f4d8c00356e301fd7bc"],"layout":"IPY_MODEL_5a87ca092cc5485a8386e6df84296235"}},"5a87ca092cc5485a8386e6df84296235":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e41cad23d1840d7ab59b4b98d53f1c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74b31c80476648308956b8089ef60473":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f37bd0252394f4d8c00356e301fd7bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80f23940a6324676bee9ef60cf1c362a","placeholder":"​","style":"IPY_MODEL_74b31c80476648308956b8089ef60473","value":" 0/20 [00:00&lt;?, ?it/s]"}},"80f23940a6324676bee9ef60cf1c362a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90093b67fbca43b59ec87ee83b6788a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"970d64ab740747558a04fb29b8b31af4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b277538f9ee44c73a910d8f2ebac1f06","max":1697,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0aa6175cc64c4438a65c6c595247035a","value":0}},"97d2e7a2cf7d4a079ee4e45431da1150":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9930888261f24609a6d91c8297ac46a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac068a00a96f480db20cbaa769587186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11f1419a443c468fb70eef44e54fc62a","placeholder":"​","style":"IPY_MODEL_90093b67fbca43b59ec87ee83b6788a2","value":" 0/1697 [00:26&lt;?, ?batch/s, loss=1.74, accuracy=0.125]"}},"b277538f9ee44c73a910d8f2ebac1f06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"baf9756108fb4071b4b581a62170a33b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6260ee12b584554b381b60b1a834161","IPY_MODEL_970d64ab740747558a04fb29b8b31af4","IPY_MODEL_ac068a00a96f480db20cbaa769587186"],"layout":"IPY_MODEL_97d2e7a2cf7d4a079ee4e45431da1150"}},"dd3a8a4a7b8342b897f0a2f2e381766a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1037b028381f46b497b2909f843abf42","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e65811ac7aab4693b5a342490c864dce","value":0}},"e3200bb6f43a429cb20aa99f41cfd4e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6260ee12b584554b381b60b1a834161":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9930888261f24609a6d91c8297ac46a4","placeholder":"​","style":"IPY_MODEL_5e41cad23d1840d7ab59b4b98d53f1c6","value":"train:   0%"}},"e65811ac7aab4693b5a342490c864dce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
