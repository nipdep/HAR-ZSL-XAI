{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794b0821-b1cf-4224-8aa6-ea44970abe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.analysis import action_evaluator\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba7eddb-7bff-4d14-9b9f-b0d86bfb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ident = \"bidiretional_lstm_hrnet_nturgb_classifier\"\n",
    "unique_iden = \"epoch10_emb1024xy\"\n",
    "\n",
    "main_dir = \"D:\\\\FYP\\\\HAR-ZSL-XAI\"\n",
    "data_dir = os.path.join(main_dir,\"data\",\"sequence_data\",\"midpoint_50f\")\n",
    "epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n",
    "models_saves = os.path.join(main_dir,\"model_saves\")\n",
    "embeddings_save = os.path.join(main_dir,\"embedding_save\")\n",
    "test_vids = os.path.join(main_dir,\"test_vids\")\n",
    "class_names = os.listdir(data_dir)\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.1\n",
    "test_ratio = 1-train_ratio - val_ratio\n",
    "batch_size = 128\n",
    "\n",
    "os.makedirs(epoch_vids,exist_ok=True)\n",
    "os.makedirs(models_saves,exist_ok=True)\n",
    "os.makedirs(embeddings_save,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open(\"../data/sequence_data/metadata/midpoint_50f/class_data.json\",\"r\") as f0:\n",
    "    loaded_class_data = json.load(f0)\n",
    "class_names = loaded_class_data[\"selected_classes\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#class_names += os.listdir(\"../data/nipun_video_dataset/PAMAP2_K10_V1\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_epochs\":100,\n",
    "    \"model_name\":\"BidirectionalLSTM\",\n",
    "    \"model\":{\n",
    "        \"seq_len\":50,\n",
    "        \"input_size\":12*2,\n",
    "        \"hidden_size\":1024,\n",
    "        \"linear_filters\":[128,256,512,1024],\n",
    "        \"embedding_size\":1024,\n",
    "        \"num_classes\":len(class_names),\n",
    "        \"num_layers\":1,\n",
    "        \"bidirectional\":True,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"dev\":device\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27b84c1-0510-42f6-b53e-503875da2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f164bb-3aa7-450b-b2b7-3f11a055c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = []\n",
    "val_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "file_list = [os.path.join(data_dir,x) for x in os.listdir(data_dir)]\n",
    "\n",
    "random.shuffle(file_list)\n",
    "num_list = len(file_list)\n",
    "\n",
    "train_range = [0,int(num_list*train_ratio)]\n",
    "val_range = [int(num_list*train_ratio),int(num_list*(train_ratio+val_ratio))]\n",
    "test_range = [int(num_list*(train_ratio+val_ratio)),num_list-1]\n",
    "\n",
    "train_file_list += file_list[train_range[0]:train_range[1]]\n",
    "val_file_list += file_list[val_range[0]:val_range[1]]\n",
    "test_file_list += file_list[test_range[0]:test_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9f2788-0c6f-414a-a65e-eab0956e5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(40320, 4480, 0)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca03ac0-3133-4e69-9c4c-7e3c346cae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = train_file_list[:(len(train_file_list)//batch_size)*batch_size]\n",
    "val_file_list = val_file_list[:(len(val_file_list)//batch_size)*batch_size]\n",
    "test_file_list = test_file_list[:(len(test_file_list)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f014d9f-b951-481e-af46-a677cb0e2c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(40320, 4480, 0)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea002f29-1936-46b1-a35f-7dc62c439642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, file_list,class2id,transform=None,\n",
    "                 target_transform=None,active_locations=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],file_name=False, is_2d=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class2id = class2id\n",
    "        self.target_transform = target_transform\n",
    "        self.active_locations = active_locations\n",
    "        self.file_name = file_name\n",
    "        self.is_2d = is_2d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_file = np.load(self.file_list[idx])\n",
    "        action_type = self.file_list[idx].strip().split(os.path.sep)[-1].split(\"_cls_\")[0]\n",
    "        coords, vid_size = a_file[\"coords\"],a_file[\"video_size\"]\n",
    "        coords = coords[:,self.active_locations,:]\n",
    "\n",
    "        if self.is_2d:\n",
    "            coords = coords[...,0:2]\n",
    "\n",
    "        shape = coords.shape\n",
    "\n",
    "        coords = torch.from_numpy(coords).float()\n",
    "\n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "\n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "\n",
    "        if self.file_name:\n",
    "            return coords, label, self.class2id[action_type],a_file[\"video_size\"],self.file_list[idx]\n",
    "        return coords, label, self.class2id[action_type],a_file[\"video_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0bfa6f8-44df-49ac-901c-1835612db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SkeletonDataset(train_file_list,clsname2id,is_2d=True)\n",
    "val_data = SkeletonDataset(val_file_list,clsname2id,is_2d=True)\n",
    "test_data = SkeletonDataset(test_file_list,clsname2id,is_2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f2a581-485c-4ac9-836a-86f74ec4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50, 24]) torch.Size([128, 50, 24]) tensor([47, 41, 50,  1, 56, 28, 61,  6, 59, 51, 31, 26, 45, 51, 46, 61, 49, 10,\n",
      "        49, 42, 43, 41, 41, 34, 21, 47, 35, 16,  9, 46, 52, 35, 18, 39, 19, 46,\n",
      "        28, 28, 26, 47, 16, 62, 48, 24, 31, 36, 20, 29, 42, 26, 63, 10, 16, 43,\n",
      "        19, 28,  6,  0, 33, 49, 29, 19, 12, 49, 58,  0, 45,  7, 38, 15,  1, 31,\n",
      "         6, 39, 22, 61, 52, 60, 56,  8, 50, 21,  3, 49, 50, 55, 26, 45, 51, 18,\n",
      "        32, 62, 63, 17, 30, 18, 49, 12,  5, 29, 15,  2, 41, 59,  7, 17, 58,  1,\n",
      "        49, 41, 42, 27, 48, 44, 51, 48, 37, 63, 25, 24, 31, 53, 40, 32, 46, 28,\n",
      "        23, 60])\n"
     ]
    }
   ],
   "source": [
    "for x in train_dl:\n",
    "    print(x[0].shape,x[1].shape,x[2])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b980c5a-2d57-4c0f-bcea-8bd2bbab1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size,num_classes, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev=dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        self.classification_header = nn.Linear(self.embedding_size,self.num_classes)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.bn = nn.BatchNorm1d(self.hidden_size*4)\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm1d(self.hidden_size*2)\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        \"\"\"\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.cat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "\n",
    "        #hidden_transformed = self.bn(hidden_transformed)\n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "\n",
    "        label = self.classification_header(hidden_transformed)\n",
    "        \n",
    "        return label, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev = dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        \"\"\"\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "            bs = h.size()[1]\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "            bs = h.size()[1]\n",
    "        \n",
    "        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,num_classes, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev = dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes= num_classes\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, num_classes,hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        label,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1910a4-fa78-4ee0-86bd-f7d9d6e559af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BiLSTMEncoder(\n",
    "    seq_len=config[\"model\"][\"seq_len\"],\n",
    "    input_size=config[\"model\"][\"input_size\"],\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
    "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
    "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
    "    num_layers = config[\"model\"][\"num_layers\"],\n",
    "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
    "    dev=config[\"model\"][\"dev\"]).to(device)\n",
    "\n",
    "decoder = BiLSTMDecoder(\n",
    "    seq_len=config[\"model\"][\"seq_len\"],\n",
    "    input_size=config[\"model\"][\"input_size\"],\n",
    "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
    "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
    "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
    "    num_layers = config[\"model\"][\"num_layers\"],\n",
    "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
    "    dev=config[\"model\"][\"dev\"]).to(device)\n",
    "\n",
    "bilstm_model = BiLSTMEncDecModel(\n",
    "    seq_len=config[\"model\"][\"seq_len\"],\n",
    "    input_size=config[\"model\"][\"input_size\"],\n",
    "    num_classes = config[\"model\"][\"num_classes\"],\n",
    "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
    "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
    "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
    "    num_layers = config[\"model\"][\"num_layers\"],\n",
    "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
    "    dev=config[\"model\"][\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "BiLSTMEncDecModel(\n  (encoder): BiLSTMEncoder(\n    (lstm): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n    (net): Sequential(\n      (0): Linear(in_features=24, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=512, bias=True)\n      (3): Linear(in_features=512, out_features=1024, bias=True)\n    )\n    (classification_header): Linear(in_features=1024, out_features=64, bias=True)\n    (bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (out_linear): Linear(in_features=4096, out_features=1024, bias=True)\n  )\n  (decoder): BiLSTMDecoder(\n    (input_linear): Linear(in_features=1024, out_features=4096, bias=True)\n    (lstm): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n    (net): Sequential(\n      (0): Linear(in_features=2048, out_features=1024, bias=True)\n      (1): Linear(in_features=1024, out_features=512, bias=True)\n      (2): Linear(in_features=512, out_features=256, bias=True)\n      (3): Linear(in_features=256, out_features=128, bias=True)\n      (4): Linear(in_features=128, out_features=24, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da6f836f-4250-4abe-b392-71d7340e4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label, embedding = encoder(torch.randn((32,50,24)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55f28e99-e59d-4b68-a1f4-41836e81d2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1024])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 64])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b673418b-ab02-4b77-8fb6-ee5f87b43225",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out = decoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71696fa1-687e-431b-85fa-0f0e4db2dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out,embedding,label = bilstm_model(torch.randn((32,50,24)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a443bd3-6669-4017-828c-d2df95e4cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 50, 24])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "model_out,embedding,label = bilstm_model(torch.randn((16,50,24)).to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 50, 24])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model_out,embedding,label = bilstm_model(torch.randn((5,50,24)).to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 50, 24])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74eac088",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mapping_l = [\n",
    "        [15, 13], [13, 11], [11, 5],\n",
    "        [12, 14], [14, 16], [12, 6],\n",
    "        [3, 1],[1, 2],[1, 0],[0, 2],[2,4],\n",
    "        [9, 7], [7,5], [5, 6],\n",
    "        [6, 8], [8, 10],\n",
    "        ]\n",
    "#mapping_l = []\n",
    "\n",
    "from dataset.SkeletonData.visualize import *\n",
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "label_map = [(k,v) for k,v in id2clsname.items()]\n",
    "labelToId = {x[0]: i for i, x in enumerate(label_map)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n",
    "    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n",
    "    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n",
    "    loss =  recon_loss + tar_loss\n",
    "\n",
    "    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n",
    "    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n",
    "\n",
    "    return loss, {\n",
    "        \"reconstruction_loss\":recon_loss.item(),\n",
    "        \"target_loss\":tar_loss.item()\n",
    "    }\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcc0d9c3-9abd-4897-b67a-6f3958b69899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(__model, train_dataset, val_dataset, n_epochs):\n",
    "    optimizer = torch.optim.Adam(__model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    std_loss = {\n",
    "        \"reconstruction_loss\" :nn.L1Loss(reduction='mean').to(device),\n",
    "        \"target_loss\" :nn.CrossEntropyLoss(reduction=\"mean\").to(device)\n",
    "    }\n",
    "    train_history = dict(train=[], val=[],train_detail=[],val_detail=[])\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(__model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        __model = __model.train()\n",
    "\n",
    "        train_pred_class = []\n",
    "        train_true_class = []\n",
    "        val_pred_class = []\n",
    "        val_true_class = []\n",
    "\n",
    "        train_losses = []\n",
    "        train_loss_detail = []\n",
    "        for input_sequence,target_sequence,target_action,target_vid_size in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_sequence = input_sequence.to(device)\n",
    "            target_sequence = target_sequence.to(device)\n",
    "            target_action = target_action.to(device)\n",
    "            predicted_sequence,_,predicted_label  = __model(input_sequence)\n",
    "            \n",
    "            loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n",
    "            #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "            #print(contrastive_loss(embed,labels=sample_label.view(-1)))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_loss_detail.append(loss_detail)\n",
    "\n",
    "            train_true_class.append(target_action.detach().cpu().numpy())\n",
    "            train_pred_class.append(predicted_label.detach().cpu().numpy())\n",
    "\n",
    "        train_pred_class = np.concatenate(train_pred_class)\n",
    "        train_true_class = np.concatenate(train_true_class)\n",
    "        #train_metrics = action_evaluator(train_pred_class,train_true_class,class_names=[labelToId[i] for i in range(138)],print_report=True)\n",
    "\n",
    "        val_losses = []\n",
    "        val_loss_detail = []\n",
    "        __model = __model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_sequence,target_sequence,target_action,target_vid_size in val_dataset:\n",
    "\n",
    "                input_sequence = input_sequence.to(device)\n",
    "                target_sequence = target_sequence.to(device)\n",
    "                target_action = target_action.to(device)\n",
    "                predicted_sequence,_,predicted_label  = __model(input_sequence)\n",
    "\n",
    "                loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n",
    "                #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "                val_loss_detail.append(loss_detail)\n",
    "\n",
    "                val_true_class.append(target_action.detach().cpu().numpy())\n",
    "                val_pred_class.extend(predicted_label.detach().cpu().numpy())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        val_pred_class = np.concatenate(val_pred_class)\n",
    "        val_true_class = np.concatenate(val_true_class)\n",
    "\n",
    "        #val_metrics = action_evaluator(val_pred_class,val_true_class,class_names=[labelToId[i] for i in range(138)],print_report=True)\n",
    "\n",
    "        train_history['train'].append(train_loss)\n",
    "        train_history['val'].append(val_loss)\n",
    "\n",
    "        summary_train_reconstruction_loss = np.mean([x[\"reconstruction_loss\"] for x in train_loss_detail])\n",
    "        summary_train_target_loss = np.mean([x[\"target_loss\"] for x in train_loss_detail])\n",
    "        train_history[\"train_detail\"].append({\n",
    "            \"reconstruction_loss\":summary_train_reconstruction_loss,\n",
    "             \"target_loss\":summary_train_target_loss\n",
    "        })\n",
    "\n",
    "        summary_val_reconstruction_loss = np.mean([x[\"reconstruction_loss\"] for x in val_loss_detail])\n",
    "        summary_val_target_loss = np.mean([x[\"target_loss\"] for x in val_loss_detail])\n",
    "        train_history[\"train_detail\"].append({\n",
    "            \"reconstruction_loss\":summary_val_reconstruction_loss,\n",
    "             \"target_loss\":summary_val_target_loss\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            save_model(__model, f\"temp_{model_ident}\", f\"{epoch}__{unique_iden}\", models_saves, config)\n",
    "\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(__model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss} \\n train_reconstruction_loss:- {summary_train_reconstruction_loss} train_target_loss {summary_train_target_loss} val_reconstruction_loss:- {summary_val_reconstruction_loss} val_target_loss {summary_val_target_loss}')\n",
    "\n",
    "    __model.load_state_dict(best_model_wts)\n",
    "    save_model(__model, model_ident, unique_iden, models_saves, config)\n",
    "    return __model.eval(), train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8151f9-65c0-40d2-a99e-6787bdf5e8a5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [07:46<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 4.233817763555617 val loss 4.2195420128958565 \n",
      " train_reconstruction_loss:- 0.07430449044184079 train_target_loss 4.159513278234573 val_reconstruction_loss:- 0.0602544761129788 val_target_loss 4.159287548065185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [15:30<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss 4.221310618567088 val loss 4.219556862967355 \n",
      " train_reconstruction_loss:- 0.06215940867624586 train_target_loss 4.159151195344471 val_reconstruction_loss:- 0.05998260442699705 val_target_loss 4.159574249812535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [15:28<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss 4.220255935002887 val loss 4.2191539628165105 \n",
      " train_reconstruction_loss:- 0.061124413531451 train_target_loss 4.1591315269470215 val_reconstruction_loss:- 0.05943522708756583 val_target_loss 4.159718758719308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [05:33<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss 4.2196679357498414 val loss 4.2190557479858395 \n",
      " train_reconstruction_loss:- 0.06054057340536799 train_target_loss 4.159127356514098 val_reconstruction_loss:- 0.05926660437669073 val_target_loss 4.159789167131696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [04:47<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss 4.219388575780959 val loss 4.219202123369489 \n",
      " train_reconstruction_loss:- 0.06026205781196791 train_target_loss 4.159126522427513 val_reconstruction_loss:- 0.05937908004437174 val_target_loss 4.159823063441685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [04:37<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss 4.219283290136428 val loss 4.219137191772461 \n",
      " train_reconstruction_loss:- 0.06015680077530089 train_target_loss 4.159126484583295 val_reconstruction_loss:- 0.059297758340835574 val_target_loss 4.15983943939209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [04:34<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss 4.218931193578811 val loss 4.218658774239676 \n",
      " train_reconstruction_loss:- 0.05980466211599017 train_target_loss 4.159126531510126 val_reconstruction_loss:- 0.05881158464721271 val_target_loss 4.159847218649728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [04:35<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss 4.218737357003349 val loss 4.21892181124006 \n",
      " train_reconstruction_loss:- 0.05961042424989125 train_target_loss 4.1591269296313085 val_reconstruction_loss:- 0.05907228301678385 val_target_loss 4.159849493844169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [23:05<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss 4.2184942866128585 val loss 4.218587521144322 \n",
      " train_reconstruction_loss:- 0.05936677375483134 train_target_loss 4.159127515459818 val_reconstruction_loss:- 0.0587372040109975 val_target_loss 4.159850311279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [38:56<00:00,  7.42s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss 4.218494030785939 val loss 4.218569346836635 \n",
      " train_reconstruction_loss:- 0.05936612847542006 train_target_loss 4.159127907525925 val_reconstruction_loss:- 0.05871912155832563 val_target_loss 4.159850215911865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [12:10<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss 4.218497120387971 val loss 4.218624918801444 \n",
      " train_reconstruction_loss:- 0.05936988031580335 train_target_loss 4.159127242981442 val_reconstruction_loss:- 0.0587751461991242 val_target_loss 4.159849779946463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [14:14<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss 4.218505826072088 val loss 4.218696090153285 \n",
      " train_reconstruction_loss:- 0.059378076593081155 train_target_loss 4.1591277531215125 val_reconstruction_loss:- 0.0588476656803063 val_target_loss 4.159848417554583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [14:04<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss 4.218511773669531 val loss 4.218755231584821 \n",
      " train_reconstruction_loss:- 0.05938231617448822 train_target_loss 4.159129440973675 val_reconstruction_loss:- 0.05890446039182799 val_target_loss 4.159850788116455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [15:28<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss 4.218517206585596 val loss 4.218746893746513 \n",
      " train_reconstruction_loss:- 0.0593896814755031 train_target_loss 4.159127516973586 val_reconstruction_loss:- 0.0588949516415596 val_target_loss 4.159851946149554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [15:15<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss 4.218521419404045 val loss 4.218726594107491 \n",
      " train_reconstruction_loss:- 0.059394634751573444 train_target_loss 4.159126791878352 val_reconstruction_loss:- 0.05887394472956657 val_target_loss 4.159852681841169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [14:20<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train loss 4.218530815366715 val loss 4.218708447047642 \n",
      " train_reconstruction_loss:- 0.05940410529100706 train_target_loss 4.159126713162377 val_reconstruction_loss:- 0.058854177062000544 val_target_loss 4.159854234967913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [14:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss 4.218537681821793 val loss 4.218701280866351 \n",
      " train_reconstruction_loss:- 0.0594117571673696 train_target_loss 4.159125919947549 val_reconstruction_loss:- 0.058844709715672905 val_target_loss 4.159856591905866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [15:19<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss 4.218545468648275 val loss 4.218699645996094 \n",
      " train_reconstruction_loss:- 0.0594184992923623 train_target_loss 4.159126956879146 val_reconstruction_loss:- 0.058843248869691576 val_target_loss 4.159856442042759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 149/315 [49:07<12:16:29, 266.20s/it]"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  bilstm_model, \n",
    "  train_dl, \n",
    "  val_dl, \n",
    "  n_epochs=config[\"n_epochs\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_history(history,model_ident,unique_iden,models_saves,config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924402fb-167c-42cf-b6bc-db7c196a27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "\n",
    "ax.plot(history['train'])\n",
    "ax.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c61a5-2d32-4151-9190-98af5bd3621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = {}\n",
    "output_list = {}\n",
    "with torch.no_grad():\n",
    "    for in_sequence,tar_sequence,action,vid_size in tqdm(test_dl):\n",
    "        in_sequence = in_sequence.to(device)\n",
    "        tar_sequence = tar_sequence.to(device)\n",
    "        seq_pred,embedding  = model(in_sequence)\n",
    "\n",
    "        for seq,emb,action_t in zip(seq_pred.unbind(0),embedding.unbind(0),action.unbind(0)):\n",
    "                try:\n",
    "                    if len(embedding_list[int(action_t)])<=50:\n",
    "                        embedding_list[int(action_t)].append(emb)\n",
    "                        output_list[int(action_t)].append(seq)\n",
    "                except KeyError:\n",
    "                    embedding_list[int(action_t)] = [emb]\n",
    "                    output_list[int(action_t)] = [emb]\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "        #embedding_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def draw_heatmaps(arr_list,nrows=2,ncols=2):\n",
    "    ran_list = random.sample(arr_list,ncols*nrows)\n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, sharex=True)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            #print(i*ncols+j,len(ran_list))\n",
    "            ax[i,j].imshow(ran_list[i*ncols+j].detach().cpu().numpy()[np.newaxis,:], cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba50071",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmaps(embedding_list[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmaps(embedding_list[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmaps(embedding_list[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_video_from_embeddings(embedding,model,save_file):\n",
    "    seq_out = model.decoder(embedding.repeat(batch_size,1,1))\n",
    "    gen_video(seq_out[0].detach().numpy(), save_file, 400, 400,mapping_list=mapping_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for __id,(in_seq,tar_seq,action,vid_size) in tqdm(enumerate(test_dl)):\n",
    "        in_seq = in_seq.to(device)\n",
    "        tar_seq = tar_seq.to(device)\n",
    "        seq_pred,embedding  = model(in_seq)\n",
    "\n",
    "        for __id,(input_vid,output_vid,action) in enumerate(zip(in_seq.unbind(0),seq_pred.unbind(0),action.unbind(0))):\n",
    "            os.makedirs(f\"{test_vids}/{int(action)}\",exist_ok=True)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "        #embedding_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dfd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_video_from_embeddings(embedding_list[1][10],model,\"embed_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c296b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = 0.5*embedding_list[19][0]+0.5*embedding_list[16][0]\n",
    "gen_video_from_embeddings(test_emb,model,\"test_embed_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26eb602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mvts_trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
