{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794b0821-b1cf-4224-8aa6-ea44970abe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba7eddb-7bff-4d14-9b9f-b0d86bfb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ident = \"bidiretional_lstm_hrnet_nturgb\"\n",
    "unique_iden = \"epoch10_emb2048_xy\"\n",
    "\n",
    "main_dir = \"D:\\\\FYP\\\\HAR-ZSL-XAI\"\n",
    "data_files = os.path.join(main_dir,\"pyskl\",\"HRNetSkeletons\",\"nipun_video_dataset\",\"PAMAP2_K10_V1\",\"combined.pkl\")\n",
    "epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n",
    "models_saves = os.path.join(main_dir,\"model_saves\")\n",
    "embeddings_save = os.path.join(main_dir,\"embedding_save\")\n",
    "prototypes_save = os.path.join(main_dir,\"prototypes\")\n",
    "test_vids = os.path.join(main_dir,\"test_vids\")\n",
    "batch_size = 32\n",
    "\n",
    "os.makedirs(embeddings_save,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.test_utils import *\n",
    "from dataset.SkeletonData.array_segment import normalize_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_params, model_config, config = get_config(f\"../model_saves/bidiretional_lstm/epoch100_emb1024_xy.pt\")\n",
    "#model_params, model_config, config = get_config(f\"{models_saves}/{model_ident}/{unique_iden}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5f6ab7-fd86-4435-9bec-331d014a6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90625df-343b-4bc3-897d-184be8d8422b",
   "metadata": {},
   "outputs": [],
   "source": [
    " class_names = [str(x) for x in os.listdir(\"../data/nipun_video_dataset/PAMAP2_K10_V1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27b84c1-0510-42f6-b53e-503875da2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f164bb-3aa7-450b-b2b7-3f11a055c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_files,\"rb\") as f0:\n",
    "    filelist = pickle.load(f0)\n",
    "\n",
    "filter_arr = []\n",
    "for __f in filelist:\n",
    "    if len(__f[\"keypoint\"].shape) == 4 and __f[\"keypoint\"].shape[0]>0:\n",
    "        filter_arr.append(__f)\n",
    "\n",
    "num_list = len(filter_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9f2788-0c6f-414a-a65e-eab0956e5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'as10',\n",
       "  'as12',\n",
       "  'as13',\n",
       "  'as14',\n",
       "  'as15',\n",
       "  'as16',\n",
       "  'as18',\n",
       "  'as7',\n",
       "  'as8',\n",
       "  'as9',\n",
       "  'c10',\n",
       "  'c14',\n",
       "  'c16',\n",
       "  'c17',\n",
       "  'c18',\n",
       "  'c2',\n",
       "  'c20',\n",
       "  'c4',\n",
       "  'c6',\n",
       "  'c9',\n",
       "  'cd11',\n",
       "  'cd16',\n",
       "  'cd18',\n",
       "  'cd19',\n",
       "  'cd5',\n",
       "  'cd6',\n",
       "  'cd9',\n",
       "  'cw10',\n",
       "  'cw11',\n",
       "  'cw13',\n",
       "  'cw17',\n",
       "  'cw18',\n",
       "  'cw19',\n",
       "  'cw2',\n",
       "  'cw23',\n",
       "  'cw24',\n",
       "  'cw6',\n",
       "  'ds1',\n",
       "  'ds10',\n",
       "  'ds12',\n",
       "  'ds13',\n",
       "  'ds18',\n",
       "  'ds20',\n",
       "  'ds22',\n",
       "  'ds5',\n",
       "  'ds7',\n",
       "  'ds9',\n",
       "  'fl10',\n",
       "  'fl12',\n",
       "  'fl13',\n",
       "  'fl16',\n",
       "  'fl18',\n",
       "  'fl2',\n",
       "  'fl4',\n",
       "  'fl5',\n",
       "  'fl6',\n",
       "  'fl8',\n",
       "  'hc11',\n",
       "  'hc13',\n",
       "  'hc17',\n",
       "  'hc18',\n",
       "  'hc2',\n",
       "  'hc20',\n",
       "  'hc22',\n",
       "  'hc5',\n",
       "  'hc7',\n",
       "  'hc9',\n",
       "  'i12',\n",
       "  'i13',\n",
       "  'i16',\n",
       "  'i19',\n",
       "  'i21',\n",
       "  'i26',\n",
       "  'i28',\n",
       "  'i32',\n",
       "  'i4',\n",
       "  'i7',\n",
       "  'l11',\n",
       "  'l12',\n",
       "  'l13',\n",
       "  'l16',\n",
       "  'l17',\n",
       "  'l18',\n",
       "  'l20',\n",
       "  'l3',\n",
       "  'l4',\n",
       "  'nw1',\n",
       "  'nw10',\n",
       "  'nw14',\n",
       "  'nw16',\n",
       "  'nw18',\n",
       "  'nw20',\n",
       "  'nw3',\n",
       "  'nw5',\n",
       "  'nw6',\n",
       "  'nw8',\n",
       "  'ps1',\n",
       "  'ps10',\n",
       "  'ps12',\n",
       "  'ps13',\n",
       "  'ps18',\n",
       "  'ps20',\n",
       "  'ps3',\n",
       "  'ps5',\n",
       "  'ps7',\n",
       "  'ps9',\n",
       "  'r1',\n",
       "  'r12',\n",
       "  'r13',\n",
       "  'r15',\n",
       "  'r16',\n",
       "  'r18',\n",
       "  'r20',\n",
       "  'r3',\n",
       "  'r5',\n",
       "  'r9',\n",
       "  'rj1',\n",
       "  'rj13',\n",
       "  'rj14',\n",
       "  'rj15',\n",
       "  'rj18',\n",
       "  'rj20',\n",
       "  'rj3',\n",
       "  'rj5',\n",
       "  'rj6',\n",
       "  'rj9',\n",
       "  's1',\n",
       "  's10',\n",
       "  's11',\n",
       "  's12',\n",
       "  's13',\n",
       "  's15',\n",
       "  's7',\n",
       "  's9',\n",
       "  'sit1',\n",
       "  'sit10',\n",
       "  'sit11',\n",
       "  'sit12',\n",
       "  'sit5',\n",
       "  'sit6',\n",
       "  'sit7',\n",
       "  'sit8',\n",
       "  'sit9',\n",
       "  'vc11',\n",
       "  'vc15',\n",
       "  'vc19',\n",
       "  'vc2',\n",
       "  'vc21',\n",
       "  'vc25',\n",
       "  'vc3',\n",
       "  'vc5',\n",
       "  'vc7',\n",
       "  'vc9',\n",
       "  'w13',\n",
       "  'w15',\n",
       "  'w17',\n",
       "  'w18',\n",
       "  'w20',\n",
       "  'w23',\n",
       "  'w26',\n",
       "  'w5',\n",
       "  'w7',\n",
       "  'w9',\n",
       "  'wt16',\n",
       "  'wt18',\n",
       "  'wt19',\n",
       "  'wt2',\n",
       "  'wt20',\n",
       "  'wt21',\n",
       "  'wt23',\n",
       "  'wt24',\n",
       "  'wt25',\n",
       "  'wt4'},\n",
       " 173)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_fil = set()\n",
    "for fil in filter_arr:\n",
    "    set_fil.add(fil[\"frame_dir\"])\n",
    "\n",
    "set_fil,len(set_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea002f29-1936-46b1-a35f-7dc62c439642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, file_list,class2id, transform=None,\n",
    "                 target_transform=None,active_locations=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],file_name=False, is_2d=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class2id = class2id\n",
    "        self.target_transform = target_transform\n",
    "        self.active_locations = active_locations\n",
    "        self.file_name = file_name\n",
    "        self.is_2d = is_2d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_file = self.file_list[idx]\n",
    "        action_type = a_file[\"label\"]\n",
    "        coords, vidsize = a_file[\"keypoint\"][0],a_file[\"original_shape\"]\n",
    "        coords = coords[10:60,:,:]\n",
    "        coords = coords[:,self.active_locations,:]\n",
    "\n",
    "        normalize_array(coords,vidsize)\n",
    "        if self.is_2d:\n",
    "            coords = coords[...,0:2]\n",
    "\n",
    "        shape = coords.shape\n",
    "\n",
    "        coords = torch.from_numpy(coords).float()\n",
    "\n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "\n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "\n",
    "        if self.file_name:\n",
    "            return coords, label, self.class2id[action_type],a_file[\"original_shape\"],self.file_list[idx][\"frame_dir\"]\n",
    "        return coords, label, self.class2id[action_type],a_file[\"original_shape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0bfa6f8-44df-49ac-901c-1835612db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skel_data = SkeletonDataset(filter_arr,clsname2id,is_2d=True,file_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f2a581-485c-4ac9-836a-86f74ec4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skel_dl = DataLoader(skel_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 24])\n"
     ]
    }
   ],
   "source": [
    "for x in skel_dl:\n",
    "    print(x[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b980c5a-2d57-4c0f-bcea-8bd2bbab1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev=dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "\n",
    "        # add linear layers \n",
    "        for __id,layer_out in enumerate(self.linear_filters):\n",
    "            if __id == 0:\n",
    "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
    "\n",
    "        # add lstm layer\n",
    "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        #add embedding out\n",
    "        if bidirectional:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
    "        else:\n",
    "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        \"\"\"\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.net(x_input)\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        hidden_transformed = torch.cat(self.hidden,0)\n",
    "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
    "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
    "        \n",
    "        hidden_transformed = self.out_linear(hidden_transformed)\n",
    "        \n",
    "        return lstm_out, hidden_transformed\n",
    "\n",
    "    \n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev = dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        if bidirectional:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
    "        else:\n",
    "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.layers = []\n",
    "        # add lstm\n",
    "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
    "                            num_layers = self.num_layers, bidirectional=True,\n",
    "                            batch_first=bidirectional)\n",
    "\n",
    "                        \n",
    "        # add linear layers \n",
    "        if bidirectional:\n",
    "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
    "\n",
    "        for __id,layer_in in enumerate(self.linear_filters):\n",
    "            if __id == len(linear_filters)-1:\n",
    "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,encoder_hidden):\n",
    "        \"\"\"\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        hidden_shape = encoder_hidden.shape\n",
    "        encoder_hidden = self.input_linear(encoder_hidden)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
    "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
    "            bs = h.size()[1]\n",
    "        else:\n",
    "            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n",
    "            hidden = torch.transpose(hidden,1,0)\n",
    "            h,c = torch.unbind(hidden,0)\n",
    "            bs = h.size()[1]\n",
    "        \n",
    "        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
    "        x = self.net(lstm_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BiLSTMEncDecModel(nn.Module):\n",
    "    def __init__(self,seq_len, input_size, hidden_size, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True,dev=device):\n",
    "        super(BiLSTMEncDecModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev = dev\n",
    "        self.num_layers = num_layers\n",
    "        self.linear_filters = linear_filters[::-1]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = BiLSTMEncoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
    "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        lstm_out,embedding = self.encoder(x)\n",
    "        decoder_out = self.decoder(embedding)\n",
    "        \n",
    "        return decoder_out, embedding  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 100,\n",
       " 'model_name': 'BidirectionalLSTM',\n",
       " 'model': {'seq_len': 50,\n",
       "  'input_size': 24,\n",
       "  'hidden_size': 512,\n",
       "  'linear_filters': [128, 256, 512],\n",
       "  'embedding_size': 1024,\n",
       "  'num_layers': 1,\n",
       "  'bidirectional': True,\n",
       "  'batch_size': 32,\n",
       "  'dev': device(type='cuda')}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1910a4-fa78-4ee0-86bd-f7d9d6e559af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model = BiLSTMEncDecModel(\n",
    "    seq_len=config[\"model\"][\"seq_len\"],\n",
    "    input_size=config[\"model\"][\"input_size\"],\n",
    "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
    "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
    "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
    "    num_layers = config[\"model\"][\"num_layers\"],\n",
    "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
    "    dev=config[\"model\"][\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMEncDecModel(\n",
       "  (encoder): BiLSTMEncoder(\n",
       "    (lstm): LSTM(512, 512, batch_first=True, bidirectional=True)\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=128, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "    (out_linear): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoder): BiLSTMDecoder(\n",
       "    (input_linear): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (lstm): LSTM(512, 512, batch_first=True, bidirectional=True)\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da6f836f-4250-4abe-b392-71d7340e4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, embedding = bilstm_model.encoder(torch.randn((32,50,24)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55f28e99-e59d-4b68-a1f4-41836e81d2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b673418b-ab02-4b77-8fb6-ee5f87b43225",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_out = bilstm_model.decoder(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71696fa1-687e-431b-85fa-0f0e4db2dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out,embedding = bilstm_model(torch.randn((1,50,24)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a443bd3-6669-4017-828c-d2df95e4cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 24])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74eac088",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mapping_l = [\n",
    "        [15, 13], [13, 11], [11, 5],\n",
    "        [12, 14], [14, 16], [12, 6],\n",
    "        [3, 1],[1, 2],[1, 0],[0, 2],[2,4],\n",
    "        [9, 7], [7,5], [5, 6],\n",
    "        [6, 8], [8, 10],\n",
    "        ]\n",
    "#mapping_l = []\n",
    "\n",
    "from dataset.SkeletonData.visualize import *\n",
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_predict(model,input_ten:torch.Tensor,dev):\n",
    "    input_ten = input_ten.to(dev)\n",
    "    _, embed_ten = model.encoder(input_ten)\n",
    "    output_ten = model.decoder(embed_ten)\n",
    "\n",
    "    return embed_ten, output_ten\n",
    "\n",
    "def model_saves(loc,embed_ten,output_ten,detail_ten):\n",
    "    np.savez(loc,embedding=embed_ten,output=output_ten,file_list=detail_ten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_map = []\n",
    "embed_map = []\n",
    "output_map = []\n",
    "for data_point in skel_dl:\n",
    "    arrays, labels, class_nms ,video_shape,file_details = data_point\n",
    "    embeddings, outputs = model_predict(bilstm_model,arrays,device)\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    for e,o,d,l,cln in zip(embeddings,outputs,file_details,labels,class_nms):\n",
    "        file_map.append([d])\n",
    "        embed_map.append(e)\n",
    "        output_map.append(o)\n",
    "\n",
    "file_map = np.asarray(file_map)\n",
    "embed_map = np.asarray(embed_map)\n",
    "output_map = np.asarray(output_map)\n",
    "\n",
    "os.makedirs(f\"{embeddings_save}/{model_ident}/{unique_iden}\",exist_ok=True)\n",
    "model_saves(f\"{embeddings_save}/{model_ident}/{unique_iden}/without_pretrain_on_local.npz\",embed_map,output_map,file_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hypertools as hyp\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_data = np.load(f\"{embeddings_save}/{model_ident}/{unique_iden}/without_pretrain_on_local.npz\")\n",
    "\n",
    "embed_arr,out_arr,file_arr = output_data[\"embedding\"],output_data[\"output\"],output_data[\"file_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_arr.shape,out_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_arr,np.unique(file_arr,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def draw_heatmaps(arr_list,nrows=2,ncols=2):\n",
    "    ran_list = random.sample(arr_list,ncols*nrows)\n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, sharex=True)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            #print(i*ncols+j,len(ran_list))\n",
    "            ax[i,j].imshow(ran_list[i*ncols+j].detach().cpu().numpy()[np.newaxis,:], cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.imshow(embed_arr, cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba50071",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.imshow(embed_arr[0][np.newaxis,:], cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.imshow(embed_arr[20][np.newaxis,:], cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [re.findall(r'[a-z]+',x[0])[0] for x in file_arr.tolist()]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_acr_full = {}\n",
    "for fol in os.listdir(\"../data/nipun_video_dataset/PAMAP2_K10_V1\"):\n",
    "    file_name = os.listdir(f\"../data/nipun_video_dataset/PAMAP2_K10_V1/{fol}\")[0]\n",
    "    map_acr_full[re.findall(r'[a-z]+',file_name)[0]] = fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names = [map_acr_full[x] for x in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=embed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp.plot(df, '.', reduce='UMAP', hue=class_names, ndims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prototypes = {}\n",
    "for k,v in map_acr_full.items():\n",
    "    prototypes[v] = np.mean(embed_arr[v == np.asarray(class_names)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(prototypes,orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp.plot(df, '.', reduce='UMAP', hue=df.index, ndims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.makedirs(f\"{prototypes_save}/{model_ident}/{unique_iden}\",exist_ok=True)\n",
    "\n",
    "with open(f\"{prototypes_save}/{model_ident}/{unique_iden}/without_pretrain_on_local.pkl\",\"wb\") as f0:\n",
    "    pickle.dump(prototypes,f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "544d855d7b0d57add784f15e62ffa31fd790b767434a09f782574915bd2ed2d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
