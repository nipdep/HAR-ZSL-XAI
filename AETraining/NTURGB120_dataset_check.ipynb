{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkIfhXqxWhOw",
    "outputId": "9b33a8e1-6f66-4945-cc59-6455a3312dc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n",
    "                        \"#FFDD00\",\n",
    "                        \"#FF7D00\",\n",
    "                        \"#FF006D\",\n",
    "                        \"#ADFF02\",\n",
    "                        \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Collection of functions which enable the evaluation of a classifier's performance,\n",
    "by showing confusion matrix, accuracy, recall, precision etc.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def save_history(history, model_name, unique_name, models_saves, config):\n",
    "    PATH = f\"{models_saves}/{model_name}\"\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n",
    "        json.dump(history, f0)\n",
    "\n",
    "def get_config(file_loc):\n",
    "    file = torch.load(file_loc)\n",
    "    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n",
    "    \n",
    "def save_model(model, model_name, unique_name, models_saves, config):\n",
    "    PATH = f\"{models_saves}/{model_name}\"\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "    torch.save({\n",
    "        \"n_epochs\": config[\"n_epochs\"],\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_config\": config[\"model\"],\n",
    "        \"config\": config\n",
    "    }, f\"{PATH}/{unique_name}.pt\")\n",
    "\n",
    "def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n",
    "    \"\"\"Plot confusion matrix in a separate window\"\"\"\n",
    "    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    if label_strings:\n",
    "        tick_marks = np.arange(len(label_strings))\n",
    "        plt.xticks(tick_marks, label_strings, rotation=90)\n",
    "        plt.yticks(tick_marks, label_strings)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n",
    "    \"\"\"\n",
    "    Returns a string of a report for given metric arrays (array length equals the number of classes).\n",
    "    Called internally by `analyze_classification`.\n",
    "        digits: number of digits after . for displaying results\n",
    "        number_of_thieves: number of biggest thieves to report\n",
    "        maxcharlength: max. number of characters to use when displaying thief names\n",
    "    \"\"\"\n",
    "\n",
    "    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n",
    "    sorted_class_indices = np.argsort(relative_freq)[\n",
    "                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n",
    "\n",
    "    last_line_heading = 'avg / total'\n",
    "\n",
    "    width = max(len(cn) for cn in existing_class_names)\n",
    "    width = max(width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n",
    "    fmt = '%% %ds' % width  # first column: class name\n",
    "    fmt += '  '\n",
    "    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n",
    "    fmt += '|\\t % 5s'\n",
    "    fmt += '\\n'\n",
    "\n",
    "    headers = [\"\"] + headers\n",
    "    report = fmt % tuple(headers)\n",
    "    report += '\\n'\n",
    "\n",
    "    for i in sorted_class_indices:\n",
    "        values = [existing_class_names[i]]\n",
    "        for v in (precision[i], recall[i], f1[i],\n",
    "                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n",
    "            values += [\"{0:0.{1}f}\".format(v, digits)]\n",
    "        values += [\"{}\".format(support[i])]\n",
    "        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n",
    "                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n",
    "        thieves = thieves[thieves != i]  # exclude self at this point\n",
    "        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n",
    "        thieves_names = [\n",
    "            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n",
    "            in thieves]  # a little inefficient but inconsequential\n",
    "        string_about_stealing = \"\"\n",
    "        for j in range(len(thieves)):\n",
    "            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n",
    "        values += [string_about_stealing]\n",
    "\n",
    "        report += fmt % tuple(values)\n",
    "\n",
    "    report += '\\n' + 100 * '-' + '\\n'\n",
    "\n",
    "    # compute averages/sums\n",
    "    values = [last_line_heading]\n",
    "    for v in (np.average(precision, weights=relative_freq),\n",
    "                np.average(recall, weights=relative_freq),\n",
    "                np.average(f1, weights=relative_freq)):\n",
    "        values += [\"{0:0.{1}f}\".format(v, digits)]\n",
    "    values += ['{0}'.format(np.sum(relative_freq))]\n",
    "    values += ['{0}'.format(np.sum(support))]\n",
    "    values += ['']\n",
    "\n",
    "    # make last (\"Total\") line for report\n",
    "    report += fmt % tuple(values)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n",
    "    \"\"\"\n",
    "    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n",
    "    Input:\n",
    "        y_pred: 1D array of predicted labels (class indices)\n",
    "        y_true: 1D array of true labels (class indices)\n",
    "        class_names: 1D array or list of class names in the order of class indices.\n",
    "            Could also be integers [0, 1, ..., num_classes-1].\n",
    "        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n",
    "    \"\"\"\n",
    "\n",
    "    # Trim class_names to include only classes existing in y_pred OR y_true\n",
    "    in_pred_labels = set(list(y_pred))\n",
    "    in_true_labels = set(list(y_true))\n",
    "    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n",
    "\n",
    "    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n",
    "    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n",
    "    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n",
    "    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n",
    "\n",
    "    # Confusion matrix\n",
    "    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure()\n",
    "        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n",
    "                                title='Confusion matrix normalized by row')\n",
    "        plt.show(block=False)\n",
    "\n",
    "    # Analyze results\n",
    "    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n",
    "    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n",
    "\n",
    "    # returns metrics for each class, in the same order as existing_class_names\n",
    "    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n",
    "    # Print report\n",
    "    if print_report:\n",
    "        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n",
    "\n",
    "    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p0XIAvcQX3l0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y7gAeWUDX7X6"
   },
   "outputs": [],
   "source": [
    "model_ident = \"NTURGB120_skeleton_classifier_10clstest\"\n",
    "unique_iden = \"epoch50_emb1024_xy\"\n",
    "\n",
    "main_dir = \"..\"\n",
    "\n",
    "data_dir = os.path.join(\"E:\\\\FYP_Data\\\\NTU120\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\nturgb+d_skeletons\")\n",
    "remove_files = [\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\NTU_RGBD120_samples_with_missing_skeletons.txt\",\n",
    "                \"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\NTU_RGBD_samples_with_missing_skeletons.txt\"]\n",
    "\n",
    "epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n",
    "models_saves = os.path.join(main_dir,\"model_saves\")\n",
    "embeddings_save = os.path.join(main_dir,\"embedding_save\")\n",
    "prototypes_save = os.path.join(main_dir,\"prototypes\")\n",
    "test_vids = os.path.join(main_dir,\"test_vids\")\n",
    "train_ratio = 0.90\n",
    "val_ratio = 0.1\n",
    "batch_size = 8\n",
    "\n",
    "os.makedirs(epoch_vids,exist_ok=True)\n",
    "os.makedirs(models_saves,exist_ok=True)\n",
    "os.makedirs(embeddings_save,exist_ok=True)\n",
    "\n",
    "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\nturgb120_label_map.json\",\"r\") as f0:\n",
    "    full_id2cls = json.load(f0)\n",
    "    \n",
    "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\sel_cls_list - Single_person.txt\",\"r\") as f0:\n",
    "    class_names = [full_id2cls[x] for x in f0.read().split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IrsB7xhhaKF5"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_epochs\":50,\n",
    "    \"model_name\":\"BidirectionalLSTM\",\n",
    "    \"model\":{\n",
    "        \"seq_len\":50,\n",
    "        \"input_size\":25*2,\n",
    "        \"hidden_size\":1024,\n",
    "        \"linear_filters\":[128,256,512,1024],\n",
    "        \"embedding_size\":1024,\n",
    "        \"num_classes\":len(class_names),\n",
    "        \"num_layers\":1,\n",
    "        \"bidirectional\":True,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"dev\":device\n",
    "    },\n",
    "    'alpha_recon': 1,\n",
    "    'alpha_target': 1,\n",
    "}\n",
    "\n",
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "25RyOW-8gA46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files to remove:=  535\n",
      "Number of Files to Total:=  113945\n",
      "Number of Files to Train:=  70034\n",
      "Number of Files to Val:=  7782\n"
     ]
    }
   ],
   "source": [
    "from dataset.SkeletonData.data import *\n",
    "\n",
    "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\shapes_keys.json\",\"r\") as f0:\n",
    "    id2shapes = json.load(f0)\n",
    "\n",
    "files_to_remove = set()\n",
    "for __f in remove_files:\n",
    "    with open(__f,\"r\") as f0:\n",
    "        for val in f0.read().split(\"\\n\"):\n",
    "            files_to_remove.add(val)\n",
    "\n",
    "print(\"Number of Files to remove:= \",len(files_to_remove))\n",
    "\n",
    "total_files = set([x.split(\".\")[0] for x in os.listdir(data_dir)]) - files_to_remove\n",
    "total_files_loc = set([f\"{os.path.join(data_dir,x)}.skeleton\" for x in total_files])\n",
    "\n",
    "#split list\n",
    "rows = [(full_id2cls[str(int(x.split(\".\")[0][-3:]))],x) for x in total_files_loc]\n",
    "info_pd = pd.DataFrame(data=rows,columns=[\"target\",\"file_loc\"])\n",
    "\n",
    "#select needed classes.\n",
    "info_pd = info_pd.loc[info_pd[\"target\"].isin(class_names)]\n",
    "train_df, val_df = train_test_split(info_pd,stratify=info_pd[\"target\"],train_size=train_ratio)\n",
    "\n",
    "print(\"Number of Files to Total:= \",len(total_files))\n",
    "\n",
    "train_builder = SkeletonFileBuilder(file_names=set(train_df[\"file_loc\"].to_list()))\n",
    "val_builder = SkeletonFileBuilder(file_names=set(val_df[\"file_loc\"].to_list()))\n",
    "\n",
    "print(\"Number of Files to Train:= \",len(train_builder))\n",
    "print(\"Number of Files to Val:= \",len(val_builder))\n",
    "\n",
    "train_file_iterator = iter(train_builder)\n",
    "val_file_iterator = iter(val_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Res0UAsOjjki"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_file_to_memory(id2shape,save_dict,each_file):\n",
    "      file_id = each_file.filepath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "      num_frame, body_data = each_file.load_data()\n",
    "      orig_vid_size = id2shape[file_id]\n",
    "      \n",
    "      for frame_data in body_data:\n",
    "        if frame_data[\"body_count\"] != 1:\n",
    "            return None\n",
    "      \n",
    "      skel_data = []\n",
    "      for frame_data in body_data:\n",
    "          frame_jd = []\n",
    "          for jd in frame_data[\"bodies\"][0][\"joint_details\"]:\n",
    "              x = jd[\"colorX\"] / orig_vid_size[1]\n",
    "              y = jd[\"colorY\"] / orig_vid_size[0]\n",
    "\n",
    "              frame_jd.append([x, y])\n",
    "\n",
    "          skel_data.append(frame_jd)\n",
    "\n",
    "      skel_data = np.asarray(skel_data)\n",
    "      save_dict[file_id] = (file_id,orig_vid_size,str(int(file_id[-3:])),skel_data)\n",
    "      return file_id\n",
    "\n",
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_builder, \n",
    "                 fileid2shape,\n",
    "                 full_label_map,\n",
    "                 cls2id,\n",
    "                 transform=None,\n",
    "                 seq_len = 100,\n",
    "                 window_size = 200,\n",
    "                 target_transform=None,\n",
    "                 active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28],\n",
    "                 file_name=False, \n",
    "                 is_2d=False):\n",
    "        self.data_builder = data_builder\n",
    "        self.transform = transform\n",
    "        self.fileid2shape = fileid2shape\n",
    "        self.window_size = window_size\n",
    "        self.seq_len = seq_len\n",
    "        self.target_transform = target_transform\n",
    "        self.active_locations = active_locations\n",
    "        self.file_name = file_name\n",
    "        self.is_2d = is_2d\n",
    "        self.cls2id = cls2id\n",
    "        self.full_label_map = full_label_map\n",
    "\n",
    "        if self.active_locations:\n",
    "          self.join_translation_map = {k:i for i,k in enumerate(self.active_locations)}\n",
    "        \n",
    "        self.data = {}\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "          self.indexes = list(\n",
    "              tqdm(\n",
    "                executor.map(\n",
    "                  partial(load_file_to_memory,self.fileid2shape,self.data),\n",
    "                  self.data_builder), \n",
    "                total=len(self.data_builder),\n",
    "                desc=\"Loaded Files\"\n",
    "              )\n",
    "            )\n",
    "        \n",
    "        #black_filter = []\n",
    "        #for idx in range(self.df.shape[0]):\n",
    "        #  if len(self.data[idx][\"coords\"].shape)<3 or self.data[idx][\"coords\"].shape[0]<20:\n",
    "        #    black_filter.append(idx)\n",
    "        #\n",
    "        self.indexes = [x for x in self.indexes if x != None]\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "      \n",
    "    def select_frames(self,sequence):\n",
    "      if sequence.shape[0]<self.seq_len:\n",
    "        times = self.seq_len//sequence.shape[0] + 1\n",
    "\n",
    "        sequence = sequence.repeat(times,1,1)\n",
    "\n",
    "      if sequence.shape[0]>self.window_size:\n",
    "        start = random.randint(0,sequence.shape[0]-self.window_size-1)\n",
    "        sequence = sequence[start:start+self.window_size,...]\n",
    "                               \n",
    "      sel_index = sorted(random.sample(range(sequence.shape[0]),self.seq_len))\n",
    "        \n",
    "      return sequence[sel_index,...]\n",
    "    \n",
    "    def create_connection_map(self,original_map):\n",
    "      if not self.active_locations:\n",
    "        return original_map\n",
    "      \n",
    "      all_possible_comb = product(self.active_locations,self.active_locations)\n",
    "      all_possible_comb = set(all_possible_comb)\n",
    "      \n",
    "      original_map = set(original_map)\n",
    "      sel_connections = list(all_possible_comb.intersection(original_map))\n",
    "      sel_connections = [(self.join_translation_map[x[0]],self.join_translation_map[x[1]]) for x in sel_connections]\n",
    "      \n",
    "      return sel_connections \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.indexes[idx]\n",
    "        \n",
    "        orig_target = self.data[idx][2]\n",
    "        file_path = self.data[idx][0]\n",
    "        vid_size = self.data[idx][1]\n",
    "        coords = self.data[idx][3]\n",
    "        \n",
    "        target = self.cls2id[self.full_label_map[orig_target]]\n",
    "        \n",
    "        if self.active_locations:\n",
    "          coords = coords[:,self.active_locations,:]\n",
    "\n",
    "        if self.is_2d:\n",
    "            coords = coords[...,0:2]\n",
    "\n",
    "        coords = torch.from_numpy(coords).float()\n",
    "        coords = self.select_frames(coords)\n",
    "\n",
    "        shape = coords.shape\n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "\n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "\n",
    "        if self.file_name:\n",
    "            return coords, label, target,vid_size,file_path\n",
    "        return coords, label, target,vid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2d95274957164fd5bba0bad5ee842b3a",
      "d3200c26f4af46ff99b287828b7e9ea7",
      "93eb418acba1422db89c28e98fa0ae76",
      "c5aa201965a0486ca00eff8f699de8f8",
      "4db48309ea6b4169975679004d422a86",
      "0591cfcae0f949e78ebf5b91699755fc",
      "3c5dd52f5ca949ac824d64a000dbfd2a",
      "3ed3488ab0874a4980032517c5966b10",
      "6e69f943a503460896239b94fdbf15c8",
      "3c1cd97dfdd64371bf2489419d334cb8",
      "c25a6b88cef244f090e8dd43d9e197d7"
     ]
    },
    "id": "o0Mlr88WEjWt",
    "outputId": "eaa5cded-8b20-490d-aae0-e14b0450cc7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded Files: 100%|██████████| 70034/70034 [42:07<00:00, 27.71it/s]  \n",
      "Loaded Files: 100%|██████████| 7782/7782 [04:36<00:00, 28.10it/s] \n"
     ]
    }
   ],
   "source": [
    "train_ds = SkeletonDataset(train_file_iterator,\n",
    "                             id2shapes,\n",
    "                             full_id2cls,\n",
    "                             clsname2id,\n",
    "                             seq_len=config[\"model\"][\"seq_len\"],\n",
    "                             is_2d=True,\n",
    "                             file_name=True,\n",
    "                             active_locations=[3,2,20,\n",
    "                                               4,5,6,\n",
    "                                               7,8,9,\n",
    "                                               10,11,\n",
    "                                               1,0,12,\n",
    "                                               13,14,16,\n",
    "                                               17,18])\n",
    "val_ds = SkeletonDataset(val_file_iterator,\n",
    "                           id2shapes,\n",
    "                           full_id2cls,\n",
    "                           clsname2id,\n",
    "                           seq_len=config[\"model\"][\"seq_len\"],\n",
    "                           is_2d=True,\n",
    "                           file_name=True,\n",
    "                           active_locations=[3,2,20,\n",
    "                                               4,5,6,\n",
    "                                               7,8,9,\n",
    "                                               10,11,\n",
    "                                               1,0,12,\n",
    "                                               13,14,16,\n",
    "                                               17,18])\n",
    "#test_data = SkeletonDataset(val_file_iterator,id2shapes,is_2d=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66255, 7345)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.indexes),len(val_ds.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count_array = {}\n",
    "for arr in train_ds.data.values():\n",
    "    try:\n",
    "        frame_count_array[arr[2]].append(arr[3].shape[0])\n",
    "    except KeyError:\n",
    "        frame_count_array[arr[2]] = [arr[3].shape[0]]\n",
    "        \n",
    "for arr in val_ds.data.values():\n",
    "    try:\n",
    "        frame_count_array[arr[2]].append(arr[3].shape[0])\n",
    "    except KeyError:\n",
    "        frame_count_array[arr[2]] = [arr[3].shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 910\n",
      "6 914\n",
      "71 896\n",
      "25 907\n",
      "16 912\n",
      "35 903\n",
      "84 895\n",
      "13 902\n",
      "29 906\n",
      "24 905\n",
      "65 895\n",
      "67 892\n",
      "97 873\n",
      "86 900\n",
      "30 912\n",
      "79 898\n",
      "14 906\n",
      "70 896\n",
      "27 906\n",
      "78 895\n",
      "63 883\n",
      "76 887\n",
      "8 900\n",
      "73 890\n",
      "69 894\n",
      "68 894\n",
      "26 905\n",
      "89 907\n",
      "18 910\n",
      "21 915\n",
      "4 916\n",
      "72 897\n",
      "75 879\n",
      "88 908\n",
      "96 874\n",
      "19 913\n",
      "23 917\n",
      "62 885\n",
      "7 914\n",
      "9 916\n",
      "31 903\n",
      "3 911\n",
      "61 872\n",
      "74 886\n",
      "12 905\n",
      "93 868\n",
      "95 874\n",
      "85 899\n",
      "20 911\n",
      "101 869\n",
      "5 913\n",
      "77 894\n",
      "83 902\n",
      "11 905\n",
      "33 906\n",
      "10 906\n",
      "87 895\n",
      "36 903\n",
      "82 892\n",
      "91 899\n",
      "38 903\n",
      "28 906\n",
      "32 906\n",
      "17 914\n",
      "80 895\n",
      "66 894\n",
      "22 915\n",
      "81 890\n",
      "1 904\n",
      "94 869\n",
      "34 908\n",
      "102 871\n",
      "39 902\n",
      "100 872\n",
      "98 874\n",
      "90 910\n",
      "64 891\n",
      "37 902\n",
      "92 851\n",
      "40 891\n",
      "99 873\n",
      "15 919\n"
     ]
    }
   ],
   "source": [
    "for tar, fcs in  frame_count_array.items():\n",
    "    print(tar,len(fcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\train_data.pkl\",\"wb\") as f0:\n",
    "    pickle.dump(train_ds.data,f0)\n",
    "    \n",
    "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\val_data.pkl\",\"wb\") as f0:\n",
    "    pickle.dump(val_ds.data,f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B6mtux6OEmIk"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "#test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gv4MRtlZbLYt"
   },
   "outputs": [],
   "source": [
    "def gen_skeleton(frame, \n",
    "                 height,\n",
    "                 width,\n",
    "                 mapping_list = [(0, 1), (1, 3), (3, 5), \n",
    "                                 (0, 2), (2, 4), (0, 6), \n",
    "                                 (1, 7), (6, 7), (6, 8), \n",
    "                                 (7, 9), (8, 10), (9, 11)]):\n",
    "    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n",
    "    img_3.fill(255)\n",
    "\n",
    "    # add circles\n",
    "    for coord in frame:\n",
    "        x, y = int(width*coord[0]), int(height*coord[1])\n",
    "        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n",
    "\n",
    "    # add lines\n",
    "    for line in mapping_list:\n",
    "        i, j = line\n",
    "        st = frame[i, :]\n",
    "        start_point = (int(width*st[0]), int(height*st[1]))\n",
    "\n",
    "        en = frame[j, :]\n",
    "        end_point = (int(width*en[0]), int(height*en[1]))\n",
    "\n",
    "        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n",
    "\n",
    "    return img_3\n",
    "\n",
    "def gen_video(points, \n",
    "              save_file, \n",
    "              frame_h, \n",
    "              frame_w, \n",
    "              is_3d=True,\n",
    "              mapping_list = [(0, 1), (1, 3), (3, 5), \n",
    "                                 (0, 2), (2, 4), (0, 6), \n",
    "                                 (1, 7), (6, 7), (6, 8), \n",
    "                                 (7, 9), (8, 10), (9, 11)]):\n",
    "    # make 3D if points are flatten\n",
    "    if len(points.shape) == 2:\n",
    "        if is_3d:\n",
    "          fts = points.shape[1]\n",
    "          x_cds = list(range(0, fts, 3))\n",
    "          y_cds = list(range(1, fts, 3))\n",
    "          z_cds = list(range(2, fts, 3))\n",
    "          points = np.transpose(np.array([points[:, x_cds], \n",
    "                                          points[:, y_cds], \n",
    "                                          points[:, z_cds]]), (1,2,0))\n",
    "        else:\n",
    "          fts = points.shape[1]\n",
    "          x_cds = list(range(0, fts, 2))\n",
    "          y_cds = list(range(1, fts, 2))\n",
    "          points = np.transpose(np.array([points[:, x_cds], \n",
    "                                          points[:, y_cds]]), (1,2,0))\n",
    "\n",
    "    size = (frame_w, frame_h)\n",
    "    result = cv2.VideoWriter(save_file,\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "\n",
    "    for __id,frame in enumerate(points):\n",
    "        skel_image = gen_skeleton(frame, frame_h, frame_w,mapping_list=mapping_list)\n",
    "        result.write(skel_image)\n",
    "\n",
    "    result.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_map = [(3,2),(2,20),(20,4),(4,5),(5,6),(6,7),(7,21),(7,22),(20,8),(8,9),(9,10),(10,11),(11,23),(11,24),\n",
    "            (20,1),(1,0),(0,12),(12,13),(13,14),(14,15),(0,16),(16,17),(17,18),(18,19)]\n",
    "\n",
    "joint_map = val_ds.create_connection_map(joint_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1013/1013 [00:01<00:00, 757.65it/s]\n"
     ]
    }
   ],
   "source": [
    "for adata in tqdm(train_dl):\n",
    "  selected_ind = random.randint(0,adata[0].shape[0]-1)\n",
    "  data = adata[0][selected_ind].numpy()\n",
    "  file_id = adata[4][selected_ind].split(\".\")[0]\n",
    "  target = id2clsname[int(adata[2][selected_ind])]\n",
    "  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n",
    "  \n",
    "  if(np.isnan(data).sum()):\n",
    "      print(np.isnan(data).sum(),file_id,\"isnan\")\n",
    "      \n",
    "  if(np.isinf(data).sum()):\n",
    "    print(np.isinf(data).sum(),file_id,\"isinf\")\n",
    "      \n",
    "  if not (vid_size[0] and vid_size[1]):\n",
    "    print(vid_size,file_id,\"isvidsize\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OJzG7jyh7pU_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsave_vids_dir = \"checking_vids/init\"\\nfor adata in tqdm(train_dl):\\n  selected_ind = random.randint(0,adata[0].shape[0]-1)\\n  data = adata[0][selected_ind].numpy()\\n  file_id = adata[4][selected_ind].split(\".\")[0]\\n  target = id2clsname[int(adata[2][selected_ind])]\\n  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\\n  try:\\n    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\\n      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\\n      gen_video(data, \\n                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\\n                vid_size[0], \\n                vid_size[1],\\n                is_3d=False,\\n                mapping_list=joint_map\\n                )\\n  except ValueError:\\n    continue\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "save_vids_dir = \"checking_vids/init\"\n",
    "for adata in tqdm(train_dl):\n",
    "  selected_ind = random.randint(0,adata[0].shape[0]-1)\n",
    "  data = adata[0][selected_ind].numpy()\n",
    "  file_id = adata[4][selected_ind].split(\".\")[0]\n",
    "  target = id2clsname[int(adata[2][selected_ind])]\n",
    "  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n",
    "  try:\n",
    "    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\n",
    "      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n",
    "      gen_video(data, \n",
    "                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\n",
    "                vid_size[0], \n",
    "                vid_size[1],\n",
    "                is_3d=False,\n",
    "                mapping_list=joint_map\n",
    "                )\n",
    "  except ValueError:\n",
    "    continue\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "544d855d7b0d57add784f15e62ffa31fd790b767434a09f782574915bd2ed2d8"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0591cfcae0f949e78ebf5b91699755fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d95274957164fd5bba0bad5ee842b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d3200c26f4af46ff99b287828b7e9ea7",
       "IPY_MODEL_93eb418acba1422db89c28e98fa0ae76",
       "IPY_MODEL_c5aa201965a0486ca00eff8f699de8f8"
      ],
      "layout": "IPY_MODEL_4db48309ea6b4169975679004d422a86"
     }
    },
    "3c1cd97dfdd64371bf2489419d334cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5dd52f5ca949ac824d64a000dbfd2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ed3488ab0874a4980032517c5966b10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4db48309ea6b4169975679004d422a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e69f943a503460896239b94fdbf15c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93eb418acba1422db89c28e98fa0ae76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ed3488ab0874a4980032517c5966b10",
      "max": 8068,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e69f943a503460896239b94fdbf15c8",
      "value": 4739
     }
    },
    "c25a6b88cef244f090e8dd43d9e197d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5aa201965a0486ca00eff8f699de8f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c1cd97dfdd64371bf2489419d334cb8",
      "placeholder": "​",
      "style": "IPY_MODEL_c25a6b88cef244f090e8dd43d9e197d7",
      "value": " 4739/8068 [03:15&lt;02:19, 23.88it/s]"
     }
    },
    "d3200c26f4af46ff99b287828b7e9ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0591cfcae0f949e78ebf5b91699755fc",
      "placeholder": "​",
      "style": "IPY_MODEL_3c5dd52f5ca949ac824d64a000dbfd2a",
      "value": "Loaded Files:  59%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
