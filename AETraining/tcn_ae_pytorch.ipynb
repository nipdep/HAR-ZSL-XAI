{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "794b0821-b1cf-4224-8aa6-ea44970abe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ba7eddb-7bff-4d14-9b9f-b0d86bfb40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ident = \"tcn\"\n",
    "unique_iden = \"epoch100_emb128\"\n",
    "\n",
    "main_dir = \"D:\\\\FYP\\\\HAR-ZSL-XAI\"\n",
    "data_dir = os.path.join(main_dir,\"data\",\"sequence_data\",\"random_50f\")\n",
    "epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n",
    "models_saves = os.path.join(main_dir,\"model_saves\")\n",
    "embeddings_save = os.path.join(main_dir,\"embedding_save\")\n",
    "test_vids = os.path.join(main_dir,\"test_vids\")\n",
    "class_names = os.listdir(data_dir)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 1-train_ratio - val_ratio\n",
    "batch_size = 32\n",
    "\n",
    "os.makedirs(epoch_vids,exist_ok=True)\n",
    "os.makedirs(models_saves,exist_ok=True)\n",
    "os.makedirs(embeddings_save,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_epochs\":100,\n",
    "    \"model_name\":\"TCN\",\n",
    "    \"model\":{\n",
    "        \"seq_len\":50,\n",
    "        \"num_features\":36,\n",
    "        \"num_channels\":[20,120,20,4],\n",
    "        \"embedding_size\":64,\n",
    "        \"kernel_size\":3,\n",
    "        \"dropout\":0.3,\n",
    "        \"dev\":device\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef5f6ab7-fd86-4435-9bec-331d014a6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classname_id(class_name_list):\n",
    "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
    "    classname2id = {v:k for k, v in id2classname.items()}\n",
    "    return id2classname, classname2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f90625df-343b-4bc3-897d-184be8d8422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(os.path.join(main_dir,\"data\",\"skel_out\",\"hmbd51\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f27b84c1-0510-42f6-b53e-503875da2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2clsname, clsname2id = classname_id(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b3f164bb-3aa7-450b-b2b7-3f11a055c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = []\n",
    "val_file_list = []\n",
    "test_file_list = []\n",
    "\n",
    "file_list = [os.path.join(data_dir,x) for x in os.listdir(data_dir)]\n",
    "\n",
    "random.shuffle(file_list)\n",
    "num_list = len(file_list)\n",
    "\n",
    "train_range = [0,int(num_list*train_ratio)]\n",
    "val_range = [int(num_list*train_ratio),int(num_list*(train_ratio+val_ratio))]\n",
    "test_range = [int(num_list*(train_ratio+val_ratio)),num_list-1]\n",
    "\n",
    "train_file_list += file_list[train_range[0]:train_range[1]]\n",
    "val_file_list += file_list[val_range[0]:val_range[1]]\n",
    "test_file_list += file_list[test_range[0]:test_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "db9f2788-0c6f-414a-a65e-eab0956e5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6746, 1446, 1445)"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cca03ac0-3133-4e69-9c4c-7e3c346cae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = train_file_list[:(len(train_file_list)//batch_size)*batch_size]\n",
    "val_file_list = val_file_list[:(len(val_file_list)//batch_size)*batch_size]\n",
    "test_file_list = test_file_list[:(len(test_file_list)//batch_size)*batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f014d9f-b951-481e-af46-a677cb0e2c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6720, 1440, 1440)"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list),len(val_file_list),len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ea002f29-1936-46b1-a35f-7dc62c439642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonDataset(Dataset):\n",
    "    def __init__(self, file_list,class2id, transform=None, \n",
    "                 target_transform=None,active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28],file_name=False):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.class2id = class2id\n",
    "        self.target_transform = target_transform\n",
    "        self.active_locations = active_locations\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a_file = np.load(self.file_list[idx])\n",
    "        action_type = self.file_list[idx].strip().split(os.path.sep)[-1].split(\"_cls_\")[0]\n",
    "        coords, vid_size = a_file[\"coords\"],a_file[\"video_size\"]\n",
    "        coords = coords[:,self.active_locations,:]\n",
    "        \n",
    "        shape = coords.shape\n",
    "        \n",
    "        coords = torch.from_numpy(coords).float()\n",
    "        \n",
    "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
    "        label = torch.clone(coords)\n",
    "        \n",
    "        if self.transform:\n",
    "            coords = self.transform(coords)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(coords)\n",
    "\n",
    "        if self.file_name:\n",
    "            return coords, label, self.class2id[action_type],a_file[\"video_size\"],self.file_list[idx]\n",
    "        return coords, label, self.class2id[action_type],a_file[\"video_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d0bfa6f8-44df-49ac-901c-1835612db26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SkeletonDataset(train_file_list,clsname2id)\n",
    "val_data = SkeletonDataset(val_file_list,clsname2id)\n",
    "test_data = SkeletonDataset(test_file_list,clsname2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "63f2a581-485c-4ac9-836a-86f74ec4aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b980c5a-2d57-4c0f-bcea-8bd2bbab1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class TCNEnc(nn.Module):\n",
    "    def __init__(self, seq_len=50,num_features=36, num_channels=[20,20,20,4], embedding_size=128, kernel_size=3, dropout=0.3):\n",
    "        super(TCNEnc, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_features = num_features\n",
    "        self.num_channels = num_channels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.tcn = TemporalConvNet(self.seq_len, num_channels=self.num_channels, kernel_size = self.kernel_size, dropout=self.dropout)\n",
    "        self.linear = nn.Linear(self.num_channels[-1]*self.num_features, self.embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        output = self.tcn(x)\n",
    "        output = torch.flatten(output,start_dim=1)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class TCNDec(nn.Module):\n",
    "    def __init__(self, seq_len=50,num_features=36, num_channels=[20,20,20], embedding_size=128, kernel_size=3, dropout=0.3,encoder_tcn_out=4):\n",
    "        super(TCNDec, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_features = num_features\n",
    "        self.num_channels = num_channels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout\n",
    "        self.encoder_tcn_out =  encoder_tcn_out\n",
    "\n",
    "        self.num_channels.append(seq_len)\n",
    "\n",
    "        self.linear = nn.Linear(self.embedding_size,self.encoder_tcn_out*self.num_features)\n",
    "        self.tcn = TemporalConvNet(self.encoder_tcn_out, num_channels=self.num_channels, kernel_size = self.kernel_size, dropout=self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        output = self.linear(x)\n",
    "        output = output.view(-1,self.encoder_tcn_out,self.num_features)\n",
    "        output = self.tcn(output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class TCNEncoderDecoder(nn.Module):\n",
    "    def __init__(self, seq_len=50,num_features=36, num_channels=[20,20,20,4], embedding_size=128, kernel_size=3, dropout=0.3):\n",
    "        super(TCNEncoderDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_features = num_features\n",
    "        self.num_channels_encoder = num_channels\n",
    "        self.num_channels_decoder = num_channels[::-1][1:]\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout = dropout   \n",
    "\n",
    "        self.encoder = TCNEnc(\n",
    "            seq_len = self.seq_len,\n",
    "            num_features = self.num_features,\n",
    "            num_channels = self.num_channels_encoder,\n",
    "            embedding_size = self.embedding_size,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout)\n",
    "\n",
    "        self.decoder = TCNDec(\n",
    "            seq_len = self.seq_len,\n",
    "            encoder_tcn_out= self.num_channels_encoder[-1],\n",
    "            num_features = self.num_features,\n",
    "            num_channels = self.num_channels_decoder,\n",
    "            embedding_size = self.embedding_size,\n",
    "            kernel_size = self.kernel_size,\n",
    "            dropout = self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        embedding = self.encoder(x)\n",
    "        output = self.decoder(embedding)\n",
    "        return output, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e0d8fe8-26de-4d03-a444-3eac3055caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_encoder = TCNEnc()\n",
    "tcn_decoder = TCNDec()\n",
    "\n",
    "tcn_model = TCNEncoderDecoder(\n",
    "    seq_len=config[\"model\"][\"seq_len\"],\n",
    "    num_features=config[\"model\"][\"num_features\"],\n",
    "    num_channels=config[\"model\"][\"num_channels\"],\n",
    "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
    "    kernel_size=config[\"model\"][\"kernel_size\"],\n",
    "    dropout=config[\"model\"][\"dropout\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ed1b0e37-a8ca-4f8c-b616-a86accc45e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample = torch.randn((32,50,36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "981aec7c-e339-4858-9fb2-7abc614ae8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 50, 36])"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_sample.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eb702c5e-acb5-4852-a264-f3fac27b6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tcn_encoder(rand_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b12b06e-44b2-4df6-9bc0-c27fb817db41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 128])"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "be9b320d-2659-435c-b6fe-2c9937360f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tcn_decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 50, 36])"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ef8a0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 50, 36])"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn_model(out)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "TCNEncoderDecoder(\n  (encoder): TCNEnc(\n    (tcn): TemporalConvNet(\n      (network): Sequential(\n        (0): TemporalBlock(\n          (conv1): Conv1d(50, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(50, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(50, 20, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (1): TemporalBlock(\n          (conv1): Conv1d(20, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(20, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(20, 120, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (2): TemporalBlock(\n          (conv1): Conv1d(120, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(120, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(120, 20, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (3): TemporalBlock(\n          (conv1): Conv1d(20, 4, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(20, 4, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(20, 4, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n      )\n    )\n    (linear): Linear(in_features=144, out_features=64, bias=True)\n  )\n  (decoder): TCNDec(\n    (linear): Linear(in_features=64, out_features=144, bias=True)\n    (tcn): TemporalConvNet(\n      (network): Sequential(\n        (0): TemporalBlock(\n          (conv1): Conv1d(4, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(4, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(2,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(4, 20, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (1): TemporalBlock(\n          (conv1): Conv1d(20, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(20, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(20, 120, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (2): TemporalBlock(\n          (conv1): Conv1d(120, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(120, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(20, 20, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(120, 20, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n        (3): TemporalBlock(\n          (conv1): Conv1d(20, 50, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n          (chomp1): Chomp1d()\n          (relu1): ReLU()\n          (dropout1): Dropout(p=0.3, inplace=False)\n          (conv2): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n          (chomp2): Chomp1d()\n          (relu2): ReLU()\n          (dropout2): Dropout(p=0.3, inplace=False)\n          (net): Sequential(\n            (0): Conv1d(20, 50, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n            (1): Chomp1d()\n            (2): ReLU()\n            (3): Dropout(p=0.3, inplace=False)\n            (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n            (5): Chomp1d()\n            (6): ReLU()\n            (7): Dropout(p=0.3, inplace=False)\n          )\n          (downsample): Conv1d(20, 50, kernel_size=(1,), stride=(1,))\n          (relu): ReLU()\n        )\n      )\n    )\n  )\n)"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn_model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3206b242-0537-4c4b-9ff2-e361cd4dff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_skeleton(frame, connections, height, width):\n",
    "    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n",
    "    img_3.fill(255)\n",
    "\n",
    "    # add circles\n",
    "    for coord in frame:\n",
    "        x, y = int(width*coord[0]), int(height*coord[1])\n",
    "        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n",
    "\n",
    "    # add lines\n",
    "    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n",
    "    for line in mapping_list:\n",
    "        i, j = line\n",
    "        st = frame[i, :]\n",
    "        start_point = (int(width*st[0]), int(height*st[1]))\n",
    "\n",
    "        en = frame[j, :]\n",
    "        end_point = (int(width*en[0]), int(height*en[1]))\n",
    "\n",
    "        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n",
    "\n",
    "    return img_3\n",
    "\n",
    "def gen_skeletons(frame1 , frame2, connections, height, width):\n",
    "    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n",
    "    img_3.fill(255)\n",
    "\n",
    "    # add circles for the frame-set-1\n",
    "    for coord in frame1:\n",
    "        x, y = int(width*coord[0]), int(height*coord[1])\n",
    "        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n",
    "\n",
    "    # add circles for the frame-set-2\n",
    "    for coord in frame2:\n",
    "        x, y = int(width*coord[0]), int(height*coord[1])\n",
    "        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(0, 255, 0), thickness=6)\n",
    "\n",
    "    # add lines\n",
    "    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n",
    "    for line in mapping_list:\n",
    "        i, j = line\n",
    "\n",
    "        st = frame1[i, :]\n",
    "        start_point = (int(width*st[0]), int(height*st[1]))\n",
    "        en = frame1[j, :]\n",
    "        end_point = (int(width*en[0]), int(height*en[1]))\n",
    "        img3_ = cv2.line(img_3, start_point, end_point, color=(250, 0, 0), thickness=3)\n",
    "\n",
    "        st = frame2[i, :]\n",
    "        start_point = (int(width*st[0]), int(height*st[1]))\n",
    "        en = frame2[j, :]\n",
    "        end_point = (int(width*en[0]), int(height*en[1]))\n",
    "        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 250, 0), thickness=3)\n",
    "\n",
    "    return img_3\n",
    "\n",
    "def gen_video(points, save_file, frame_h, frame_w):\n",
    "    # make 3D if points are flatten\n",
    "    if len(points.shape) == 2:\n",
    "        fts = points.shape[1]\n",
    "        x_cds = list(range(0, fts, 3))\n",
    "        y_cds = list(range(1, fts, 3))\n",
    "        z_cds = list(range(2, fts, 3))\n",
    "        points = np.transpose(np.array([points[:, x_cds], points[:, y_cds], points[:, z_cds]]), (1,2,0))\n",
    "\n",
    "    size = (frame_w, frame_h)\n",
    "    result = cv2.VideoWriter(save_file,\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "\n",
    "    # mapping_list = [(11, 12), (11, 13), (13, 15), (12, 14), (14, 16), (12, 24), (11, 23), (23, 24), (24, 26), (26, 28), (23, 25), (25, 27)]\n",
    "    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n",
    "    for __id,frame in enumerate(points):\n",
    "        skel_image = gen_skeleton(frame, mapping_list, frame_h, frame_w)\n",
    "        result.write(skel_image)\n",
    "\n",
    "    result.release()\n",
    "\n",
    "def gen_cmp_video(points1, points2, save_file, frame_h, frame_w):\n",
    "    # make 3D if points are flatten\n",
    "    if len(points1.shape) == 2:\n",
    "        fts = points1.shape[1]\n",
    "        x_cds = list(range(0, fts, 3))\n",
    "        y_cds = list(range(1, fts, 3))\n",
    "        z_cds = list(range(2, fts, 3))\n",
    "        points1 = np.transpose(np.array([points1[:, x_cds], points1[:, y_cds], points1[:, z_cds]]), (1,2,0))\n",
    "\n",
    "    if len(points2.shape) == 2:\n",
    "        fts = points2.shape[1]\n",
    "        x_cds = list(range(0, fts, 3))\n",
    "        y_cds = list(range(1, fts, 3))\n",
    "        z_cds = list(range(2, fts, 3))\n",
    "        points2 = np.transpose(np.array([points2[:, x_cds], points2[:, y_cds], points2[:, z_cds]]), (1,2,0))\n",
    "\n",
    "    size = (frame_w, frame_h)\n",
    "    result = cv2.VideoWriter(save_file,\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "\n",
    "    # mapping_list = [(11, 12), (11, 13), (13, 15), (12, 14), (14, 16), (12, 24), (11, 23), (23, 24), (24, 26), (26, 28), (23, 25), (25, 27)]\n",
    "    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n",
    "    for __id,frame_1 in enumerate(points1):\n",
    "        frame_2 = points2[__id]\n",
    "        skel_image = gen_skeletons(frame_1, frame_2, mapping_list, frame_h, frame_w)\n",
    "        result.write(skel_image)\n",
    "\n",
    "    result.release()\n",
    "\n",
    "def gen_random_video(model,dataset,epoch,batch_size,model_name=model_ident,unique_name=unique_iden):\n",
    "    ind = random.randint(0,len(dataset)-1)\n",
    "    batch_sel = random.randint(0,batch_size-1)\n",
    "\n",
    "    in_seq,tar_seq,action,vid_size = dataset[ind]\n",
    "    pred_seq, _ = model(in_seq.repeat(batch_size,1,1).to(device))\n",
    "\n",
    "    os.makedirs(f\"{epoch_vids}/{model_name}/{unique_name}/{epoch}\",exist_ok=True)\n",
    "    gen_video(pred_seq[batch_sel].squeeze().cpu().detach().numpy(),f\"{epoch_vids}/{model_name}/{unique_name}/{epoch}/pred.mp4\",int(vid_size[0]),int(vid_size[1]))\n",
    "    gen_video(in_seq.detach().numpy(),f\"{epoch_vids}/{model_name}/{unique_name}/{epoch}/true.mp4\",int(vid_size[0]),int(vid_size[1]))\n",
    "\n",
    "def save_history(history,model_name,unique_name):\n",
    "    PATH = f\"{models_saves}/{model_name}\"\n",
    "    os.makedirs(PATH,exist_ok=True)\n",
    "\n",
    "    with open(f\"{PATH}/{unique_name}.json\",\"w+\") as f0:\n",
    "        json.dump(history,f0)\n",
    "\n",
    "def save_model(model,model_name,unique_name):\n",
    "    PATH = f\"{models_saves}/{model_name}\"\n",
    "    os.makedirs(PATH,exist_ok=True)\n",
    "    torch.save({\n",
    "        \"n_epochs\" : config[\"n_epochs\"],\n",
    "        \"model_state_dict\":model.state_dict(),\n",
    "        \"model_config\": config[\"model\"],\n",
    "        \"config\":config\n",
    "    }, f\"{PATH}/{unique_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bcc0d9c3-9abd-4897-b67a-6f3958b69899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    std_loss = nn.L1Loss(reduction='mean').to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for in_seq,tar_seq,action,vid_size in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            in_seq = in_seq.to(device)\n",
    "            tar_seq = tar_seq.to(device)\n",
    "            seq_pred,_  = model(in_seq)\n",
    "            \n",
    "            loss = std_loss(seq_pred, tar_seq)\n",
    "            #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "            #print(contrastive_loss(embed,labels=sample_label.view(-1)))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for in_seq,tar_seq,action,vid_size in val_dataset:\n",
    "\n",
    "                in_seq = in_seq.to(device)\n",
    "                tar_seq = tar_seq.to(device)\n",
    "                seq_pred,_  = model(in_seq)\n",
    "\n",
    "                loss = std_loss(seq_pred, tar_seq)\n",
    "                #loss += 0.5*contrastive_loss(embed,labels=sample_label.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "            gen_random_video(model,val_data,epoch,batch_size,model_name=model_ident,unique_name=unique_iden)\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    save_model(model,model_ident,unique_iden)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8151f9-65c0-40d2-a99e-6787bdf5e8a5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:05<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.24290376504262287 val loss 0.15960990720325047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss 0.14459537963072458 val loss 0.1245271580086814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:43<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss 0.12182749293389775 val loss 0.11572516345315509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss 0.11385064572095871 val loss 0.10929773267772462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:42<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss 0.10884031933687982 val loss 0.10574090381463369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:47<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss 0.1052983188203403 val loss 0.10365051130453745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss 0.10404500507173084 val loss 0.10394467049174838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:46<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss 0.10336963839474178 val loss 0.10347019251849916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:48<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss 0.10247907741438775 val loss 0.1017137583759096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:51<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss 0.10338200518772715 val loss 0.10668973674376805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:47<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss 0.10144967092644601 val loss 0.10142818954255846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:50<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss 0.10189019455796196 val loss 0.10040643049610985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss 0.10087397325606573 val loss 0.10087570680512323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:47<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss 0.10079583107005982 val loss 0.10043657471736273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:46<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss 0.10104508119679632 val loss 0.09985092828671137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:46<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train loss 0.10051294621967134 val loss 0.1000378140144878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:47<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss 0.10158181126628603 val loss 0.10648167894946205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:48<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss 0.10026379156680335 val loss 0.09957877132627699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:48<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train loss 0.10009334498927706 val loss 0.09965840313169691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train loss 0.10024074337312154 val loss 0.09979111535681619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:44<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train loss 0.10044382874454771 val loss 0.09941149552663167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:47<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train loss 0.10001534287418638 val loss 0.09937429510884815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:48<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: train loss 0.10029141317520823 val loss 0.09997311863634321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 149/210 [00:33<00:09,  6.18it/s]"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  tcn_model, \n",
    "  train_dl, \n",
    "  val_dl, \n",
    "  n_epochs=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924402fb-167c-42cf-b6bc-db7c196a27f9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "\n",
    "ax.plot(history['train'])\n",
    "ax.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8e165",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_list(model,test_set,model_identifier=model_ident,unique_iden=unique_iden, save_file=True):\n",
    "    embedding_list = {}\n",
    "    output_list = {}\n",
    "    with torch.no_grad():\n",
    "        for in_seq,tar_seq,action,vid_size in tqdm(test_set):\n",
    "            in_seq = in_seq.to(device)\n",
    "            tar_seq = tar_seq.to(device)\n",
    "            seq_pred,embedding  = model(in_seq)\n",
    "\n",
    "            for seq,emb,action in zip(seq_pred.unbind(0),embedding.unbind(0),action.unbind(0)):\n",
    "                    try:\n",
    "                        if len(embedding_list[int(action)])<=50:\n",
    "                            embedding_list[int(action)].append(emb)\n",
    "                            output_list[int(action)].append(seq)\n",
    "                    except KeyError:\n",
    "                        embedding_list[int(action)] = [emb]\n",
    "                        output_list[int(action)] = [emb]\n",
    "\n",
    "    if save_file:  \n",
    "        os.makedirs(f\"{embeddings_save}/{model_identifier}/{unique_iden}\",exist_ok=True)\n",
    "        with open(f\"{embeddings_save}/{model_identifier}/{unique_iden}/embedding_list.pkl\",\"wb\") as f0:\n",
    "            pickle.dump(embedding_list,f0)\n",
    "\n",
    "        with open(f\"{embeddings_save}/{model_identifier}/{unique_iden}/outputs_list.pkl\",\"wb\") as f0:\n",
    "            pickle.dump(output_list,f0)\n",
    "\n",
    "\n",
    "    return embedding_list,output_list\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "        \n",
    "        #embedding_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe986b2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "emb_list,out_list = get_embedding_list(model,test_dl,model_ident,\"epoch50_randsample_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ac08a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_heatmaps(arr_list,nrows=5,ncols=2):\n",
    "    ran_list = random.sample(arr_list,ncols*nrows)\n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, sharex=True)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            #print(i*ncols+j,len(ran_list))\n",
    "            ax[i,j].imshow(ran_list[i*ncols+j].detach().numpy()[np.newaxis,:], cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee6b2f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_heatmaps(emb_list[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f871eba",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_heatmaps(emb_list[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272b6af",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_heatmaps(emb_list[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb5e4b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "id2clsname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31952457",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_data = SkeletonDataset(test_file_list,clsname2id,file_name=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccf54e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def gen_video_from_embeddings(embedding,model,save_file):\n",
    "    seq_out = model.decoder(embedding.repeat(batch_size,1,1))\n",
    "    gen_video(seq_out[0].detach().numpy(), save_file, 400, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c42ec",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def save_video_dataset(model,dataset,model_identifier,unique_identifier):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for __id,(in_seq,tar_seq,action,vid_size,__file_name) in tqdm(enumerate(dataset)):\n",
    "            in_seq = in_seq.to(device)\n",
    "            tar_seq = tar_seq.to(device)\n",
    "            seq_pred,embedding  = model(in_seq)\n",
    "\n",
    "            for __id,(input_vid,output_vid,action,__fln,vid_s) in enumerate(zip(in_seq.unbind(0),seq_pred.unbind(0),action.unbind(0),__file_name,vid_size.unbind(0))):\n",
    "                __fln = __fln.split('.')[0].split(\"/\")[-1]\n",
    "                os.makedirs(f\"{test_vids}/{model_identifier}/{unique_identifier}/{int(action)}/{__fln}\",exist_ok=True)\n",
    "                # gen_video(input_vid.detach().numpy(),f\"{test_vids}/{int(action)}/{__fln}/true.mp4\",int(vid_s[0]),int(vid_s[1]))\n",
    "                # gen_video(output_vid.detach().numpy(),f\"{test_vids}/{int(action)}/{__fln}/pred.mp4\",int(vid_s[0]),int(vid_s[1]))\n",
    "                gen_cmp_video(input_vid.detach().numpy(), output_vid.detach().numpy(),f\"{test_vids}/{model_identifier}/{unique_identifier}/{int(action)}/{__fln}/cmp.mp4\",int(vid_s[0]),int(vid_s[1]))\n",
    "                \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "        \n",
    "        #embedding_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a9965",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_video_dataset(model,test_dl,model_ident,unique_iden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27364a15",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvts_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccc35149ad8fa032444ebff1245e6ef176e6c1ce3af8dec48e3374f21a6b0f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
