{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkIfhXqxWhOw",
        "outputId": "9b33a8e1-6f66-4945-cc59-6455a3312dc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ACER\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch \n",
        "from torch import nn \n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.autonotebook import tqdm\n",
        "import itertools\n",
        "import random\n",
        "import copy\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import partial\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "from itertools import product\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n",
        "                        \"#FFDD00\",\n",
        "                        \"#FF7D00\",\n",
        "                        \"#FF006D\",\n",
        "                        \"#ADFF02\",\n",
        "                        \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Collection of functions which enable the evaluation of a classifier's performance,\n",
        "by showing confusion matrix, accuracy, recall, precision etc.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "from tabulate import tabulate\n",
        "import math\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def save_history(history, model_name, unique_name, models_saves, config):\n",
        "    PATH = f\"{models_saves}/{model_name}\"\n",
        "    os.makedirs(PATH, exist_ok=True)\n",
        "\n",
        "    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n",
        "        json.dump(history, f0)\n",
        "\n",
        "def get_config(file_loc):\n",
        "    file = torch.load(file_loc)\n",
        "    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n",
        "    \n",
        "def save_model(model, model_name, unique_name, models_saves, config):\n",
        "    PATH = f\"{models_saves}/{model_name}\"\n",
        "    os.makedirs(PATH, exist_ok=True)\n",
        "    torch.save({\n",
        "        \"n_epochs\": config[\"n_epochs\"],\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"model_config\": config[\"model\"],\n",
        "        \"config\": config\n",
        "    }, f\"{PATH}/{unique_name}.pt\")\n",
        "\n",
        "def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n",
        "    \"\"\"Plot confusion matrix in a separate window\"\"\"\n",
        "    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    if label_strings:\n",
        "        tick_marks = np.arange(len(label_strings))\n",
        "        plt.xticks(tick_marks, label_strings, rotation=90)\n",
        "        plt.yticks(tick_marks, label_strings)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n",
        "    \"\"\"\n",
        "    Returns a string of a report for given metric arrays (array length equals the number of classes).\n",
        "    Called internally by `analyze_classification`.\n",
        "        digits: number of digits after . for displaying results\n",
        "        number_of_thieves: number of biggest thieves to report\n",
        "        maxcharlength: max. number of characters to use when displaying thief names\n",
        "    \"\"\"\n",
        "\n",
        "    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n",
        "    sorted_class_indices = np.argsort(relative_freq)[\n",
        "                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n",
        "\n",
        "    last_line_heading = 'avg / total'\n",
        "\n",
        "    width = max(len(cn) for cn in existing_class_names)\n",
        "    width = max(width, len(last_line_heading), digits)\n",
        "\n",
        "    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n",
        "    fmt = '%% %ds' % width  # first column: class name\n",
        "    fmt += '  '\n",
        "    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n",
        "    fmt += '|\\t % 5s'\n",
        "    fmt += '\\n'\n",
        "\n",
        "    headers = [\"\"] + headers\n",
        "    report = fmt % tuple(headers)\n",
        "    report += '\\n'\n",
        "\n",
        "    for i in sorted_class_indices:\n",
        "        values = [existing_class_names[i]]\n",
        "        for v in (precision[i], recall[i], f1[i],\n",
        "                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n",
        "            values += [\"{0:0.{1}f}\".format(v, digits)]\n",
        "        values += [\"{}\".format(support[i])]\n",
        "        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n",
        "                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n",
        "        thieves = thieves[thieves != i]  # exclude self at this point\n",
        "        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n",
        "        thieves_names = [\n",
        "            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n",
        "            in thieves]  # a little inefficient but inconsequential\n",
        "        string_about_stealing = \"\"\n",
        "        for j in range(len(thieves)):\n",
        "            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n",
        "        values += [string_about_stealing]\n",
        "\n",
        "        report += fmt % tuple(values)\n",
        "\n",
        "    report += '\\n' + 100 * '-' + '\\n'\n",
        "\n",
        "    # compute averages/sums\n",
        "    values = [last_line_heading]\n",
        "    for v in (np.average(precision, weights=relative_freq),\n",
        "                np.average(recall, weights=relative_freq),\n",
        "                np.average(f1, weights=relative_freq)):\n",
        "        values += [\"{0:0.{1}f}\".format(v, digits)]\n",
        "    values += ['{0}'.format(np.sum(relative_freq))]\n",
        "    values += ['{0}'.format(np.sum(support))]\n",
        "    values += ['']\n",
        "\n",
        "    # make last (\"Total\") line for report\n",
        "    report += fmt % tuple(values)\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n",
        "    \"\"\"\n",
        "    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n",
        "    Input:\n",
        "        y_pred: 1D array of predicted labels (class indices)\n",
        "        y_true: 1D array of true labels (class indices)\n",
        "        class_names: 1D array or list of class names in the order of class indices.\n",
        "            Could also be integers [0, 1, ..., num_classes-1].\n",
        "        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n",
        "    \"\"\"\n",
        "\n",
        "    # Trim class_names to include only classes existing in y_pred OR y_true\n",
        "    in_pred_labels = set(list(y_pred))\n",
        "    in_true_labels = set(list(y_true))\n",
        "    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n",
        "\n",
        "    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n",
        "    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n",
        "    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n",
        "    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n",
        "\n",
        "    # Confusion matrix\n",
        "    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
        "    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n",
        "\n",
        "    if show_plot:\n",
        "        plt.figure()\n",
        "        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n",
        "                                title='Confusion matrix normalized by row')\n",
        "        plt.show(block=False)\n",
        "\n",
        "    # Analyze results\n",
        "    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n",
        "    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n",
        "\n",
        "    # returns metrics for each class, in the same order as existing_class_names\n",
        "    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n",
        "    # Print report\n",
        "    if print_report:\n",
        "        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n",
        "\n",
        "    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0XIAvcQX3l0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def classname_id(class_name_list):\n",
        "    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n",
        "    classname2id = {v:k for k, v in id2classname.items()}\n",
        "    return id2classname, classname2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y7gAeWUDX7X6"
      },
      "outputs": [],
      "source": [
        "model_ident = \"NTURGB120_skeleton_classifier\"\n",
        "unique_iden = \"epoch50_emb1024_xy\"\n",
        "\n",
        "main_dir = \"..\"\n",
        "\n",
        "data_dir = os.path.join(\"E:\\\\FYP_Data\\\\NTU120\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\nturgb+d_skeletons\")\n",
        "remove_files = [\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\NTU_RGBD120_samples_with_missing_skeletons.txt\",\n",
        "                \"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\NTU_RGBD_samples_with_missing_skeletons.txt\"]\n",
        "\n",
        "epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n",
        "models_saves = os.path.join(main_dir,\"model_saves\")\n",
        "embeddings_save = os.path.join(main_dir,\"embedding_save\")\n",
        "prototypes_save = os.path.join(main_dir,\"prototypes\")\n",
        "test_vids = os.path.join(main_dir,\"test_vids\")\n",
        "train_ratio = 0.90\n",
        "val_ratio = 0.1\n",
        "batch_size = 8\n",
        "\n",
        "os.makedirs(epoch_vids,exist_ok=True)\n",
        "os.makedirs(models_saves,exist_ok=True)\n",
        "os.makedirs(embeddings_save,exist_ok=True)\n",
        "\n",
        "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\nturgb120_label_map.json\",\"r\") as f0:\n",
        "    full_id2cls = json.load(f0)\n",
        "    \n",
        "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\skel\\\\nturgbd_skeletons_s001_to_s032\\\\sel_cls_list.txt\",\"r\") as f0:\n",
        "    class_names = [full_id2cls[x] for x in f0.read().split(\" \")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IrsB7xhhaKF5"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"n_epochs\":50,\n",
        "    \"model_name\":\"BidirectionalLSTM\",\n",
        "    \"model\":{\n",
        "        \"seq_len\":100,\n",
        "        \"input_size\":25*2,\n",
        "        \"hidden_size\":2048,\n",
        "        \"linear_filters\":[128,256,512,1024],\n",
        "        \"embedding_size\":2048,\n",
        "        \"num_classes\":len(class_names),\n",
        "        \"num_layers\":1,\n",
        "        \"bidirectional\":True,\n",
        "        \"batch_size\":batch_size,\n",
        "        \"dev\":device\n",
        "    },\n",
        "    'alpha_recon': 1,\n",
        "    'alpha_target': 0.05,\n",
        "}\n",
        "\n",
        "id2clsname, clsname2id = classname_id(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "25RyOW-8gA46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Files to remove:=  535\n",
            "Number of Files to Total:=  113945\n",
            "Number of Files to Train:=  70034\n",
            "Number of Files to Val:=  7782\n"
          ]
        }
      ],
      "source": [
        "from dataset.SkeletonData.data import *\n",
        "\n",
        "with open(\"E:\\\\FYP_Data\\\\NTU120\\\\shapes_keys.json\",\"r\") as f0:\n",
        "    id2shapes = json.load(f0)\n",
        "\n",
        "files_to_remove = set()\n",
        "for __f in remove_files:\n",
        "    with open(__f,\"r\") as f0:\n",
        "        for val in f0.read().split(\"\\n\"):\n",
        "            files_to_remove.add(val)\n",
        "\n",
        "print(\"Number of Files to remove:= \",len(files_to_remove))\n",
        "\n",
        "total_files = set([x.split(\".\")[0] for x in os.listdir(data_dir)]) - files_to_remove\n",
        "total_files_loc = set([f\"{os.path.join(data_dir,x)}.skeleton\" for x in total_files])\n",
        "\n",
        "#split list\n",
        "rows = [(full_id2cls[str(int(x.split(\".\")[0][-3:]))],x) for x in total_files_loc]\n",
        "info_pd = pd.DataFrame(data=rows,columns=[\"target\",\"file_loc\"])\n",
        "\n",
        "#select needed classes.\n",
        "info_pd = info_pd.loc[info_pd[\"target\"].isin(class_names)]\n",
        "train_df, val_df = train_test_split(info_pd,stratify=info_pd[\"target\"],train_size=train_ratio)\n",
        "\n",
        "print(\"Number of Files to Total:= \",len(total_files))\n",
        "\n",
        "train_builder = SkeletonFileBuilder(file_names=set(train_df[\"file_loc\"].to_list()))\n",
        "val_builder = SkeletonFileBuilder(file_names=set(val_df[\"file_loc\"].to_list()))\n",
        "\n",
        "print(\"Number of Files to Train:= \",len(train_builder))\n",
        "print(\"Number of Files to Val:= \",len(val_builder))\n",
        "\n",
        "train_file_iterator = iter(train_builder)\n",
        "val_file_iterator = iter(val_builder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Res0UAsOjjki"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_file_to_memory(id2shape,save_dict,each_file):\n",
        "      file_id = each_file.filepath.split(os.path.sep)[-1].split(\".\")[0]\n",
        "      num_frame, body_data = each_file.load_data()\n",
        "      orig_vid_size = id2shape[file_id]\n",
        "      \n",
        "      for frame_data in body_data:\n",
        "        if frame_data[\"body_count\"] != 1:\n",
        "            return None\n",
        "      \n",
        "      skel_data = []\n",
        "      for frame_data in body_data:\n",
        "          frame_jd = []\n",
        "          for jd in frame_data[\"bodies\"][0][\"joint_details\"]:\n",
        "              x = jd[\"colorX\"] / orig_vid_size[1]\n",
        "              y = jd[\"colorY\"] / orig_vid_size[0]\n",
        "\n",
        "              frame_jd.append([x, y])\n",
        "\n",
        "          skel_data.append(frame_jd)\n",
        "\n",
        "      skel_data = np.asarray(skel_data)\n",
        "      save_dict[file_id] = (file_id,orig_vid_size,str(int(file_id[-3:])),skel_data)\n",
        "      return file_id\n",
        "\n",
        "class SkeletonDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 data_builder, \n",
        "                 fileid2shape,\n",
        "                 full_label_map,\n",
        "                 cls2id,\n",
        "                 transform=None,\n",
        "                 seq_len = 100,\n",
        "                 window_size = 200,\n",
        "                 target_transform=None,\n",
        "                 active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28],\n",
        "                 file_name=False, \n",
        "                 is_2d=False):\n",
        "        self.data_builder = data_builder\n",
        "        self.transform = transform\n",
        "        self.fileid2shape = fileid2shape\n",
        "        self.window_size = window_size\n",
        "        self.seq_len = seq_len\n",
        "        self.target_transform = target_transform\n",
        "        self.active_locations = active_locations\n",
        "        self.file_name = file_name\n",
        "        self.is_2d = is_2d\n",
        "        self.cls2id = cls2id\n",
        "        self.full_label_map = full_label_map\n",
        "\n",
        "        if self.active_locations:\n",
        "          self.join_translation_map = {k:i for i,k in enumerate(self.active_locations)}\n",
        "        \n",
        "        self.data = {}\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "          self.indexes = list(\n",
        "              tqdm(\n",
        "                executor.map(\n",
        "                  partial(load_file_to_memory,self.fileid2shape,self.data),\n",
        "                  self.data_builder), \n",
        "                total=len(self.data_builder),\n",
        "                desc=\"Loaded Files\"\n",
        "              )\n",
        "            )\n",
        "        \n",
        "        #black_filter = []\n",
        "        #for idx in range(self.df.shape[0]):\n",
        "        #  if len(self.data[idx][\"coords\"].shape)<3 or self.data[idx][\"coords\"].shape[0]<20:\n",
        "        #    black_filter.append(idx)\n",
        "        #\n",
        "        self.indexes = [x for x in self.indexes if x != None]\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes)\n",
        "      \n",
        "    def select_frames(self,sequence):\n",
        "      if sequence.shape[0]<self.seq_len:\n",
        "        times = self.seq_len//sequence.shape[0] + 1\n",
        "\n",
        "        sequence = sequence.repeat(times,1,1)\n",
        "\n",
        "      if sequence.shape[0]>self.window_size:\n",
        "        start = random.randint(0,sequence.shape[0]-self.window_size-1)\n",
        "        sequence = sequence[start:start+self.window_size,...]\n",
        "                               \n",
        "      sel_index = sorted(random.sample(range(sequence.shape[0]),self.seq_len))\n",
        "        \n",
        "      return sequence[sel_index,...]\n",
        "    \n",
        "    def create_connection_map(self,original_map):\n",
        "      if not self.active_locations:\n",
        "        return original_map\n",
        "      \n",
        "      all_possible_comb = product(self.active_locations,self.active_locations)\n",
        "      all_possible_comb = set(all_possible_comb)\n",
        "      \n",
        "      original_map = set(original_map)\n",
        "      sel_connections = list(all_possible_comb.intersection(original_map))\n",
        "      sel_connections = [(self.join_translation_map[x[0]],self.join_translation_map[x[1]]) for x in sel_connections]\n",
        "      \n",
        "      return sel_connections \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx = self.indexes[idx]\n",
        "        \n",
        "        orig_target = self.data[idx][2]\n",
        "        file_path = self.data[idx][0]\n",
        "        vid_size = self.data[idx][1]\n",
        "        coords = self.data[idx][3]\n",
        "        \n",
        "        target = self.cls2id[self.full_label_map[orig_target]]\n",
        "        \n",
        "        if self.active_locations:\n",
        "          coords = coords[:,self.active_locations,:]\n",
        "\n",
        "        if self.is_2d:\n",
        "            coords = coords[...,0:2]\n",
        "\n",
        "        coords = torch.from_numpy(coords).float()\n",
        "        coords = self.select_frames(coords)\n",
        "\n",
        "        shape = coords.shape\n",
        "        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n",
        "        label = torch.clone(coords)\n",
        "\n",
        "        if self.transform:\n",
        "            coords = self.transform(coords)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(coords)\n",
        "\n",
        "        if self.file_name:\n",
        "            return coords, label, target,vid_size,file_path\n",
        "        return coords, label, target,vid_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#active_locations=[\n",
        "#                                 3,2,20,4,5,6,7,8,9,10,11,1,0,12,13,14,16,17,18\n",
        "#                             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2d95274957164fd5bba0bad5ee842b3a",
            "d3200c26f4af46ff99b287828b7e9ea7",
            "93eb418acba1422db89c28e98fa0ae76",
            "c5aa201965a0486ca00eff8f699de8f8",
            "4db48309ea6b4169975679004d422a86",
            "0591cfcae0f949e78ebf5b91699755fc",
            "3c5dd52f5ca949ac824d64a000dbfd2a",
            "3ed3488ab0874a4980032517c5966b10",
            "6e69f943a503460896239b94fdbf15c8",
            "3c1cd97dfdd64371bf2489419d334cb8",
            "c25a6b88cef244f090e8dd43d9e197d7"
          ]
        },
        "id": "o0Mlr88WEjWt",
        "outputId": "eaa5cded-8b20-490d-aae0-e14b0450cc7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loaded Files: 100%|██████████| 70034/70034 [37:13<00:00, 31.35it/s]  \n",
            "Loaded Files: 100%|██████████| 7782/7782 [04:57<00:00, 26.18it/s] \n"
          ]
        }
      ],
      "source": [
        "train_ds = SkeletonDataset(train_file_iterator,\n",
        "                             id2shapes,\n",
        "                             full_id2cls,\n",
        "                             clsname2id,\n",
        "                             is_2d=True,\n",
        "                             file_name=True,\n",
        "                             active_locations=None)\n",
        "val_ds = SkeletonDataset(val_file_iterator,\n",
        "                           id2shapes,\n",
        "                           full_id2cls,\n",
        "                           clsname2id,\n",
        "                           seq_len=config[\"model\"][\"seq_len\"],\n",
        "                           is_2d=True,\n",
        "                           file_name=True,\n",
        "                           active_locations=None)\n",
        "#test_data = SkeletonDataset(val_file_iterator,id2shapes,is_2d=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(66232, 7368)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_ds.indexes),len(val_ds.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B6mtux6OEmIk"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "#test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gv4MRtlZbLYt"
      },
      "outputs": [],
      "source": [
        "def gen_skeleton(frame, \n",
        "                 height,\n",
        "                 width,\n",
        "                 mapping_list = [(0, 1), (1, 3), (3, 5), \n",
        "                                 (0, 2), (2, 4), (0, 6), \n",
        "                                 (1, 7), (6, 7), (6, 8), \n",
        "                                 (7, 9), (8, 10), (9, 11)]):\n",
        "    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n",
        "    img_3.fill(255)\n",
        "\n",
        "    # add circles\n",
        "    for coord in frame:\n",
        "        x, y = int(width*coord[0]), int(height*coord[1])\n",
        "        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n",
        "\n",
        "    # add lines\n",
        "    for line in mapping_list:\n",
        "        i, j = line\n",
        "        st = frame[i, :]\n",
        "        start_point = (int(width*st[0]), int(height*st[1]))\n",
        "\n",
        "        en = frame[j, :]\n",
        "        end_point = (int(width*en[0]), int(height*en[1]))\n",
        "\n",
        "        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n",
        "\n",
        "    return img_3\n",
        "\n",
        "def gen_video(points, \n",
        "              save_file, \n",
        "              frame_h, \n",
        "              frame_w, \n",
        "              is_3d=True,\n",
        "              mapping_list = [(0, 1), (1, 3), (3, 5), \n",
        "                                 (0, 2), (2, 4), (0, 6), \n",
        "                                 (1, 7), (6, 7), (6, 8), \n",
        "                                 (7, 9), (8, 10), (9, 11)]):\n",
        "    # make 3D if points are flatten\n",
        "    if len(points.shape) == 2:\n",
        "        if is_3d:\n",
        "          fts = points.shape[1]\n",
        "          x_cds = list(range(0, fts, 3))\n",
        "          y_cds = list(range(1, fts, 3))\n",
        "          z_cds = list(range(2, fts, 3))\n",
        "          points = np.transpose(np.array([points[:, x_cds], \n",
        "                                          points[:, y_cds], \n",
        "                                          points[:, z_cds]]), (1,2,0))\n",
        "        else:\n",
        "          fts = points.shape[1]\n",
        "          x_cds = list(range(0, fts, 2))\n",
        "          y_cds = list(range(1, fts, 2))\n",
        "          points = np.transpose(np.array([points[:, x_cds], \n",
        "                                          points[:, y_cds]]), (1,2,0))\n",
        "\n",
        "    size = (frame_w, frame_h)\n",
        "    result = cv2.VideoWriter(save_file,\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         10, size)\n",
        "\n",
        "    for __id,frame in enumerate(points):\n",
        "        skel_image = gen_skeleton(frame, frame_h, frame_w,mapping_list=mapping_list)\n",
        "        result.write(skel_image)\n",
        "\n",
        "    result.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "joint_map = [(3,2),(2,20),(20,4),(4,5),(5,6),(6,7),(7,21),(7,22),(20,8),(8,9),(9,10),(10,11),(11,23),(11,24),\n",
        "            (20,1),(1,0),(0,12),(12,13),(13,14),(14,15),(0,16),(16,17),(17,18),(18,19)]\n",
        "\n",
        "joint_map = val_ds.create_connection_map(joint_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfor adata in tqdm(train_dl):\\n  selected_ind = random.randint(0,adata[0].shape[0]-1)\\n  data = adata[0][selected_ind].numpy()\\n  file_id = adata[4][selected_ind].split(\".\")[0]\\n  target = id2clsname[int(adata[2][selected_ind])]\\n  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\\n  \\n  if(np.isnan(data).any() or np.isinf(data).any()):\\n      print(data.shape,file_id)\\n      \\n  if not (vid_size[0] and vid_size[1]):\\n    print(vid_size,file_id)\\n    \\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "for adata in tqdm(train_dl):\n",
        "  selected_ind = random.randint(0,adata[0].shape[0]-1)\n",
        "  data = adata[0][selected_ind].numpy()\n",
        "  file_id = adata[4][selected_ind].split(\".\")[0]\n",
        "  target = id2clsname[int(adata[2][selected_ind])]\n",
        "  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n",
        "  \n",
        "  if(np.isnan(data).any() or np.isinf(data).any()):\n",
        "      print(data.shape,file_id)\n",
        "      \n",
        "  if not (vid_size[0] and vid_size[1]):\n",
        "    print(vid_size,file_id)\n",
        "    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OJzG7jyh7pU_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nsave_vids_dir = \"checking_vids/init\"\\nfor adata in tqdm(train_dl):\\n  selected_ind = random.randint(0,adata[0].shape[0]-1)\\n  data = adata[0][selected_ind].numpy()\\n  file_id = adata[4][selected_ind].split(\".\")[0]\\n  target = id2clsname[int(adata[2][selected_ind])]\\n  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\\n  try:\\n    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\\n      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\\n      gen_video(data, \\n                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\\n                vid_size[0], \\n                vid_size[1],\\n                is_3d=False,\\n                mapping_list=joint_map\\n                )\\n  except ValueError:\\n    continue\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "save_vids_dir = \"checking_vids/init\"\n",
        "for adata in tqdm(train_dl):\n",
        "  selected_ind = random.randint(0,adata[0].shape[0]-1)\n",
        "  data = adata[0][selected_ind].numpy()\n",
        "  file_id = adata[4][selected_ind].split(\".\")[0]\n",
        "  target = id2clsname[int(adata[2][selected_ind])]\n",
        "  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n",
        "  try:\n",
        "    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\n",
        "      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n",
        "      gen_video(data, \n",
        "                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\n",
        "                vid_size[0], \n",
        "                vid_size[1],\n",
        "                is_3d=False,\n",
        "                mapping_list=joint_map\n",
        "                )\n",
        "  except ValueError:\n",
        "    continue\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hZ2J16W-KBNw"
      },
      "outputs": [],
      "source": [
        "#!zip -r /content/check_vids.zip /content/checking_vids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QNhRUvwyFUQb"
      },
      "outputs": [],
      "source": [
        "class BiLSTMEncoder(nn.Module):\n",
        "    def __init__(self,seq_len, input_size,num_classes, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
        "        super(BiLSTMEncoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dev=dev\n",
        "        self.num_layers = num_layers\n",
        "        self.linear_filters = linear_filters\n",
        "        self.embedding_size = embedding_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.seq_len = seq_len\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # define LSTM layer\n",
        "        self.layers = []\n",
        "\n",
        "        # add linear layers \n",
        "        for __id,layer_out in enumerate(self.linear_filters):\n",
        "            if __id == 0:\n",
        "                self.layers.append(nn.Linear(self.input_size, layer_out))\n",
        "            else:\n",
        "                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n",
        "\n",
        "        # add lstm layer\n",
        "        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n",
        "                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "        self.classification_header = nn.Linear(self.embedding_size,self.num_classes)\n",
        "\n",
        "        #add embedding out\n",
        "        if bidirectional:\n",
        "            self.bn = nn.BatchNorm1d(self.hidden_size*4)\n",
        "            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n",
        "        else:\n",
        "            self.bn = nn.BatchNorm1d(self.hidden_size*2)\n",
        "            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n",
        "\n",
        "        \n",
        "    def forward(self, x_input):\n",
        "        \"\"\"\n",
        "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
        "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
        "        \"\"\"\n",
        "        \n",
        "        x = self.net(x_input)\n",
        "        lstm_out, self.hidden = self.lstm(x)\n",
        "        hidden_transformed = torch.cat(self.hidden,0)\n",
        "        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n",
        "        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n",
        "\n",
        "        #hidden_transformed = self.bn(hidden_transformed)\n",
        "        hidden_transformed = self.out_linear(hidden_transformed)\n",
        "\n",
        "        label = self.classification_header(hidden_transformed)\n",
        "        \n",
        "        return label, hidden_transformed\n",
        "\n",
        "    \n",
        "class BiLSTMDecoder(nn.Module):\n",
        "    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n",
        "        super(BiLSTMDecoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dev = dev\n",
        "        self.num_layers = num_layers\n",
        "        self.linear_filters = linear_filters[::-1]\n",
        "        self.embedding_size = embedding_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        if bidirectional:\n",
        "            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n",
        "        else:\n",
        "            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n",
        "\n",
        "        # define LSTM layer\n",
        "        self.layers = []\n",
        "        # add lstm\n",
        "        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n",
        "                            num_layers = self.num_layers, bidirectional=True,\n",
        "                            batch_first=bidirectional)\n",
        "\n",
        "                        \n",
        "        # add linear layers \n",
        "        if bidirectional:\n",
        "            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n",
        "        else:\n",
        "            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n",
        "\n",
        "        for __id,layer_in in enumerate(self.linear_filters):\n",
        "            if __id == len(linear_filters)-1:\n",
        "                self.layers.append(nn.Linear(layer_in,self.input_size))\n",
        "            else:\n",
        "                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n",
        "\n",
        "        self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self,encoder_hidden):\n",
        "        \"\"\"\n",
        "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
        "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        hidden_shape = encoder_hidden.shape\n",
        "        encoder_hidden = self.input_linear(encoder_hidden)\n",
        "        \n",
        "        if self.bidirectional:\n",
        "            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n",
        "            hidden = torch.transpose(hidden,1,0)\n",
        "            h1,h2,c1,c2 = torch.unbind(hidden,0)\n",
        "            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n",
        "            bs = h.size()[1]\n",
        "        else:\n",
        "            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n",
        "            hidden = torch.transpose(hidden,1,0)\n",
        "            h,c = torch.unbind(hidden,0)\n",
        "            bs = h.size()[1]\n",
        "        \n",
        "        dummy_input = torch.rand((bs,self.seq_len,self.linear_filters[0]), requires_grad=True).to(self.dev)\n",
        "        \n",
        "        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n",
        "        x = self.net(lstm_out)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class BiLSTMEncDecModel(nn.Module):\n",
        "    def __init__(self,seq_len, input_size, hidden_size,num_classes, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True,dev=device):\n",
        "        super(BiLSTMEncDecModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dev = dev\n",
        "        self.num_layers = num_layers\n",
        "        self.linear_filters = linear_filters[::-1]\n",
        "        self.embedding_size = embedding_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.num_classes= num_classes\n",
        "        \n",
        "        self.encoder = BiLSTMEncoder(seq_len, input_size, num_classes,hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
        "        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        label,embedding = self.encoder(x)\n",
        "        decoder_out = self.decoder(embedding)\n",
        "        \n",
        "        return decoder_out, embedding, label\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SxOl8YuW6f71"
      },
      "outputs": [],
      "source": [
        "encoder = BiLSTMEncoder(\n",
        "    seq_len=config[\"model\"][\"seq_len\"],\n",
        "    input_size=config[\"model\"][\"input_size\"],\n",
        "    num_classes = config[\"model\"][\"num_classes\"],\n",
        "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
        "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
        "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
        "    num_layers = config[\"model\"][\"num_layers\"],\n",
        "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
        "    dev=config[\"model\"][\"dev\"]).to(device)\n",
        "\n",
        "decoder = BiLSTMDecoder(\n",
        "    seq_len=config[\"model\"][\"seq_len\"],\n",
        "    input_size=config[\"model\"][\"input_size\"],\n",
        "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
        "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
        "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
        "    num_layers = config[\"model\"][\"num_layers\"],\n",
        "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
        "    dev=config[\"model\"][\"dev\"]).to(device)\n",
        "\n",
        "bilstm_model = BiLSTMEncDecModel(\n",
        "    seq_len=config[\"model\"][\"seq_len\"],\n",
        "    input_size=config[\"model\"][\"input_size\"],\n",
        "    num_classes = config[\"model\"][\"num_classes\"],\n",
        "    hidden_size=config[\"model\"][\"hidden_size\"],\n",
        "    linear_filters=config[\"model\"][\"linear_filters\"],\n",
        "    embedding_size=config[\"model\"][\"embedding_size\"],\n",
        "    num_layers = config[\"model\"][\"num_layers\"],\n",
        "    bidirectional=config[\"model\"][\"bidirectional\"],\n",
        "    dev=config[\"model\"][\"dev\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RETBAwj46i3Z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTMEncDecModel(\n",
              "  (encoder): BiLSTMEncoder(\n",
              "    (lstm): LSTM(1024, 2048, batch_first=True, bidirectional=True)\n",
              "    (net): Sequential(\n",
              "      (0): Linear(in_features=50, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (3): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    )\n",
              "    (classification_header): Linear(in_features=2048, out_features=82, bias=True)\n",
              "    (bn): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (out_linear): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "  )\n",
              "  (decoder): BiLSTMDecoder(\n",
              "    (input_linear): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "    (lstm): LSTM(1024, 2048, batch_first=True, bidirectional=True)\n",
              "    (net): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "      (1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (4): Linear(in_features=128, out_features=50, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bilstm_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F5Kn20ZT6m4H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 2048]), torch.Size([8, 82]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label, embedding = encoder(torch.randn((\n",
        "    batch_size,\n",
        "    config[\"model\"][\"seq_len\"],\n",
        "    config[\"model\"][\"input_size\"])\n",
        "                                       ).to(device))\n",
        "embedding.shape,label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8t79v6xM6wx7"
      },
      "outputs": [],
      "source": [
        "label_map = [(k,v) for k,v in id2clsname.items()]\n",
        "labelToId = {x[0]: i for i, x in enumerate(label_map)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Mfa8NjY57En3"
      },
      "outputs": [],
      "source": [
        "def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n",
        "    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n",
        "    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n",
        "    loss =  recon_loss + tar_loss\n",
        "\n",
        "    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n",
        "    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n",
        "\n",
        "    return loss, {\n",
        "        \"reconstruction_loss\":recon_loss.item(),\n",
        "        \"target_loss\":tar_loss.item()\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "brTkTW9BEa3i"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "std_loss = {\n",
        "    \"reconstruction_loss\" :nn.L1Loss(),\n",
        "    \"target_loss\" :nn.CrossEntropyLoss()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RUigg8dNcWJO"
      },
      "outputs": [],
      "source": [
        "def plot_curves(df):\n",
        "    df['loss'] = df['loss']/df['samples']\n",
        "    df['feat. loss'] = df['feat. loss']/df['samples']\n",
        "    df['classi. loss'] = df['classi. loss']/df['samples']\n",
        "    \n",
        "    fig, axs = plt.subplots(nrows=4)\n",
        "    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n",
        "    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n",
        "    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n",
        "    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DksWHRbgDgPQ"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, optimizer, loss_module, device, class_names):\n",
        "    model = model.train()\n",
        "    epoch_loss = 0  # total loss of epoch\n",
        "    total_samples = 0  # total samples in epoch\n",
        "    targets = []\n",
        "    predicts = []\n",
        "\n",
        "    with tqdm(dataloader, unit=\"batch\", desc=\"train\") as tepoch:\n",
        "          for input_sequence,target_sequence,target_action,target_vid_size,file_name in tepoch:\n",
        "            input_sequence = input_sequence.to(device)\n",
        "            target_sequence = target_sequence.to(device)\n",
        "            target_action = target_action.to(device)\n",
        "            \n",
        "\n",
        "            # Zero gradients, perform a backward pass, and update the weights.\n",
        "            optimizer.zero_grad()\n",
        "            # forward track history if only in train\n",
        "            with torch.set_grad_enabled(True):\n",
        "            # with autocast():\n",
        "                predicted_sequence, _, predicted_label  = model(input_sequence)\n",
        "            \n",
        "            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n",
        "            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n",
        "            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n",
        "            loss =  config['alpha_recon']*recon_loss + config['alpha_target']*tar_loss\n",
        "            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n",
        "\n",
        "            class_output = torch.argmax(predicted_label,dim=1)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            metrics = {\"loss\": loss.item()}\n",
        "            with torch.no_grad():\n",
        "                total_samples += len(target_action)\n",
        "                epoch_loss += loss.item()  # add total loss of batch\n",
        "\n",
        "            # convert feature vector into action class using cosine\n",
        "            pred_class = class_output.cpu().detach().numpy()\n",
        "            metrics[\"accuracy\"] = accuracy_score(y_true=target_action.cpu().detach().numpy(), y_pred=pred_class)\n",
        "            tepoch.set_postfix(metrics)\n",
        "\n",
        "            targets.append(target_action.cpu().detach().numpy())\n",
        "            predicts.append(pred_class)\n",
        "\n",
        "    \n",
        "    predicts = np.concatenate(predicts)\n",
        "    targets = np.concatenate(targets)\n",
        "    #train_metrics = action_evaluator(predicts,targets,class_names=list(clsname2id.keys()),print_report=False)\n",
        "\n",
        "    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TmXFlJJdEN9N"
      },
      "outputs": [],
      "source": [
        "def eval_step(model, dataloader,loss_module, device, class_names,  print_report=False, show_plot=False):\n",
        "    model = model.eval()\n",
        "    epoch_loss = 0  # total loss of epoch\n",
        "    total_samples = 0  # total samples in epoch\n",
        "    per_batch = {'targets': [], 'predictions': [], 'metrics': []}\n",
        "    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "      with tqdm(dataloader, unit=\"batch\", desc=\"eval\") as tepoch:\n",
        "        for input_sequence,target_sequence,target_action,target_vid_size,file_name in tepoch:\n",
        "\n",
        "            input_sequence = input_sequence.to(device)\n",
        "            target_sequence = target_sequence.to(device)\n",
        "            target_action = target_action.to(device)\n",
        "\n",
        "            # forward track history if only in train\n",
        "            with torch.set_grad_enabled(False):\n",
        "            # with autocast():\n",
        "                predicted_sequence,_,predicted_label  = model(input_sequence)\n",
        "\n",
        "            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n",
        "            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n",
        "            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n",
        "            loss =  config['alpha_recon']*recon_loss + config['alpha_target']*tar_loss\n",
        "            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n",
        "            \n",
        "            pred_action = torch.argmax(predicted_label,dim=1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                metrics['samples'] += len(target_action)\n",
        "                metrics['loss'] += loss.item()  # add total loss of batch\n",
        "                metrics['feat. loss'] += loss_detail[\"reconstruction_loss\"]\n",
        "                metrics['classi. loss'] += loss_detail[\"target_loss\"]\n",
        "\n",
        "            per_batch['targets'].append(target_action.cpu().numpy())\n",
        "            per_batch['predictions'].append(pred_action.cpu().numpy())\n",
        "            per_batch['metrics'].append([loss.cpu().numpy()])\n",
        "\n",
        "            tepoch.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    all_preds = np.concatenate(per_batch[\"predictions\"])\n",
        "    all_targets = np.concatenate(per_batch[\"targets\"])\n",
        "    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n",
        "    metrics_dict.update(metrics)\n",
        "    return metrics_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gEfzDnIP81qB"
      },
      "outputs": [],
      "source": [
        "#model_params, model_config, config = get_config(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_HMDB51_skeleton_classifier/100__epoch50_emb1024_xy.pt\")\n",
        "#bilstm_model.load_state_dict(model_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wyUEFUlp7HOt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "best_model_wts = copy.deepcopy(bilstm_model.state_dict())\n",
        "best_acc = 0.0\n",
        "show_interval = 10\n",
        "\n",
        "train_data = []\n",
        "for epoch in tqdm(range(1, config[\"n_epochs\"] + 1), desc='Training Epoch', leave=False):\n",
        "  \n",
        "  train_metrics = train_step(bilstm_model, train_dl, optimizer, std_loss, device, class_names)\n",
        "  train_metrics['epoch'] = epoch\n",
        "  train_metrics['phase'] = 'train'\n",
        "  train_data.append(train_metrics)\n",
        "  \n",
        "  if epoch % 10 == 0:\n",
        "    eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=True, show_plot=True)\n",
        "  else:\n",
        "    eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=False, show_plot=False)\n",
        "  eval_metrics['epoch'] = epoch \n",
        "  eval_metrics['phase'] = 'valid'\n",
        "  train_data.append(eval_metrics)\n",
        "\n",
        "  if epoch%10 == 0:\n",
        "    save_model(\n",
        "        bilstm_model, \n",
        "        f\"temp_{model_ident}\", \n",
        "        f\"{epoch}__{unique_iden}\",\n",
        "         models_saves, \n",
        "         config)\n",
        "    \n",
        "  if eval_metrics['accuracy'] > best_acc:\n",
        "    best_model = copy.deepcopy(bilstm_model.state_dict())\n",
        "  \n",
        "train_df = pd.DataFrame().from_records(train_data)\n",
        "plot_curves(train_df)\n",
        "\n",
        "# replace by best model \n",
        "bilstm_model.load_state_dict(best_model)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "fyp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "544d855d7b0d57add784f15e62ffa31fd790b767434a09f782574915bd2ed2d8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0591cfcae0f949e78ebf5b91699755fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d95274957164fd5bba0bad5ee842b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3200c26f4af46ff99b287828b7e9ea7",
              "IPY_MODEL_93eb418acba1422db89c28e98fa0ae76",
              "IPY_MODEL_c5aa201965a0486ca00eff8f699de8f8"
            ],
            "layout": "IPY_MODEL_4db48309ea6b4169975679004d422a86"
          }
        },
        "3c1cd97dfdd64371bf2489419d334cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c5dd52f5ca949ac824d64a000dbfd2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ed3488ab0874a4980032517c5966b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db48309ea6b4169975679004d422a86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e69f943a503460896239b94fdbf15c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93eb418acba1422db89c28e98fa0ae76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed3488ab0874a4980032517c5966b10",
            "max": 8068,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e69f943a503460896239b94fdbf15c8",
            "value": 4739
          }
        },
        "c25a6b88cef244f090e8dd43d9e197d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5aa201965a0486ca00eff8f699de8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c1cd97dfdd64371bf2489419d334cb8",
            "placeholder": "​",
            "style": "IPY_MODEL_c25a6b88cef244f090e8dd43d9e197d7",
            "value": " 4739/8068 [03:15&lt;02:19, 23.88it/s]"
          }
        },
        "d3200c26f4af46ff99b287828b7e9ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0591cfcae0f949e78ebf5b91699755fc",
            "placeholder": "​",
            "style": "IPY_MODEL_3c5dd52f5ca949ac824d64a000dbfd2a",
            "value": "Loaded Files:  59%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
